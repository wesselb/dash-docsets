<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.custom_gradient" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.custom_gradient" class="dashAnchor"></a><h1 id="tf.custom_gradient">tf.custom_gradient</h1>
<p>Decorator to define a function with a custom gradient.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.custom_gradient</code></li>
<li><code>tf.compat.v2.custom_gradient</code></li>
<li><code>tf.custom_gradient</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.custom_gradient(f)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/custom_gradient.py"><code>python/ops/custom_gradient.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.</p>
<p>For example, consider the following function that commonly occurs in the computation of cross entropy and log likelihoods:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> log1pexp(x):
  <span class="cf">return</span> tf.math.log(<span class="dv">1</span> <span class="op">+</span> tf.exp(x))</code></pre></div>
<p>Due to numerical instability, the gradient this function evaluated at x=100 is NaN. For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant(<span class="dv">100</span>.)
y <span class="op">=</span> log1pexp(x)
dy <span class="op">=</span> tf.gradients(y, x) <span class="co"># Will be NaN when evaluated.</span></code></pre></div>
<p>The gradient expression can be analytically simplified to provide numerical stability:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@tf.custom_gradient</span>
<span class="kw">def</span> log1pexp(x):
  e <span class="op">=</span> tf.exp(x)
  <span class="kw">def</span> grad(dy):
    <span class="cf">return</span> dy <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> e))
  <span class="cf">return</span> tf.math.log(<span class="dv">1</span> <span class="op">+</span> e), grad</code></pre></div>
<p>With this definition, the gradient at x=100 will be correctly evaluated as 1.0.</p>
<p>See also <a href="../tf/RegisterGradient.html"><code>tf.RegisterGradient</code></a> which registers a gradient function for a primitive TensorFlow operation. <a href="../tf/custom_gradient.html"><code>tf.custom_gradient</code></a> on the other hand allows for fine grained control over the gradient computation of a sequence of operations.</p>
<p>Note that if the decorated function uses <code>Variable</code>s, the enclosing variable scope must be using <code>ResourceVariable</code>s.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>f</code></b>: function <code>f(*x)</code> that returns a tuple <code>(y, grad_fn)</code> where:</li>
<li><code>x</code> is a sequence of <code>Tensor</code> inputs to the function.</li>
<li><code>y</code> is a <code>Tensor</code> or sequence of <code>Tensor</code> outputs of applying TensorFlow operations in <code>f</code> to <code>x</code>.</li>
<li><p><code>grad_fn</code> is a function with the signature <code>g(*grad_ys)</code> which returns a list of <code>Tensor</code>s - the derivatives of <code>Tensor</code>s in <code>y</code> with respect to the <code>Tensor</code>s in <code>x</code>. <code>grad_ys</code> is a <code>Tensor</code> or sequence of <code>Tensor</code>s the same size as <code>y</code> holding the initial value gradients for each <code>Tensor</code> in <code>y</code>. In a pure mathematical sense, a vector-argument vector-valued function <code>f</code>'s derivatives should be its Jacobian matrix <code>J</code>. Here we are expressing the Jacobian <code>J</code> as a function <code>grad_fn</code> which defines how <code>J</code> will transform a vector <code>grad_ys</code> when left-multiplied with it (<code>grad_ys * J</code>). This functional representation of a matrix is convenient to use for chain-rule calculation (in e.g. the back-propagation algorithm).</p>
<p>If <code>f</code> uses <code>Variable</code>s (that are not part of the inputs), i.e. through <code>get_variable</code>, then <code>grad_fn</code> should have signature <code>g(*grad_ys, variables=None)</code>, where <code>variables</code> is a list of the <code>Variable</code>s, and return a 2-tuple <code>(grad_xs, grad_vars)</code>, where <code>grad_xs</code> is the same as above, and <code>grad_vars</code> is a <code>list&lt;Tensor&gt;</code> with the derivatives of <code>Tensor</code>s in <code>y</code> with respect to the variables (that is, grad_vars has one Tensor per variable in variables).</p></li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A function <code>h(x)</code> which returns the same value as <code>f(x)[0]</code> and whose gradient (as calculated by <a href="../tf/gradients.html"><code>tf.gradients</code></a>) is determined by <code>f(x)[1]</code>.</p>
