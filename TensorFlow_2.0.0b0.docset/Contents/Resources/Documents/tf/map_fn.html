<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.map_fn" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.map_fn" class="dashAnchor"></a><h1 id="tf.map_fn">tf.map_fn</h1>
<p>map on the list of tensors unpacked from <code>elems</code> on dimension 0.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.map_fn</code></li>
<li><code>tf.compat.v2.map_fn</code></li>
<li><code>tf.map_fn</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.map_fn(
    fn,
    elems,
    dtype<span class="op">=</span><span class="va">None</span>,
    parallel_iterations<span class="op">=</span><span class="va">None</span>,
    back_prop<span class="op">=</span><span class="va">True</span>,
    swap_memory<span class="op">=</span><span class="va">False</span>,
    infer_shape<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/map_fn.py"><code>python/ops/map_fn.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>The simplest version of <code>map_fn</code> repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code>. <code>dtype</code> is the data type of the return value of <code>fn</code>. Users must provide <code>dtype</code> if it is different from the data type of <code>elems</code>.</p>
<p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>[values.shape[0]] + fn(values[0]).shape</code>.</p>
<p>This method also allows multi-arity <code>elems</code> and output of <code>fn</code>. If <code>elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of <code>fn</code> may match the structure of <code>elems</code>. That is, if <code>elems</code> is <code>(t1, [t2, t3, [t4, t5]])</code>, then an appropriate signature for <code>fn</code> is: <code>fn = lambda (t1, [t2, t3, [t4, t5]]):</code>.</p>
<p>Furthermore, <code>fn</code> may emit a different structure than its input. For example, <code>fn</code> may look like: <code>fn = lambda t1: return (t1 + 1, t1 - 1)</code>. In this case, the <code>dtype</code> parameter is not optional: <code>dtype</code> must be a type or (possibly nested) tuple of types matching the output of <code>fn</code>.</p>
<p>To apply a functional operation to the nonzero elements of a SparseTensor one of the following methods is recommended. First, if the function is expressible as TensorFlow ops, use</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">  result <span class="op">=</span> SparseTensor(<span class="bu">input</span>.indices, fn(<span class="bu">input</span>.values), <span class="bu">input</span>.dense_shape)</code></pre></div>
<p>If, however, the function is not expressible as a TensorFlow op, then use</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> SparseTensor(
  <span class="bu">input</span>.indices, map_fn(fn, <span class="bu">input</span>.values), <span class="bu">input</span>.dense_shape)</code></pre></div>
<p>instead.</p>
<p>When executing eagerly, map_fn does not execute in parallel even if <code>parallel_iterations</code> is set to a value &gt; 1. You can still get the performance benefits of running a function in parallel by using the <code>tf.contrib.eager.defun</code> decorator,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Assume the function being used in map_fn is fn.</span>
<span class="co"># To ensure map_fn calls fn in parallel, use the defun decorator.</span>
<span class="at">@tf.contrib.eager.defun</span>
<span class="kw">def</span> func(tensor):
  <span class="cf">return</span> tf.map_fn(fn, tensor)</code></pre></div>
<p>Note that if you use the defun decorator, any non-TensorFlow Python code that you may have written in your function won't get executed. See <code>tf.contrib.eager.defun</code> for more details. The recommendation would be to debug without defun but switch to defun to get performance benefits of running map_fn in parallel.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>fn</code></b>: The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as <code>elems</code>. Its output must have the same structure as <code>dtype</code> if one is provided, otherwise it must have the same structure as <code>elems</code>.</li>
<li><b><code>elems</code></b>: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be applied to <code>fn</code>.</li>
<li><b><code>dtype</code></b>: (optional) The output type(s) of <code>fn</code>. If <code>fn</code> returns a structure of Tensors differing from the structure of <code>elems</code>, then <code>dtype</code> is not optional and must have the same structure as the output of <code>fn</code>.</li>
<li><b><code>parallel_iterations</code></b>: (optional) The number of iterations allowed to run in parallel. When graph building, the default value is 10. While executing eagerly, the default value is set to 1.</li>
<li><b><code>back_prop</code></b>: (optional) True enables support for back propagation.</li>
<li><b><code>swap_memory</code></b>: (optional) True enables GPU-CPU memory swapping.</li>
<li><b><code>infer_shape</code></b>: (optional) False disables tests for consistent output shapes.</li>
<li><b><code>name</code></b>: (optional) Name prefix for the returned tensors.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code>fn</code> to tensors unpacked from <code>elems</code> along the first dimension, from first to last.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>fn</code> is not callable or the structure of the output of <code>fn</code> and <code>dtype</code> do not match, or if elems is a SparseTensor.</li>
<li><b><code>ValueError</code></b>: if the lengths of the output of <code>fn</code> and <code>dtype</code> do not match.</li>
</ul>
<h4 id="examples">Examples:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">elems <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])
squares <span class="op">=</span> map_fn(<span class="kw">lambda</span> x: x <span class="op">*</span> x, elems)
<span class="co"># squares == [1, 4, 9, 16, 25, 36]</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">elems <span class="op">=</span> (np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]), np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]))
alternate <span class="op">=</span> map_fn(<span class="kw">lambda</span> x: x[<span class="dv">0</span>] <span class="op">*</span> x[<span class="dv">1</span>], elems, dtype<span class="op">=</span>tf.int64)
<span class="co"># alternate == [-1, 2, -3]</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">elems <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])
alternates <span class="op">=</span> map_fn(<span class="kw">lambda</span> x: (x, <span class="op">-</span>x), elems, dtype<span class="op">=</span>(tf.int64, tf.int64))
<span class="co"># alternates[0] == [1, 2, 3]</span>
<span class="co"># alternates[1] == [-1, -2, -3]</span></code></pre></div>
