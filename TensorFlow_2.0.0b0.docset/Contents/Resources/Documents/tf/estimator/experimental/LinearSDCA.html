<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.estimator.experimental.LinearSDCA" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="get_train_step"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.estimator.experimental.LinearSDCA" class="dashAnchor"></a><h1 id="tf.estimator.experimental.linearsdca">tf.estimator.experimental.LinearSDCA</h1>
<h2 id="class-linearsdca">Class <code>LinearSDCA</code></h2>
<p>Stochastic Dual Coordinate Ascent helper for linear estimators.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.estimator.experimental.LinearSDCA</code></li>
<li>Class <code>tf.compat.v2.estimator.experimental.LinearSDCA</code></li>
<li>Class <code>tf.estimator.experimental.LinearSDCA</code></li>
</ul>
<p>Defined in <a href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear.py"><code>python/estimator/canned/linear.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>Objects of this class are intended to be provided as the optimizer argument (though LinearSDCA objects do not implement the <code>tf.train.Optimizer</code> interface) when creating <a href="../../../tf/estimator/LinearClassifier.html"><code>tf.estimator.LinearClassifier</code></a> or <a href="../../../tf/estimator/LinearRegressor.html"><code>tf.estimator.LinearRegressor</code></a>.</p>
<p>SDCA can only be used with <code>LinearClassifier</code> and <code>LinearRegressor</code> under the following conditions:</p>
<ul>
<li>Feature columns are of type V2.</li>
<li>Multivalent categorical columns are not normalized. In other words the <code>sparse_combiner</code> argument in the estimator constructor should be &quot;sum&quot;.</li>
<li>For classification: binary label.</li>
<li>For regression: one-dimensional label.</li>
</ul>
<h4 id="example-usage">Example usage:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">real_feature_column <span class="op">=</span> numeric_column(...)
sparse_feature_column <span class="op">=</span> categorical_column_with_hash_bucket(...)
linear_sdca <span class="op">=</span> tf.estimator.experimental.LinearSDCA(
    example_id_column<span class="op">=</span><span class="st">&#39;example_id&#39;</span>,
    num_loss_partitions<span class="op">=</span><span class="dv">1</span>,
    num_table_shards<span class="op">=</span><span class="dv">1</span>,
    symmetric_l2_regularization<span class="op">=</span><span class="fl">2.0</span>)
classifier <span class="op">=</span> tf.estimator.LinearClassifier(
    feature_columns<span class="op">=</span>[real_feature_column, sparse_feature_column],
    weight_column<span class="op">=</span>...,
    optimizer<span class="op">=</span>linear_sdca)
classifier.train(input_fn_train, steps<span class="op">=</span><span class="dv">50</span>)
classifier.evaluate(input_fn<span class="op">=</span>input_fn_eval)</code></pre></div>
<p>Here the expectation is that the <code>input_fn_*</code> functions passed to train and evaluate return a pair (dict, label_tensor) where dict has <code>example_id_column</code> as <code>key</code> whose value is a <code>Tensor</code> of shape [batch_size] and dtype string. num_loss_partitions defines sigma' in eq (11) of [3]. Convergence of (global) loss is guaranteed if <code>num_loss_partitions</code> is larger or equal to the product <code>(#concurrent train ops/per worker) x (#workers)</code>. Larger values for <code>num_loss_partitions</code> lead to slower convergence. The recommended value for <code>num_loss_partitions</code> in <a href="../../../tf/estimator.html"><code>tf.estimator</code></a> (where currently there is one process per worker) is the number of workers running the train steps. It defaults to 1 (single machine). <code>num_table_shards</code> defines the number of shards for the internal state table, typically set to match the number of parameter servers for large data sets.</p>
<p>The SDCA algorithm was originally introduced in [1] and it was followed by the L1 proximal step [2], a distributed version [3] and adaptive sampling [4]. [1] www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf [2] https://arxiv.org/pdf/1309.2375.pdf [3] https://arxiv.org/pdf/1502.03508.pdf [4] https://arxiv.org/pdf/1502.08053.pdf Details specific to this implementation are provided in: https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear_optimizer/doc/sdca.ipynb</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    example_id_column,
    num_loss_partitions<span class="op">=</span><span class="dv">1</span>,
    num_table_shards<span class="op">=</span><span class="va">None</span>,
    symmetric_l1_regularization<span class="op">=</span><span class="fl">0.0</span>,
    symmetric_l2_regularization<span class="op">=</span><span class="fl">1.0</span>,
    adaptive<span class="op">=</span><span class="va">False</span>
)</code></pre></div>
<p>Construct a new SDCA optimizer for linear estimators.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>example_id_column</code></b>: The column name containing the example ids.</li>
<li><b><code>num_loss_partitions</code></b>: Number of workers.</li>
<li><b><code>num_table_shards</code></b>: Number of shards of the internal state table, typically set to match the number of parameter servers.</li>
<li><b><code>symmetric_l1_regularization</code></b>: A float value, must be greater than or equal to zero.</li>
<li><b><code>symmetric_l2_regularization</code></b>: A float value, must be greater than zero and should typically be greater than 1.</li>
<li><b><code>adaptive</code></b>: A boolean indicating whether to use adaptive sampling.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="get_train_step">
<code>get_train_step</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_train_step(
    state_manager,
    weight_column_name,
    loss_type,
    feature_columns,
    features,
    targets,
    bias_var,
    global_step
)</code></pre></div>
<p>Returns the training operation of an SdcaModel optimizer.</p>
