<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.distribute.ReductionToOneDevice" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="batch_reduce"/> <meta itemprop="property" content="broadcast"/> <meta itemprop="property" content="reduce"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.distribute.ReductionToOneDevice" class="dashAnchor"></a><h1 id="tf.distribute.reductiontoonedevice">tf.distribute.ReductionToOneDevice</h1>
<h2 id="class-reductiontoonedevice">Class <code>ReductionToOneDevice</code></h2>
<p>Always do reduction to one device first and then do broadcasting.</p>
<p>Inherits From: <a href="../../tf/distribute/CrossDeviceOps.html"><code>CrossDeviceOps</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.distribute.ReductionToOneDevice</code></li>
<li>Class <code>tf.compat.v2.distribute.ReductionToOneDevice</code></li>
<li>Class <code>tf.distribute.ReductionToOneDevice</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/distribute/cross_device_ops.py"><code>python/distribute/cross_device_ops.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>Batch reduction is done by reduction on each element one by one.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    reduce_to_device<span class="op">=</span><span class="va">None</span>,
    accumulation_fn<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Constructor.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>reduce_to_device</code></b>: the intermediate device to reduce to. If None, reduce to the first device in <code>destinations</code> of the reduce() method.</li>
<li><b><code>accumulation_fn</code></b>: a function that does accumulation. If None, then <a href="../../tf/math/add_n.html"><code>tf.math.add_n</code></a> is used.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="batch_reduce">
<code>batch_reduce</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">batch_reduce(
    reduce_op,
    value_destination_pairs
)</code></pre></div>
<p>Reduce PerReplica objects in a batch.</p>
<p>Reduce each first element in <code>value_destination_pairs</code> to each second element which indicates the destinations.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>reduce_op</code></b>: Indicates how per_replica_value will be reduced. Accepted values are <a href="../../tf/distribute/ReduceOp.md#SUM"><code>tf.distribute.ReduceOp.SUM</code></a>, <a href="../../tf/distribute/ReduceOp.md#MEAN"><code>tf.distribute.ReduceOp.MEAN</code></a>.</li>
<li><b><code>value_destination_pairs</code></b>: a list or a tuple of tuples of PerReplica objects (or tensors with device set if there is one device) and destinations.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>a list of Mirrored objects.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if <code>value_destination_pairs</code> is not a list or a tuple of tuples of PerReplica objects and destinations</li>
</ul>
<h3 id="broadcast">
<code>broadcast</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">broadcast(
    tensor,
    destinations
)</code></pre></div>
<p>Broadcast the <code>tensor</code> to destinations.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>tensor</code></b>: the tensor to broadcast.</li>
<li><b><code>destinations</code></b>: the broadcast destinations.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>a Mirrored object.</p>
<h3 id="reduce">
<code>reduce</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">reduce</span>(
    reduce_op,
    per_replica_value,
    destinations
)</code></pre></div>
<p>Reduce <code>per_replica_value</code> to <code>destinations</code>.</p>
<p>It runs the reduction operation defined by <code>reduce_op</code> and put the result on <code>destinations</code>.</p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>reduce_op</code></b>: Indicates how per_replica_value will be reduced. Accepted values are <a href="../../tf/distribute/ReduceOp.md#SUM"><code>tf.distribute.ReduceOp.SUM</code></a>, <a href="../../tf/distribute/ReduceOp.md#MEAN"><code>tf.distribute.ReduceOp.MEAN</code></a>.</li>
<li><b><code>per_replica_value</code></b>: a PerReplica object or a tensor with device set.</li>
<li><b><code>destinations</code></b>: the reduction destinations.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>a Mirrored object.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if per_replica_value can't be converted to a PerReplica object.</li>
</ul>
