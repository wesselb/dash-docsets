<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.compat.v1.summary.image" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Class/tf.compat.v1.Summary.Image" class="dashAnchor"></a><a name="//apple_ref/cpp/Function/tf.compat.v1.summary.image" class="dashAnchor"></a><h1 id="tf.compat.v1.summary.image">tf.compat.v1.summary.image</h1>
<p>Outputs a <code>Summary</code> protocol buffer with images.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.compat.v1.summary.image(
    name,
    tensor,
    max_outputs<span class="op">=</span><span class="dv">3</span>,
    collections<span class="op">=</span><span class="va">None</span>,
    family<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/summary/summary.py"><code>python/summary/summary.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>The summary has up to <code>max_outputs</code> summary values containing images. The images are built from <code>tensor</code> which must be 4-D with shape <code>[batch_size, height, width, channels]</code> and where <code>channels</code> can be:</p>
<ul>
<li>1: <code>tensor</code> is interpreted as Grayscale.</li>
<li>3: <code>tensor</code> is interpreted as RGB.</li>
<li>4: <code>tensor</code> is interpreted as RGBA.</li>
</ul>
<p>The images have the same number of channels as the input tensor. For float input, the values are normalized one image at a time to fit in the range <code>[0, 255]</code>. <code>uint8</code> values are unchanged. The op uses two different normalization algorithms:</p>
<ul>
<li><p>If the input values are all positive, they are rescaled so the largest one is 255.</p></li>
<li><p>If any input value is negative, the values are shifted so input value 0.0 is at 127. They are then rescaled so that either the smallest value is 0, or the largest one is 255.</p></li>
</ul>
<p>The <code>tag</code> in the outputted Summary.Value protobufs is generated based on the name, with a suffix depending on the max_outputs setting:</p>
<ul>
<li>If <code>max_outputs</code> is 1, the summary value tag is '<em>name</em>/image'.</li>
<li>If <code>max_outputs</code> is greater than 1, the summary value tags are generated sequentially as '<em>name</em>/image/0', '<em>name</em>/image/1', etc.</li>
</ul>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name for the generated node. Will also serve as a series name in TensorBoard.</li>
<li><b><code>tensor</code></b>: A 4-D <code>uint8</code> or <code>float32</code> <code>Tensor</code> of shape <code>[batch_size, height,   width, channels]</code> where <code>channels</code> is 1, 3, or 4.</li>
<li><b><code>max_outputs</code></b>: Max number of batch elements to generate images for.</li>
<li><b><code>collections</code></b>: Optional list of ops.GraphKeys. The collections to add the summary to. Defaults to [_ops.GraphKeys.SUMMARIES]</li>
<li><b><code>family</code></b>: Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A scalar <code>Tensor</code> of type <code>string</code>. The serialized <code>Summary</code> protocol buffer.</p>
