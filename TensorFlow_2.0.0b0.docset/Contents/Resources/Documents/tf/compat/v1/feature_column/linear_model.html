<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.compat.v1.feature_column.linear_model" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.compat.v1.feature_column.linear_model" class="dashAnchor"></a><h1 id="tf.compat.v1.feature_column.linear_model">tf.compat.v1.feature_column.linear_model</h1>
<p>Returns a linear prediction <code>Tensor</code> based on given <code>feature_columns</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.compat.v1.feature_column.linear_model(
    features,
    feature_columns,
    units<span class="op">=</span><span class="dv">1</span>,
    sparse_combiner<span class="op">=</span><span class="st">&#39;sum&#39;</span>,
    weight_collections<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>,
    cols_to_vars<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/feature_column/feature_column.py"><code>python/feature_column/feature_column.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>This function generates a weighted sum based on output dimension <code>units</code>. Weighted sum refers to logits in classification problems. It refers to the prediction itself for linear regression problems.</p>
<p>Note on supported columns: <code>linear_model</code> treats categorical columns as <code>indicator_column</code>s. To be specific, assume the input as <code>SparseTensor</code> looks like:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">  shape <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">2</span>]
  {
      [<span class="dv">0</span>, <span class="dv">0</span>]: <span class="st">&quot;a&quot;</span>
      [<span class="dv">1</span>, <span class="dv">0</span>]: <span class="st">&quot;b&quot;</span>
      [<span class="dv">1</span>, <span class="dv">1</span>]: <span class="st">&quot;c&quot;</span>
  }</code></pre></div>
<p><code>linear_model</code> assigns weights for the presence of &quot;a&quot;, &quot;b&quot;, &quot;c' implicitly, just like <code>indicator_column</code>, while <code>input_layer</code> explicitly requires wrapping each of categorical columns with an <code>embedding_column</code> or an <code>indicator_column</code>.</p>
<h4 id="example-of-usage">Example of usage:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">price <span class="op">=</span> numeric_column(<span class="st">&#39;price&#39;</span>)
price_buckets <span class="op">=</span> bucketized_column(price, boundaries<span class="op">=</span>[<span class="dv">0</span>., <span class="dv">10</span>., <span class="dv">100</span>., <span class="dv">1000</span>.])
keywords <span class="op">=</span> categorical_column_with_hash_bucket(<span class="st">&quot;keywords&quot;</span>, 10K)
keywords_price <span class="op">=</span> crossed_column(<span class="st">&#39;keywords&#39;</span>, price_buckets, ...)
columns <span class="op">=</span> [price_buckets, keywords, keywords_price ...]
features <span class="op">=</span> tf.io.parse_example(..., features<span class="op">=</span>make_parse_example_spec(columns))
prediction <span class="op">=</span> linear_model(features, columns)</code></pre></div>
<p>The <code>sparse_combiner</code> argument works as follows For example, for two features represented as the categorical columns:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">  <span class="co"># Feature 1</span>

  shape <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">2</span>]
  {
      [<span class="dv">0</span>, <span class="dv">0</span>]: <span class="st">&quot;a&quot;</span>
      [<span class="dv">0</span>, <span class="dv">1</span>]: <span class="st">&quot;b&quot;</span>
      [<span class="dv">1</span>, <span class="dv">0</span>]: <span class="st">&quot;c&quot;</span>
  }

  <span class="co"># Feature 2</span>

  shape <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]
  {
      [<span class="dv">0</span>, <span class="dv">0</span>]: <span class="st">&quot;d&quot;</span>
      [<span class="dv">1</span>, <span class="dv">0</span>]: <span class="st">&quot;e&quot;</span>
      [<span class="dv">1</span>, <span class="dv">1</span>]: <span class="st">&quot;f&quot;</span>
      [<span class="dv">1</span>, <span class="dv">2</span>]: <span class="st">&quot;f&quot;</span>
  }</code></pre></div>
<p>with <code>sparse_combiner</code> as &quot;mean&quot;, the linear model outputs consequently are:</p>
<pre><code>  y_0 = 1.0 / 2.0 * ( w_a + w_b ) + w_d + b
  y_1 = w_c + 1.0 / 3.0 * ( w_e + 2.0 * w_f ) + b</code></pre>
<p>where <code>y_i</code> is the output, <code>b</code> is the bias, and <code>w_x</code> is the weight assigned to the presence of <code>x</code> in the input features.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>features</code></b>: A mapping from key to tensors. <code>_FeatureColumn</code>s look up via these keys. For example <code>numeric_column('price')</code> will look at 'price' key in this dict. Values are <code>Tensor</code> or <code>SparseTensor</code> depending on corresponding <code>_FeatureColumn</code>.</li>
<li><b><code>feature_columns</code></b>: An iterable containing the FeatureColumns to use as inputs to your model. All items should be instances of classes derived from <code>_FeatureColumn</code>s.</li>
<li><b><code>units</code></b>: An integer, dimensionality of the output space. Default value is 1.</li>
<li><b><code>sparse_combiner</code></b>: A string specifying how to reduce if a categorical column is multivalent. Except <code>numeric_column</code>, almost all columns passed to <code>linear_model</code> are considered as categorical columns. It combines each categorical column independently. Currently &quot;mean&quot;, &quot;sqrtn&quot; and &quot;sum&quot; are supported, with &quot;sum&quot; the default for linear model. &quot;sqrtn&quot; often achieves good accuracy, in particular with bag-of-words columns.
<ul>
<li>&quot;sum&quot;: do not normalize features in the column</li>
<li>&quot;mean&quot;: do l1 normalization on features in the column</li>
<li>&quot;sqrtn&quot;: do l2 normalization on features in the column</li>
</ul></li>
<li><b><code>weight_collections</code></b>: A list of collection names to which the Variable will be added. Note that, variables will also be added to collections <code>tf.GraphKeys.GLOBAL_VARIABLES</code> and <code>ops.GraphKeys.MODEL_VARIABLES</code>.</li>
<li><b><code>trainable</code></b>: If <code>True</code> also add the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see <a href="../../../../tf/Variable.html"><code>tf.Variable</code></a>).</li>
<li><b><code>cols_to_vars</code></b>: If not <code>None</code>, must be a dictionary that will be filled with a mapping from <code>_FeatureColumn</code> to associated list of <code>Variable</code>s. For example, after the call, we might have cols_to_vars = { _NumericColumn( key='numeric_feature1', shape=(1,): [&lt;tf.Variable 'linear_model/price2/weights:0' shape=(1, 1)&gt;], 'bias': [&lt;tf.Variable 'linear_model/bias_weights:0' shape=(1,)&gt;], _NumericColumn( key='numeric_feature2', shape=(2,)): [&lt;tf.Variable 'linear_model/price1/weights:0' shape=(2, 1)&gt;]} If a column creates no variables, its value will be an empty list. Note that cols_to_vars will also contain a string key 'bias' that maps to a list of Variables.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> which represents predictions/logits of a linear model. Its shape is (batch_size, units) and its dtype is <code>float32</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if an item in <code>feature_columns</code> is neither a <code>_DenseColumn</code> nor <code>_CategoricalColumn</code>.</li>
</ul>
