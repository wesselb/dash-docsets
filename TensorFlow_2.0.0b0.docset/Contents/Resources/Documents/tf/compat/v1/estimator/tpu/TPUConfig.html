<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.compat.v1.estimator.tpu.TPUConfig" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="iterations_per_loop"/> <meta itemprop="property" content="num_shards"/> <meta itemprop="property" content="num_cores_per_replica"/> <meta itemprop="property" content="per_host_input_for_training"/> <meta itemprop="property" content="tpu_job_name"/> <meta itemprop="property" content="initial_infeed_sleep_secs"/> <meta itemprop="property" content="input_partition_dims"/> <meta itemprop="property" content="eval_training_input_configuration"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.compat.v1.estimator.tpu.TPUConfig" class="dashAnchor"></a><h1 id="tf.compat.v1.estimator.tpu.tpuconfig">tf.compat.v1.estimator.tpu.TPUConfig</h1>
<h2 id="class-tpuconfig">Class <code>TPUConfig</code></h2>
<p>TPU related configuration required by <code>TPUEstimator</code>.</p>
<p>Defined in <a href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/tpu/tpu_config.py"><code>python/estimator/tpu/tpu_config.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<h4 id="args">Args:</h4>
<ul>
<li><b><code>iterations_per_loop</code></b>: This is the number of train steps running in TPU system before returning to CPU host for each <code>Session.run</code>. This means global step is increased <code>iterations_per_loop</code> times in one <code>Session.run</code>. It is recommended to be set as number of global steps for next checkpoint. Note that in evaluation don't use this value, instead we run total eval <code>steps</code> on TPU for a single <code>Session.run</code>. [Experimental]: <code>iterations_per_loop</code> can be specified as a time interval. To specify N seconds in one <code>Session.run</code>, one can specify it as <code>Ns</code> and substitute the N with the N with the number of desired seconds. Alternatively, the unit of time can also be specified in minutes or hours, e.g. <code>3600s</code> or <code>60m</code> or <code>1h</code>.</li>
<li><b><code>num_shards</code></b>: (Deprecated, ignored by TPUEstimator). The number of model replicas in the system. For non-model-parallelism case, this number equals the total number of TPU cores. For model-parallelism, the total number of TPU cores equals num_cores_per_replica * num_shards.</li>
<li><b><code>num_cores_per_replica</code></b>: Defaults to <code>None</code>, which disables model parallelism. An integer which describes the number of TPU cores per model replica. This is required by model-parallelism which enables partitioning the model to multiple cores. Currently num_cores_per_replica must be 1, 2, 4, or 8.</li>
<li><b><code>per_host_input_for_training</code></b>: If <code>True</code>, <code>PER_HOST_V1</code>, or <code>PER_HOST_V2</code>, <code>input_fn</code> is invoked once on each host. With the per-core input pipeline configuration, it is invoked once for each core. With a global batch size <code>train_batch_size</code> in <code>TPUEstimator</code> constructor, the batch size for each shard is <code>train_batch_size</code> // #hosts in the <code>True</code> or <code>PER_HOST_V1</code> mode. In <code>PER_HOST_V2</code> mode, it is <code>train_batch_size</code> // #cores. In <code>BROADCAST</code> mode, <code>input_fn</code> is only invoked once on host 0 and the tensors are broadcasted to all other replicas. The batch size equals to <code>train_batch_size</code>. With the per-core input pipeline configuration, the shard batch size is also <code>train_batch_size</code> // #cores. Note: per_host_input_for_training==PER_SHARD_V1 only supports mode.TRAIN.</li>
<li><b><code>tpu_job_name</code></b>: The name of the TPU job. Typically, this name is auto-inferred within TPUEstimator, however when using ClusterSpec propagation in more esoteric cluster configurations, you may need to specify the job name as a string.</li>
<li><b><code>initial_infeed_sleep_secs</code></b>: The number of seconds the infeed thread should wait before enqueueing the first batch. This helps avoid timeouts for models that require a long compilation time.</li>
<li><b><code>input_partition_dims</code></b>: A nested list to describe the partition dims for all the tensors from input_fn(). The structure of input_partition_dims must match the structure of <code>features</code> and <code>labels</code> from input_fn(). The total number of partitions must match <code>num_cores_per_replica</code>. For example, if input_fn() returns two tensors: images with shape [N, H, W, C] and labels [N]. input_partition_dims = [[1, 2, 2, 1], None] will split the images to 4 pieces and feed into 4 TPU cores. labels tensor are directly broadcasted to all the TPU cores since the partition dims is <code>None</code>. Current limitations: This feature is only supported with the PER_HOST_V2 input mode.</li>
<li><b><code>eval_training_input_configuration</code></b>: If <code>SLICED</code>, <code>input_fn</code> is only invoked once on host 0 and the tensors are broadcasted to all other replicas. Unlike per_host_input_for_training=BROADCAST, each replica will only get a slice of the data instead of a whole copy. If <code>PER_HOST_V1</code>, the behaviour is determined by per_host_input_for_training.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>num_cores_per_replica</code> is not 1, 2, 4, 8 or 16.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="iterations_per_loop">
<code>iterations_per_loop</code>
</h3>
<h3 id="num_shards">
<code>num_shards</code>
</h3>
<h3 id="num_cores_per_replica">
<code>num_cores_per_replica</code>
</h3>
<h3 id="per_host_input_for_training">
<code>per_host_input_for_training</code>
</h3>
<h3 id="tpu_job_name">
<code>tpu_job_name</code>
</h3>
<h3 id="initial_infeed_sleep_secs">
<code>initial_infeed_sleep_secs</code>
</h3>
<h3 id="input_partition_dims">
<code>input_partition_dims</code>
</h3>
<h3 id="eval_training_input_configuration">
<code>eval_training_input_configuration</code>
</h3>
