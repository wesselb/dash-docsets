<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.ConvLSTM2D" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="activation"/> <meta itemprop="property" content="bias_constraint"/> <meta itemprop="property" content="bias_initializer"/> <meta itemprop="property" content="bias_regularizer"/> <meta itemprop="property" content="data_format"/> <meta itemprop="property" content="dilation_rate"/> <meta itemprop="property" content="dropout"/> <meta itemprop="property" content="filters"/> <meta itemprop="property" content="kernel_constraint"/> <meta itemprop="property" content="kernel_initializer"/> <meta itemprop="property" content="kernel_regularizer"/> <meta itemprop="property" content="kernel_size"/> <meta itemprop="property" content="padding"/> <meta itemprop="property" content="recurrent_activation"/> <meta itemprop="property" content="recurrent_constraint"/> <meta itemprop="property" content="recurrent_dropout"/> <meta itemprop="property" content="recurrent_initializer"/> <meta itemprop="property" content="recurrent_regularizer"/> <meta itemprop="property" content="states"/> <meta itemprop="property" content="strides"/> <meta itemprop="property" content="unit_forget_bias"/> <meta itemprop="property" content="use_bias"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="get_initial_state"/> <meta itemprop="property" content="reset_states"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.ConvLSTM2D" class="dashAnchor"></a><h1 id="tf.keras.layers.convlstm2d">tf.keras.layers.ConvLSTM2D</h1>
<h2 id="class-convlstm2d">Class <code>ConvLSTM2D</code></h2>
<p>Convolutional LSTM.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.ConvLSTM2D</code></li>
<li>Class <code>tf.compat.v2.keras.layers.ConvLSTM2D</code></li>
<li>Class <code>tf.keras.layers.ConvLSTM2D</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/convolutional_recurrent.py"><code>python/keras/layers/convolutional_recurrent.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>It is similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>filters</code></b>: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</li>
<li><b><code>kernel_size</code></b>: An integer or tuple/list of n integers, specifying the dimensions of the convolution window.</li>
<li><b><code>strides</code></b>: An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any <code>dilation_rate</code> value != 1.</li>
<li><b><code>padding</code></b>: One of <code>&quot;valid&quot;</code> or <code>&quot;same&quot;</code> (case-insensitive).</li>
<li><b><code>data_format</code></b>: A string, one of <code>channels_last</code> (default) or <code>channels_first</code>. The ordering of the dimensions in the inputs. <code>channels_last</code> corresponds to inputs with shape <code>(batch, time, ..., channels)</code> while <code>channels_first</code> corresponds to inputs with shape <code>(batch, time, channels, ...)</code>. It defaults to the <code>image_data_format</code> value found in your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then it will be &quot;channels_last&quot;.</li>
<li><b><code>dilation_rate</code></b>: An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any <code>dilation_rate</code> value != 1 is incompatible with specifying any <code>strides</code> value != 1.</li>
<li><b><code>activation</code></b>: Activation function to use. If you don't specify anything, no activation is applied (ie. &quot;linear&quot; activation: <code>a(x) = x</code>).</li>
<li><b><code>recurrent_activation</code></b>: Activation function to use for the recurrent step.</li>
<li><b><code>use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li>
<li><b><code>kernel_initializer</code></b>: Initializer for the <code>kernel</code> weights matrix, used for the linear transformation of the inputs.</li>
<li><b><code>recurrent_initializer</code></b>: Initializer for the <code>recurrent_kernel</code> weights matrix, used for the linear transformation of the recurrent state.</li>
<li><b><code>bias_initializer</code></b>: Initializer for the bias vector.</li>
<li><b><code>unit_forget_bias</code></b>: Boolean. If True, add 1 to the bias of the forget gate at initialization. Use in combination with <code>bias_initializer=&quot;zeros&quot;</code>. This is recommended in [Jozefowicz et al.] (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)</li>
<li><b><code>kernel_regularizer</code></b>: Regularizer function applied to the <code>kernel</code> weights matrix.</li>
<li><b><code>recurrent_regularizer</code></b>: Regularizer function applied to the <code>recurrent_kernel</code> weights matrix.</li>
<li><b><code>bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li>
<li><b><code>activity_regularizer</code></b>: Regularizer function applied to.</li>
<li><b><code>kernel_constraint</code></b>: Constraint function applied to the <code>kernel</code> weights matrix.</li>
<li><b><code>recurrent_constraint</code></b>: Constraint function applied to the <code>recurrent_kernel</code> weights matrix.</li>
<li><b><code>bias_constraint</code></b>: Constraint function applied to the bias vector.</li>
<li><b><code>return_sequences</code></b>: Boolean. Whether to return the last output in the output sequence, or the full sequence.</li>
<li><b><code>go_backwards</code></b>: Boolean (default False). If True, process the input sequence backwards.</li>
<li><b><code>stateful</code></b>: Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.</li>
<li><b><code>dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.</li>
<li><b><code>recurrent_dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.</li>
</ul>
<h4 id="call-arguments">Call arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: A 5D tensor.</li>
<li><b><code>mask</code></b>: Binary tensor of shape <code>(samples, timesteps)</code> indicating whether a given timestep should be masked.</li>
<li><b><code>training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the cell when calling it. This is only relevant if <code>dropout</code> or <code>recurrent_dropout</code> are set.</li>
<li><b><code>initial_state</code></b>: List of initial state tensors to be passed to the first call of the cell.</li>
</ul>
<h4 id="input-shape">Input shape:</h4>
<ul>
<li>If data_format='channels_first' 5D tensor with shape: <code>(samples, time, channels, rows, cols)</code></li>
<li>If data_format='channels_last' 5D tensor with shape: <code>(samples, time, rows, cols, channels)</code></li>
</ul>
<h4 id="output-shape">Output shape:</h4>
<ul>
<li>If <code>return_sequences</code></li>
<li>If data_format='channels_first' 5D tensor with shape: <code>(samples, time, filters, output_row, output_col)</code></li>
<li>If data_format='channels_last' 5D tensor with shape: <code>(samples, time, output_row, output_col, filters)</code></li>
<li>Else</li>
<li>If data_format ='channels_first' 4D tensor with shape: <code>(samples, filters, output_row, output_col)</code></li>
<li>If data_format='channels_last' 4D tensor with shape: <code>(samples, output_row, output_col, filters)</code> where <code>o_row</code> and <code>o_col</code> depend on the shape of the filter and the padding</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: in case of invalid constructor arguments.</li>
</ul>
<h4 id="references">References:</h4>
<ul>
<li><a href="http://arxiv.org/abs/1506.04214v1">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a> The current implementation does not include the feedback loop on the cells output.</li>
</ul>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    filters,
    kernel_size,
    strides<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>),
    padding<span class="op">=</span><span class="st">&#39;valid&#39;</span>,
    data_format<span class="op">=</span><span class="va">None</span>,
    dilation_rate<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>),
    activation<span class="op">=</span><span class="st">&#39;tanh&#39;</span>,
    recurrent_activation<span class="op">=</span><span class="st">&#39;hard_sigmoid&#39;</span>,
    use_bias<span class="op">=</span><span class="va">True</span>,
    kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>,
    recurrent_initializer<span class="op">=</span><span class="st">&#39;orthogonal&#39;</span>,
    bias_initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    unit_forget_bias<span class="op">=</span><span class="va">True</span>,
    kernel_regularizer<span class="op">=</span><span class="va">None</span>,
    recurrent_regularizer<span class="op">=</span><span class="va">None</span>,
    bias_regularizer<span class="op">=</span><span class="va">None</span>,
    activity_regularizer<span class="op">=</span><span class="va">None</span>,
    kernel_constraint<span class="op">=</span><span class="va">None</span>,
    recurrent_constraint<span class="op">=</span><span class="va">None</span>,
    bias_constraint<span class="op">=</span><span class="va">None</span>,
    return_sequences<span class="op">=</span><span class="va">False</span>,
    go_backwards<span class="op">=</span><span class="va">False</span>,
    stateful<span class="op">=</span><span class="va">False</span>,
    dropout<span class="op">=</span><span class="fl">0.0</span>,
    recurrent_dropout<span class="op">=</span><span class="fl">0.0</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h2 id="properties">Properties</h2>
<h3 id="activation">
<code>activation</code>
</h3>
<h3 id="bias_constraint">
<code>bias_constraint</code>
</h3>
<h3 id="bias_initializer">
<code>bias_initializer</code>
</h3>
<h3 id="bias_regularizer">
<code>bias_regularizer</code>
</h3>
<h3 id="data_format">
<code>data_format</code>
</h3>
<h3 id="dilation_rate">
<code>dilation_rate</code>
</h3>
<h3 id="dropout">
<code>dropout</code>
</h3>
<h3 id="filters">
<code>filters</code>
</h3>
<h3 id="kernel_constraint">
<code>kernel_constraint</code>
</h3>
<h3 id="kernel_initializer">
<code>kernel_initializer</code>
</h3>
<h3 id="kernel_regularizer">
<code>kernel_regularizer</code>
</h3>
<h3 id="kernel_size">
<code>kernel_size</code>
</h3>
<h3 id="padding">
<code>padding</code>
</h3>
<h3 id="recurrent_activation">
<code>recurrent_activation</code>
</h3>
<h3 id="recurrent_constraint">
<code>recurrent_constraint</code>
</h3>
<h3 id="recurrent_dropout">
<code>recurrent_dropout</code>
</h3>
<h3 id="recurrent_initializer">
<code>recurrent_initializer</code>
</h3>
<h3 id="recurrent_regularizer">
<code>recurrent_regularizer</code>
</h3>
<h3 id="states">
<code>states</code>
</h3>
<h3 id="strides">
<code>strides</code>
</h3>
<h3 id="unit_forget_bias">
<code>unit_forget_bias</code>
</h3>
<h3 id="use_bias">
<code>use_bias</code>
</h3>
<h2 id="methods">Methods</h2>
<h3 id="get_initial_state">
<code>get_initial_state</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_initial_state(inputs)</code></pre></div>
<h3 id="reset_states">
<code>reset_states</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">reset_states(states<span class="op">=</span><span class="va">None</span>)</code></pre></div>
