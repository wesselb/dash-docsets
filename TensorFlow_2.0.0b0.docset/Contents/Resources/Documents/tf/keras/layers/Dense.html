<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.Dense" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.Dense" class="dashAnchor"></a><h1 id="tf.keras.layers.dense">tf.keras.layers.Dense</h1>
<h2 id="class-dense">Class <code>Dense</code></h2>
<p>Just your regular densely-connected NN layer.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.Dense</code></li>
<li>Class <code>tf.compat.v2.keras.layers.Dense</code></li>
<li>Class <code>tf.keras.layers.Dense</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/core.py"><code>python/keras/layers/core.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p><code>Dense</code> implements the operation: <code>output = activation(dot(input, kernel) + bias)</code> where <code>activation</code> is the element-wise activation function passed as the <code>activation</code> argument, <code>kernel</code> is a weights matrix created by the layer, and <code>bias</code> is a bias vector created by the layer (only applicable if <code>use_bias</code> is <code>True</code>).</p>
<p>Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with <code>kernel</code>.</p>
<h4 id="example">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># as first layer in a sequential model:</span>
model <span class="op">=</span> Sequential()
model.add(Dense(<span class="dv">32</span>, input_shape<span class="op">=</span>(<span class="dv">16</span>,)))
<span class="co"># now the model will take as input arrays of shape (*, 16)</span>
<span class="co"># and output arrays of shape (*, 32)</span>

<span class="co"># after the first layer, you don&#39;t need to specify</span>
<span class="co"># the size of the input anymore:</span>
model.add(Dense(<span class="dv">32</span>))</code></pre></div>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>units</code></b>: Positive integer, dimensionality of the output space.</li>
<li><b><code>activation</code></b>: Activation function to use. If you don't specify anything, no activation is applied (ie. &quot;linear&quot; activation: <code>a(x) = x</code>).</li>
<li><b><code>use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li>
<li><b><code>kernel_initializer</code></b>: Initializer for the <code>kernel</code> weights matrix.</li>
<li><b><code>bias_initializer</code></b>: Initializer for the bias vector.</li>
<li><b><code>kernel_regularizer</code></b>: Regularizer function applied to the <code>kernel</code> weights matrix.</li>
<li><b><code>bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li>
<li><b><code>activity_regularizer</code></b>: Regularizer function applied to the output of the layer (its &quot;activation&quot;)..</li>
<li><b><code>kernel_constraint</code></b>: Constraint function applied to the <code>kernel</code> weights matrix.</li>
<li><b><code>bias_constraint</code></b>: Constraint function applied to the bias vector.</li>
</ul>
<h4 id="input-shape">Input shape:</h4>
<p>N-D tensor with shape: <code>(batch_size, ..., input_dim)</code>. The most common situation would be a 2D input with shape <code>(batch_size, input_dim)</code>.</p>
<h4 id="output-shape">Output shape:</h4>
<p>N-D tensor with shape: <code>(batch_size, ..., units)</code>. For instance, for a 2D input with shape <code>(batch_size, input_dim)</code>, the output would have shape <code>(batch_size, units)</code>.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    units,
    activation<span class="op">=</span><span class="va">None</span>,
    use_bias<span class="op">=</span><span class="va">True</span>,
    kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>,
    bias_initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    kernel_regularizer<span class="op">=</span><span class="va">None</span>,
    bias_regularizer<span class="op">=</span><span class="va">None</span>,
    activity_regularizer<span class="op">=</span><span class="va">None</span>,
    kernel_constraint<span class="op">=</span><span class="va">None</span>,
    bias_constraint<span class="op">=</span><span class="va">None</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
