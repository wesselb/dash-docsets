<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.Lambda" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.Lambda" class="dashAnchor"></a><h1 id="tf.keras.layers.lambda">tf.keras.layers.Lambda</h1>
<h2 id="class-lambda">Class <code>Lambda</code></h2>
<p>Wraps arbitrary expressions as a <code>Layer</code> object.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.Lambda</code></li>
<li>Class <code>tf.compat.v2.keras.layers.Lambda</code></li>
<li>Class <code>tf.keras.layers.Lambda</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/core.py"><code>python/keras/layers/core.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>The <code>Lambda</code> layer exists so that arbitrary TensorFlow functions can be used when constructing <code>Sequential</code> and Functional API models. <code>Lambda</code> layers are best suited for simple operations or quick experimentation. For more advanced use cases, subclassing <code>keras.layers.Layer</code> is preferred. One reason for this is that when saving a Model, <code>Lambda</code> layers are saved by serializing the Python bytecode, whereas subclassed Layers are saved via overriding their <code>get_config</code> method and are thus more portable. Models that rely on subclassed Layers are also often easier to visualize and reason about.</p>
<h4 id="examples">Examples:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># add a x -&gt; x^2 layer</span>
model.add(Lambda(<span class="kw">lambda</span> x: x <span class="op">**</span> <span class="dv">2</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># add a layer that returns the concatenation</span>
<span class="co"># of the positive part of the input and</span>
<span class="co"># the opposite of the negative part</span>

<span class="kw">def</span> antirectifier(x):
    x <span class="op">-=</span> K.mean(x, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)
    x <span class="op">=</span> K.l2_normalize(x, axis<span class="op">=</span><span class="dv">1</span>)
    pos <span class="op">=</span> K.relu(x)
    neg <span class="op">=</span> K.relu(<span class="op">-</span>x)
    <span class="cf">return</span> K.concatenate([pos, neg], axis<span class="op">=</span><span class="dv">1</span>)

model.add(Lambda(antirectifier))</code></pre></div>
<p>Variables can be created within a <code>Lambda</code> layer. Like with other layers, these variables will be created only once and reused if the <code>Lambda</code> layer is called on new inputs. If creating more than one variable in a given <code>Lambda</code> instance, be sure to use a different name for each variable. Note that calling sublayers from within a <code>Lambda</code> is not supported.</p>
<p>Example of variable creation:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> linear_transform(x):
  v1 <span class="op">=</span> tf.Variable(<span class="dv">1</span>., name<span class="op">=</span><span class="st">&#39;multiplier&#39;</span>)
  v2 <span class="op">=</span> tf.Variable(<span class="dv">0</span>., name<span class="op">=</span><span class="st">&#39;bias&#39;</span>)
  <span class="cf">return</span> x<span class="op">*</span>v1 <span class="op">+</span> v2

linear_layer <span class="op">=</span> Lambda(linear_transform)
model.add(linear_layer)
model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))
model.add(linear_layer)  <span class="co"># Reuses existing Variables</span></code></pre></div>
<p>Note that creating two instances of <code>Lambda</code> using the same function will <em>not</em> share Variables between the two instances. Each instance of <code>Lambda</code> will create and manage its own weights.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>function</code></b>: The function to be evaluated. Takes input tensor as first argument.</li>
<li><b><code>output_shape</code></b>: Expected output shape from function. This argument can be inferred if not explicitly provided. Can be a tuple or function. If a tuple, it only specifies the first dimension onward; sample dimension is assumed either the same as the input: <code>output_shape = (input_shape[0], ) + output_shape</code> or, the input is <code>None</code> and the sample dimension is also <code>None</code>: <code>output_shape = (None, ) + output_shape</code> If a function, it specifies the entire shape as a function of the input shape: <code>output_shape = f(input_shape)</code></li>
<li><b><code>mask</code></b>: Either None (indicating no masking) or a callable with the same signature as the <code>compute_mask</code> layer method, or a tensor that will be returned as output mask regardless what the input is.</li>
<li><b><code>arguments</code></b>: Optional dictionary of keyword arguments to be passed to the function. Input shape: Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape: Specified by <code>output_shape</code> argument</li>
</ul>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    function,
    output_shape<span class="op">=</span><span class="va">None</span>,
    mask<span class="op">=</span><span class="va">None</span>,
    arguments<span class="op">=</span><span class="va">None</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
