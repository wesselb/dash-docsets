<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.backend.batch_dot" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.keras.backend.batch_dot" class="dashAnchor"></a><h1 id="tf.keras.backend.batch_dot">tf.keras.backend.batch_dot</h1>
<p>Batchwise dot product.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.keras.backend.batch_dot</code></li>
<li><code>tf.compat.v2.keras.backend.batch_dot</code></li>
<li><code>tf.keras.backend.batch_dot</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.keras.backend.batch_dot(
    x,
    y,
    axes<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/backend.py"><code>python/keras/backend.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p><code>batch_dot</code> is used to compute dot product of <code>x</code> and <code>y</code> when <code>x</code> and <code>y</code> are data in batch, i.e. in a shape of <code>(batch_size, :)</code>. <code>batch_dot</code> results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use <code>expand_dims</code> to make sure that ndim is at least 2.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>x</code></b>: Keras tensor or variable with <code>ndim &gt;= 2</code>.</li>
<li><b><code>y</code></b>: Keras tensor or variable with <code>ndim &gt;= 2</code>.</li>
<li><b><code>axes</code></b>: list of (or single) int with target dimensions. The lengths of <code>axes[0]</code> and <code>axes[1]</code> should be the same.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor with shape equal to the concatenation of <code>x</code>'s shape (less the dimension that was summed over) and <code>y</code>'s shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to <code>(batch_size, 1)</code>.</p>
<h4 id="examples">Examples:</h4>
<p>Assume <code>x = [[1, 2], [3, 4]]</code> and <code>y = [[5, 6], [7, 8]]</code> <code>batch_dot(x, y, axes=1) = [[17, 53]]</code> which is the main diagonal of <code>x.dot(y.T)</code>, although we never have to calculate the off-diagonal elements.</p>
<p>Shape inference: Let <code>x</code>'s shape be <code>(100, 20)</code> and <code>y</code>'s shape be <code>(100, 30, 20)</code>. If <code>axes</code> is (1, 2), to find the output shape of resultant tensor, loop through each dimension in <code>x</code>'s shape and <code>y</code>'s shape:</p>
<ul>
<li><code>x.shape[0]</code> : 100 : append to output shape</li>
<li><code>x.shape[1]</code> : 20 : do not append to output shape, dimension 1 of <code>x</code> has been summed over. (<code>dot_axes[0]</code> = 1)</li>
<li><code>y.shape[0]</code> : 100 : do not append to output shape, always ignore first dimension of <code>y</code></li>
<li><code>y.shape[1]</code> : 30 : append to output shape</li>
<li><code>y.shape[2]</code> : 20 : do not append to output shape, dimension 2 of <code>y</code> has been summed over. (<code>dot_axes[1]</code> = 2) <code>output_shape</code> = <code>(100, 30)</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="op">&gt;&gt;&gt;</span> x_batch <span class="op">=</span> K.ones(shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">20</span>, <span class="dv">1</span>))
    <span class="op">&gt;&gt;&gt;</span> y_batch <span class="op">=</span> K.ones(shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">30</span>, <span class="dv">20</span>))
    <span class="op">&gt;&gt;&gt;</span> xy_batch_dot <span class="op">=</span> K.batch_dot(x_batch, y_batch, axes<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>])
    <span class="op">&gt;&gt;&gt;</span> K.int_shape(xy_batch_dot)
    (<span class="dv">32</span>, <span class="dv">1</span>, <span class="dv">30</span>)</code></pre></div>
