<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.optimizers.Nadam" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="iterations"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_slot"/> <meta itemprop="property" content="add_weight"/> <meta itemprop="property" content="apply_gradients"/> <meta itemprop="property" content="from_config"/> <meta itemprop="property" content="get_config"/> <meta itemprop="property" content="get_gradients"/> <meta itemprop="property" content="get_slot"/> <meta itemprop="property" content="get_slot_names"/> <meta itemprop="property" content="get_updates"/> <meta itemprop="property" content="get_weights"/> <meta itemprop="property" content="minimize"/> <meta itemprop="property" content="set_weights"/> <meta itemprop="property" content="variables"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.optimizers.Nadam" class="dashAnchor"></a><h1 id="tf.keras.optimizers.nadam">tf.keras.optimizers.Nadam</h1>
<h2 id="class-nadam">Class <code>Nadam</code></h2>
<p>Optimizer that implements the NAdam algorithm.</p>
<p>Inherits From: <a href="../../../tf/keras/optimizers/Optimizer.html"><code>Optimizer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.optimizers.Nadam</code></li>
<li>Class <code>tf.compat.v2.keras.optimizers.Nadam</code></li>
<li>Class <code>tf.compat.v2.optimizers.Nadam</code></li>
<li>Class <code>tf.keras.optimizers.Nadam</code></li>
<li>Class <code>tf.optimizers.Nadam</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/optimizer_v2/nadam.py"><code>python/keras/optimizer_v2/nadam.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>Much like Adam is essentially RMSprop with momentum, Nadam is Adam with Nesterov momentum.</p>
<h4 id="initialization">Initialization:</h4>
<p><br /><span class="math display"><em>m</em><sub>0</sub> := 0(Initialize 1st moment vector)</span><br /> <br /><span class="math display"><em>v</em><sub>0</sub> := 0(Initialize 2nd moment vector)</span><br /> <br /><span class="math display"><em>m</em><em>u</em><sub>0</sub> := 1</span><br /> <br /><span class="math display"><em>t</em> := 0(Initialize timestep)</span><br /></p>
<h4 id="computes">Computes:</h4>
<p><br /><span class="math display"><em>t</em> := <em>t</em> + 1</span><br /> <br /><span class="math display"><em>μ</em><sub><em>t</em></sub> := <em>β</em><sub>1</sub> * (1 − 0.5 * 0.96<sup>0.004 * <em>t</em></sup>)</span><br /> <br /><span class="math display">$$g' := g / (1 - \prod_{i=1}^{t}{\mu_i})$$</span><br /> <br /><span class="math display"><em>m</em><sub><em>t</em></sub> := <em>β</em><sub>1</sub> * <em>m</em><sub><em>t</em> − 1</sub> + (1 − <em>β</em><sub>1</sub>)*<em>g</em></span><br /> <br /><span class="math display">$$m' := m_t / (1 - \prod_{i=1}^{t+1}{\mu_i})$$</span><br /> <br /><span class="math display"><em>v</em><sub><em>t</em></sub> := <em>β</em><sub>2</sub> * <em>v</em><sub><em>t</em> − 1</sub> + (1 − <em>β</em><sub>2</sub>)*<em>g</em> * <em>g</em></span><br /> <br /><span class="math display"><em>v</em>′:=<em>v</em><sub><em>t</em></sub>/(1 − <em>β</em><sub>2</sub><sup><em>t</em></sup>)</span><br /> <br /><span class="math display">$$\bar{m} := (1 - \mu_t) * g' + \mu_{t+1} * m'$$</span><br /> <br /><span class="math display">$$\theta_t := \theta_{t-1} - lr * \bar{m} / (\sqrt{v'} + \epsilon)$$</span><br /></p>
<p>gradient is evaluated at theta(t) + momentum * v(t), and the variables always store theta + beta_1 * m / sqrt(v) instead of theta.</p>
<p>References See <a href="http://cs229.stanford.edu/proj2015/054_report.pdf">Dozat, T., 2015</a>.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    learning_rate<span class="op">=</span><span class="fl">0.001</span>,
    beta_1<span class="op">=</span><span class="fl">0.9</span>,
    beta_2<span class="op">=</span><span class="fl">0.999</span>,
    epsilon<span class="op">=</span><span class="fl">1e-07</span>,
    name<span class="op">=</span><span class="st">&#39;Nadam&#39;</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Construct a new Nadam optimizer.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>learning_rate</code></b>: A Tensor or a floating point value. The learning rate.</li>
<li><b><code>beta_1</code></b>: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.</li>
<li><b><code>beta_2</code></b>: A float value or a constant float tensor. The exponential decay rate for the exponentially weighted infinity norm.</li>
<li><b><code>epsilon</code></b>: A small constant for numerical stability.</li>
<li><b><code>name</code></b>: Optional name for the operations created when applying gradients. Defaults to &quot;Adamax&quot;.</li>
<li><b><code>**kwargs</code></b>: keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use <code>learning_rate</code> instead.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="iterations">
<code>iterations</code>
</h3>
<p>Variable. The number of training steps this Optimizer has run.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns variables of this Optimizer based on the order created.</p>
<h2 id="methods">Methods</h2>
<h3 id="add_slot">
<code>add_slot</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_slot(
    var,
    slot_name,
    initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>
)</code></pre></div>
<p>Add a new slot variable for <code>var</code>.</p>
<h3 id="add_weight">
<code>add_weight</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_weight(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    trainable<span class="op">=</span><span class="va">None</span>,
    synchronization<span class="op">=</span>tf.VariableSynchronization.AUTO,
    aggregation<span class="op">=</span>tf.compat.v1.VariableAggregation.NONE
)</code></pre></div>
<h3 id="apply_gradients">
<code>apply_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">apply_gradients(
    grads_and_vars,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Apply gradients to variables.</p>
<p>This is the second part of <code>minimize()</code>. It returns an <code>Operation</code> that applies gradients.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>grads_and_vars</code></b>: List of (gradient, variable) pairs.</li>
<li><b><code>name</code></b>: Optional name for the returned operation. Default to the name passed to the <code>Optimizer</code> constructor.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>An <code>Operation</code> that applies the specified gradients. If <code>global_step</code> was not None, that operation also increments <code>global_step</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>grads_and_vars</code> is malformed.</li>
<li><b><code>ValueError</code></b>: If none of the variables have gradients.</li>
</ul>
<h3 id="from_config">
<code>from_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">from_config(
    cls,
    config,
    custom_objects<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Creates an optimizer from its config.</p>
<p>This method is the reverse of <code>get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>config</code></b>: A Python dictionary, typically the output of get_config.</li>
<li><b><code>custom_objects</code></b>: A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>An optimizer instance.</p>
<h3 id="get_config">
<code>get_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_config()</code></pre></div>
<p>Returns the config of the optimimizer.</p>
<p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p>
<h4 id="returns-2">Returns:</h4>
<p>Python dictionary.</p>
<h3 id="get_gradients">
<code>get_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_gradients(
    loss,
    params
)</code></pre></div>
<p>Returns gradients of <code>loss</code> with respect to <code>params</code>.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>loss</code></b>: Loss tensor.</li>
<li><b><code>params</code></b>: List of variables.</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>List of gradient tensors.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient function not implemented).</li>
</ul>
<h3 id="get_slot">
<code>get_slot</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_slot(
    var,
    slot_name
)</code></pre></div>
<h3 id="get_slot_names">
<code>get_slot_names</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_slot_names()</code></pre></div>
<p>A list of names for this optimizer's slots.</p>
<h3 id="get_updates">
<code>get_updates</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates(
    loss,
    params
)</code></pre></div>
<h3 id="get_weights">
<code>get_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_weights()</code></pre></div>
<h3 id="minimize">
<code>minimize</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">minimize(
    loss,
    var_list,
    grad_loss<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Minimize <code>loss</code> by updating <code>var_list</code>.</p>
<p>This method simply computes gradient using <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and calls <code>apply_gradients()</code>. If you want to process the gradient before applying then call <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and <code>apply_gradients()</code> explicitly instead of using this function.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>loss</code></b>: A callable taking no arguments which returns the value to minimize.</li>
<li><b><code>var_list</code></b>: list or tuple of <code>Variable</code> objects to update to minimize <code>loss</code>, or a callable returning the list or tuple of <code>Variable</code> objects. Use callable when the variable list would otherwise be incomplete before <code>minimize</code> since the variables are created at the first time <code>loss</code> is called.</li>
<li><b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>An Operation that updates the variables in <code>var_list</code>. If <code>global_step</code> was not <code>None</code>, that operation also increments <code>global_step</code>.</p>
<h4 id="raises-2">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If some of the variables are not <code>Variable</code> objects.</li>
</ul>
<h3 id="set_weights">
<code>set_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_weights(weights)</code></pre></div>
<h3 id="variables">
<code>variables</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">variables()</code></pre></div>
<p>Returns variables of this Optimizer based on the order created.</p>
