<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.preprocessing.sequence.skipgrams" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.keras.preprocessing.sequence.skipgrams" class="dashAnchor"></a><h1 id="tf.keras.preprocessing.sequence.skipgrams">tf.keras.preprocessing.sequence.skipgrams</h1>
<p>Generates skipgram word pairs.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.keras.preprocessing.sequence.skipgrams</code></li>
<li><code>tf.compat.v2.keras.preprocessing.sequence.skipgrams</code></li>
<li><code>tf.keras.preprocessing.sequence.skipgrams</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.keras.preprocessing.sequence.skipgrams(
    sequence,
    vocabulary_size,
    window_size<span class="op">=</span><span class="dv">4</span>,
    negative_samples<span class="op">=</span><span class="fl">1.0</span>,
    shuffle<span class="op">=</span><span class="va">True</span>,
    categorical<span class="op">=</span><span class="va">False</span>,
    sampling_table<span class="op">=</span><span class="va">None</span>,
    seed<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<!-- Placeholder for "Used in" -->
<p>This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:</p>
<ul>
<li>(word, word in the same window), with label 1 (positive samples).</li>
<li>(word, random word from the vocabulary), with label 0 (negative samples).</li>
</ul>
<p>Read more about Skipgram in this gnomic paper by Mikolov et al.: <a href="http://arxiv.org/pdf/1301.3781v3.pdf">Efficient Estimation of Word Representations in Vector Space</a></p>
<h1 id="arguments">Arguments</h1>
<pre><code>sequence: A word sequence (sentence), encoded as a list
    of word indices (integers). If using a `sampling_table`,
    word indices are expected to match the rank
    of the words in a reference dataset (e.g. 10 would encode
    the 10-th most frequently occurring token).
    Note that index 0 is expected to be a non-word and will be skipped.
vocabulary_size: Int, maximum possible word index + 1
window_size: Int, size of sampling windows (technically half-window).
    The window of a word `w_i` will be
    `[i - window_size, i + window_size+1]`.
negative_samples: Float &gt;= 0. 0 for no negative (i.e. random) samples.
    1 for same number as positive samples.
shuffle: Whether to shuffle the word couples before returning them.
categorical: bool. if False, labels will be
    integers (eg. `[0, 1, 1 .. ]`),
    if `True`, labels will be categorical, e.g.
    `[[1,0],[0,1],[0,1] .. ]`.
sampling_table: 1D array of size `vocabulary_size` where the entry i
    encodes the probability to sample a word of rank i.
seed: Random seed.</code></pre>
<h1 id="returns">Returns</h1>
<pre><code>couples, labels: where `couples` are int pairs and
    `labels` are either 0 or 1.</code></pre>
<h1 id="note">Note</h1>
<pre><code>By convention, index 0 in the vocabulary is
a non-word and will be skipped.</code></pre>
