<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.test.create_local_cluster" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.test.create_local_cluster" class="dashAnchor"></a><h1 id="tf.test.create_local_cluster">tf.test.create_local_cluster</h1>
<p>Create and start local servers and return the associated <code>Server</code> objects.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.test.create_local_cluster</code></li>
<li><code>tf.compat.v2.test.create_local_cluster</code></li>
<li><code>tf.test.create_local_cluster</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.test.create_local_cluster(
    num_workers,
    num_ps,
    protocol<span class="op">=</span><span class="st">&#39;grpc&#39;</span>,
    worker_config<span class="op">=</span><span class="va">None</span>,
    ps_config<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/framework/test_util.py"><code>python/framework/test_util.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>&quot;PS&quot; stands for &quot;parameter server&quot;: a task responsible for storing and updating the model's parameters. Other tasks send updates to these parameters as they work on optimizing the parameters. This particular division of labor between tasks is not required, but is common for distributed training.</p>
<p>Read more at https://www.tensorflow.org/guide/extend/architecture</p>
<div class="figure">
<img src="https://www.tensorflow.org/images/diag1.svg" title="components" alt="components" />
<p class="caption">components</p>
</div>
<p>Figure illustrates the interaction of these components. &quot;/job:worker/task:0&quot; and &quot;/job:ps/task:0&quot; are both tasks with worker services.</p>
<h4 id="example">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">workers, _ <span class="op">=</span> tf.test.create_local_cluster(num_workers<span class="op">=</span><span class="dv">2</span>, num_ps<span class="op">=</span><span class="dv">2</span>)

worker_sessions <span class="op">=</span> [tf.compat.v1.Session(w.target) <span class="cf">for</span> w <span class="kw">in</span> workers]

<span class="cf">with</span> tf.device(<span class="st">&quot;/job:ps/task:0&quot;</span>):
  ...
<span class="cf">with</span> tf.device(<span class="st">&quot;/job:ps/task:1&quot;</span>):
  ...
<span class="cf">with</span> tf.device(<span class="st">&quot;/job:worker/task:0&quot;</span>):
  ...
<span class="cf">with</span> tf.device(<span class="st">&quot;/job:worker/task:1&quot;</span>):
  ...

worker_sessions[<span class="dv">0</span>].run(...)</code></pre></div>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>num_workers</code></b>: Number of worker servers to start.</li>
<li><b><code>num_ps</code></b>: Number of PS servers to start.</li>
<li><b><code>protocol</code></b>: Communication protocol. Allowed values are documented in the documentation of <a href="../../tf/distribute/Server.html"><code>tf.distribute.Server</code></a>.</li>
<li><b><code>worker_config</code></b>: (optional) <code>tf.ConfigProto</code> to initialize workers. Can be used to instantiate multiple devices etc.</li>
<li><b><code>ps_config</code></b>: (optional) <code>tf.ConfigProto</code> to initialize PS servers.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tuple <code>(worker_servers, ps_servers)</code>. <code>worker_servers</code> is a list of <code>num_workers</code> objects of type <a href="../../tf/distribute/Server.html"><code>tf.distribute.Server</code></a> (all running locally); and <code>ps_servers</code> is a list of <code>num_ps</code> objects of similar type.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ImportError</code></b>: if portpicker module was not found at load time</li>
</ul>
