<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.TimeDistributed" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.TimeDistributed" class="dashAnchor"></a><h1 id="tf.keras.layers.timedistributed">tf.keras.layers.TimeDistributed</h1>
<h2 id="class-timedistributed">Class <code>TimeDistributed</code></h2>
<p>This wrapper allows to apply a layer to every temporal slice of an input.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Wrapper.html"><code>Wrapper</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.TimeDistributed</code></li>
<li>Class <code>tf.compat.v2.keras.layers.TimeDistributed</code></li>
<li>Class <code>tf.keras.layers.TimeDistributed</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/wrappers.py"><code>python/keras/layers/wrappers.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.</p>
<p>Consider a batch of 32 samples, where each sample is a sequence of 10 vectors of 16 dimensions. The batch input shape of the layer is then <code>(32, 10, 16)</code>, and the <code>input_shape</code>, not including the samples dimension, is <code>(10, 16)</code>.</p>
<p>You can then use <code>TimeDistributed</code> to apply a <code>Dense</code> layer to each of the 10 timesteps, independently:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># as the first layer in a model</span>
model <span class="op">=</span> Sequential()
model.add(TimeDistributed(Dense(<span class="dv">8</span>), input_shape<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">16</span>)))
<span class="co"># now model.output_shape == (None, 10, 8)</span></code></pre></div>
<p>The output will then have shape <code>(32, 10, 8)</code>.</p>
<p>In subsequent layers, there is no need for the <code>input_shape</code>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model.add(TimeDistributed(Dense(<span class="dv">32</span>)))
<span class="co"># now model.output_shape == (None, 10, 32)</span></code></pre></div>
<p>The output will then have shape <code>(32, 10, 32)</code>.</p>
<p><code>TimeDistributed</code> can be used with arbitrary layers, not just <code>Dense</code>, for instance with a <code>Conv2D</code> layer:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model <span class="op">=</span> Sequential()
model.add(TimeDistributed(Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>)),
                          input_shape<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">299</span>, <span class="dv">299</span>, <span class="dv">3</span>)))</code></pre></div>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>layer</code></b>: a layer instance.</li>
</ul>
<h4 id="call-arguments">Call arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor.</li>
<li><b><code>training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the wrapped layer (only if the layer supports this argument).</li>
<li><b><code>mask</code></b>: Binary tensor of shape <code>(samples, timesteps)</code> indicating whether a given timestep should be masked. This argument is passed to the wrapped layer (only if the layer supports this argument).</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If not initialized with a <code>Layer</code> instance.</li>
</ul>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    layer,
    <span class="op">**</span>kwargs
)</code></pre></div>
