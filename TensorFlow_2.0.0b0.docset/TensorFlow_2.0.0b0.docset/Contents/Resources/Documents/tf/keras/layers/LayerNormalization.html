<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.LayerNormalization" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.LayerNormalization" class="dashAnchor"></a><h1 id="tf.keras.layers.layernormalization">tf.keras.layers.LayerNormalization</h1>
<h2 id="class-layernormalization">Class <code>LayerNormalization</code></h2>
<p>Layer normalization layer (Ba et al., 2016).</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.LayerNormalization</code></li>
<li>Class <code>tf.compat.v2.keras.layers.LayerNormalization</code></li>
<li>Class <code>tf.keras.layers.LayerNormalization</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/normalization.py"><code>python/keras/layers/normalization.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>Normalize the activations of the previous layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>axis</code></b>: Integer or List/Tuple. The axis that should be normalized (typically the features axis).</li>
<li><b><code>epsilon</code></b>: Small float added to variance to avoid dividing by zero.</li>
<li><b><code>center</code></b>: If True, add offset of <code>beta</code> to normalized tensor. If False, <code>beta</code> is ignored.</li>
<li><b><code>scale</code></b>: If True, multiply by <code>gamma</code>. If False, <code>gamma</code> is not used. When the next layer is linear (also e.g. <code>nn.relu</code>), this can be disabled since the scaling will be done by the next layer.</li>
<li><b><code>beta_initializer</code></b>: Initializer for the beta weight.</li>
<li><b><code>gamma_initializer</code></b>: Initializer for the gamma weight.</li>
<li><b><code>beta_regularizer</code></b>: Optional regularizer for the beta weight.</li>
<li><b><code>gamma_regularizer</code></b>: Optional regularizer for the gamma weight.</li>
<li><b><code>beta_constraint</code></b>: Optional constraint for the beta weight.</li>
<li><b><code>gamma_constraint</code></b>: Optional constraint for the gamma weight.</li>
<li><b><code>trainable</code></b>: Boolean, if <code>True</code> the variables will be marked as trainable.</li>
</ul>
<h4 id="input-shape">Input shape:</h4>
<p>Arbitrary. Use the keyword argument <code>input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p>
<h4 id="output-shape">Output shape:</h4>
<p>Same shape as input.</p>
<h4 id="references">References:</h4>
<ul>
<li><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></li>
</ul>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    axis<span class="op">=-</span><span class="dv">1</span>,
    epsilon<span class="op">=</span><span class="fl">0.001</span>,
    center<span class="op">=</span><span class="va">True</span>,
    scale<span class="op">=</span><span class="va">True</span>,
    beta_initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    gamma_initializer<span class="op">=</span><span class="st">&#39;ones&#39;</span>,
    beta_regularizer<span class="op">=</span><span class="va">None</span>,
    gamma_regularizer<span class="op">=</span><span class="va">None</span>,
    beta_constraint<span class="op">=</span><span class="va">None</span>,
    gamma_constraint<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
