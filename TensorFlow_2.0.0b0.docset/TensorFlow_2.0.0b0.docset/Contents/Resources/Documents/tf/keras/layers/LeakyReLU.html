<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.LeakyReLU" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.LeakyReLU" class="dashAnchor"></a><h1 id="tf.keras.layers.leakyrelu">tf.keras.layers.LeakyReLU</h1>
<h2 id="class-leakyrelu">Class <code>LeakyReLU</code></h2>
<p>Leaky version of a Rectified Linear Unit.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.LeakyReLU</code></li>
<li>Class <code>tf.compat.v2.keras.layers.LeakyReLU</code></li>
<li>Class <code>tf.keras.layers.LeakyReLU</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/advanced_activations.py"><code>python/keras/layers/advanced_activations.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>It allows a small gradient when the unit is not active: <code>f(x) = alpha * x for x &lt; 0</code>, <code>f(x) = x for x &gt;= 0</code>.</p>
<h4 id="input-shape">Input shape:</h4>
<p>Arbitrary. Use the keyword argument <code>input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p>
<h4 id="output-shape">Output shape:</h4>
<p>Same shape as the input.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>alpha</code></b>: Float &gt;= 0. Negative slope coefficient.</li>
</ul>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    alpha<span class="op">=</span><span class="fl">0.3</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
