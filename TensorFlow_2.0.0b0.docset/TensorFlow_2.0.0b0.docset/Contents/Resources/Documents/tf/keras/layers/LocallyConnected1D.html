<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.keras.layers.LocallyConnected1D" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="__init__"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.keras.layers.LocallyConnected1D" class="dashAnchor"></a><h1 id="tf.keras.layers.locallyconnected1d">tf.keras.layers.LocallyConnected1D</h1>
<h2 id="class-locallyconnected1d">Class <code>LocallyConnected1D</code></h2>
<p>Locally-connected layer for 1D inputs.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.LocallyConnected1D</code></li>
<li>Class <code>tf.compat.v2.keras.layers.LocallyConnected1D</code></li>
<li>Class <code>tf.keras.layers.LocallyConnected1D</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/keras/layers/local.py"><code>python/keras/layers/local.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>The <code>LocallyConnected1D</code> layer works similarly to the <code>Conv1D</code> layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</p>
<h4 id="example">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="co"># apply a unshared weight convolution 1d of length 3 to a sequence with</span>
    <span class="co"># 10 timesteps, with 64 output filters</span>
    model <span class="op">=</span> Sequential()
    model.add(LocallyConnected1D(<span class="dv">64</span>, <span class="dv">3</span>, input_shape<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">32</span>)))
    <span class="co"># now model.output_shape == (None, 8, 64)</span>
    <span class="co"># add a new conv1d on top</span>
    model.add(LocallyConnected1D(<span class="dv">32</span>, <span class="dv">3</span>))
    <span class="co"># now model.output_shape == (None, 6, 32)</span></code></pre></div>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>filters</code></b>: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</li>
<li><b><code>kernel_size</code></b>: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.</li>
<li><b><code>strides</code></b>: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any <code>dilation_rate</code> value != 1.</li>
<li><b><code>padding</code></b>: Currently only supports <code>&quot;valid&quot;</code> (case-insensitive). <code>&quot;same&quot;</code> may be supported in the future.</li>
<li><b><code>data_format</code></b>: A string, one of <code>channels_last</code> (default) or <code>channels_first</code>. The ordering of the dimensions in the inputs. <code>channels_last</code> corresponds to inputs with shape <code>(batch, length, channels)</code> while <code>channels_first</code> corresponds to inputs with shape <code>(batch, channels, length)</code>. It defaults to the <code>image_data_format</code> value found in your Keras config file at <code>~/.keras/keras.json</code>. If you never set it, then it will be &quot;channels_last&quot;.</li>
<li><b><code>activation</code></b>: Activation function to use. If you don't specify anything, no activation is applied (ie. &quot;linear&quot; activation: <code>a(x) = x</code>).</li>
<li><b><code>use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li>
<li><b><code>kernel_initializer</code></b>: Initializer for the <code>kernel</code> weights matrix.</li>
<li><b><code>bias_initializer</code></b>: Initializer for the bias vector.</li>
<li><b><code>kernel_regularizer</code></b>: Regularizer function applied to the <code>kernel</code> weights matrix.</li>
<li><b><code>bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li>
<li><b><code>activity_regularizer</code></b>: Regularizer function applied to the output of the layer (its &quot;activation&quot;)..</li>
<li><b><code>kernel_constraint</code></b>: Constraint function applied to the kernel matrix.</li>
<li><b><code>bias_constraint</code></b>: Constraint function applied to the bias vector.</li>
<li><p><b><code>implementation</code></b>: implementation mode, either <code>1</code> or <code>2</code>. <code>1</code> loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops.</p>
<p><code>2</code> stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops.</p>
<p>Depending on the inputs, layer parameters, hardware, and <code>tf.executing_eagerly()</code> one implementation can be dramatically faster (e.g. 50X) than another.</p>
<p>It is recommended to benchmark both in the setting of interest to pick the most efficient one (in terms of speed and memory usage).</p>
<p>Following scenarios could benefit from setting <code>implementation=2</code>: - eager execution; - inference; - running on CPU; - large amount of RAM available; - small models (few filters, small kernel); - using <code>padding=same</code> (only possible with <code>implementation=2</code>).</p></li>
</ul>
<h4 id="input-shape">Input shape:</h4>
<p>3D tensor with shape: <code>(batch_size, steps, input_dim)</code></p>
<h4 id="output-shape">Output shape:</h4>
<p>3D tensor with shape: <code>(batch_size, new_steps, filters)</code> <code>steps</code> value might have changed due to padding or strides.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    filters,
    kernel_size,
    strides<span class="op">=</span><span class="dv">1</span>,
    padding<span class="op">=</span><span class="st">&#39;valid&#39;</span>,
    data_format<span class="op">=</span><span class="va">None</span>,
    activation<span class="op">=</span><span class="va">None</span>,
    use_bias<span class="op">=</span><span class="va">True</span>,
    kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>,
    bias_initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    kernel_regularizer<span class="op">=</span><span class="va">None</span>,
    bias_regularizer<span class="op">=</span><span class="va">None</span>,
    activity_regularizer<span class="op">=</span><span class="va">None</span>,
    kernel_constraint<span class="op">=</span><span class="va">None</span>,
    bias_constraint<span class="op">=</span><span class="va">None</span>,
    implementation<span class="op">=</span><span class="dv">1</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
