<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.py_function" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.py_function" class="dashAnchor"></a><h1 id="tf.py_function">tf.py_function</h1>
<p>Wraps a python function into a TensorFlow op that executes it eagerly.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.py_function</code></li>
<li><code>tf.compat.v2.py_function</code></li>
<li><code>tf.py_function</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.py_function(
    func,
    inp,
    Tout,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/script_ops.py"><code>python/ops/script_ops.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>This function allows expressing computations in a TensorFlow graph as Python functions. In particular, it wraps a Python function <code>func</code> in a once-differentiable TensorFlow operation that executes it with eager execution enabled. As a consequence, <a href="../tf/py_function.html"><code>tf.py_function</code></a> makes it possible to express control flow using Python constructs (<code>if</code>, <code>while</code>, <code>for</code>, etc.), instead of TensorFlow control flow constructs (<a href="../tf/cond.html"><code>tf.cond</code></a>, <a href="../tf/while_loop.html"><code>tf.while_loop</code></a>). For example, you might use <a href="../tf/py_function.html"><code>tf.py_function</code></a> to implement the log huber function:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> log_huber(x, m):
  <span class="cf">if</span> tf.<span class="bu">abs</span>(x) <span class="op">&lt;=</span> m:
    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span>
  <span class="cf">else</span>:
    <span class="cf">return</span> m<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> tf.math.log(m) <span class="op">+</span> tf.math.log(x<span class="op">**</span><span class="dv">2</span>))

x <span class="op">=</span> tf.compat.v1.placeholder(tf.float32)
m <span class="op">=</span> tf.compat.v1.placeholder(tf.float32)

y <span class="op">=</span> tf.py_function(func<span class="op">=</span>log_huber, inp<span class="op">=</span>[x, m], Tout<span class="op">=</span>tf.float32)
dy_dx <span class="op">=</span> tf.gradients(y, x)[<span class="dv">0</span>]

<span class="cf">with</span> tf.compat.v1.Session() <span class="im">as</span> sess:
  <span class="co"># The session executes `log_huber` eagerly. Given the feed values below,</span>
  <span class="co"># it will take the first branch, so `y` evaluates to 1.0 and</span>
  <span class="co"># `dy_dx` evaluates to 2.0.</span>
  y, dy_dx <span class="op">=</span> sess.run([y, dy_dx], feed_dict<span class="op">=</span>{x: <span class="fl">1.0</span>, m: <span class="fl">2.0</span>})</code></pre></div>
<p>You can also use <a href="../tf/py_function.html"><code>tf.py_function</code></a> to debug your models at runtime using Python tools, i.e., you can isolate portions of your code that you want to debug, wrap them in Python functions and insert <code>pdb</code> tracepoints or print statements as desired, and wrap those functions in <a href="../tf/py_function.html"><code>tf.py_function</code></a>.</p>
<p>For more information on eager execution, see the <a href="https://tensorflow.org/guide/eager">Eager guide</a>.</p>
<p><a href="../tf/py_function.html"><code>tf.py_function</code></a> is similar in spirit to <a href="../tf/compat/v1/py_func.html"><code>tf.compat.v1.py_func</code></a>, but unlike the latter, the former lets you use TensorFlow operations in the wrapped Python function. In particular, while <a href="../tf/compat/v1/py_func.html"><code>tf.compat.v1.py_func</code></a> only runs on CPUs and wraps functions that take NumPy arrays as inputs and return NumPy arrays as outputs, <a href="../tf/py_function.html"><code>tf.py_function</code></a> can be placed on GPUs and wraps functions that take Tensors as inputs, execute TensorFlow operations in their bodies, and return Tensors as outputs.</p>
<p>Like <a href="../tf/compat/v1/py_func.html"><code>tf.compat.v1.py_func</code></a>, <a href="../tf/py_function.html"><code>tf.py_function</code></a> has the following limitations with respect to serialization and distribution:</p>
<ul>
<li><p>The body of the function (i.e. <code>func</code>) will not be serialized in a <code>GraphDef</code>. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.</p></li>
<li><p>The operation must run in the same address space as the Python program that calls <code>tf.py_function()</code>. If you are using distributed TensorFlow, you must run a <a href="../tf/distribute/Server.html"><code>tf.distribute.Server</code></a> in the same process as the program that calls <code>tf.py_function()</code> and you must pin the created operation to a device in that server (e.g. using <code>with tf.device():</code>).</p></li>
</ul>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>func</code></b>: A Python function which accepts a list of <code>Tensor</code> objects having element types that match the corresponding <a href="../tf/Tensor.html"><code>tf.Tensor</code></a> objects in <code>inp</code> and returns a list of <code>Tensor</code> objects (or a single <code>Tensor</code>, or <code>None</code>) having element types that match the corresponding values in <code>Tout</code>.</li>
<li><b><code>inp</code></b>: A list of <code>Tensor</code> objects.</li>
<li><b><code>Tout</code></b>: A list or tuple of tensorflow data types or a single tensorflow data type if there is only one, indicating what <code>func</code> returns; an empty list if no value is returned (i.e., if the return value is <code>None</code>).</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A list of <code>Tensor</code> or a single <code>Tensor</code> which <code>func</code> computes; an empty list if <code>func</code> returns None.</p>
