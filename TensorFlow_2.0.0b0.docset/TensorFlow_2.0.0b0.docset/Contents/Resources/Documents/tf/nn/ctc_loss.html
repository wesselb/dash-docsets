<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.ctc_loss" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.ctc_loss" class="dashAnchor"></a><h1 id="tf.nn.ctc_loss">tf.nn.ctc_loss</h1>
<p>Computes CTC (Connectionist Temporal Classification) loss.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.nn.ctc_loss_v2</code></li>
<li><code>tf.compat.v2.nn.ctc_loss</code></li>
<li><code>tf.nn.ctc_loss</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.nn.ctc_loss(
    labels,
    logits,
    label_length,
    logit_length,
    logits_time_major<span class="op">=</span><span class="va">True</span>,
    unique<span class="op">=</span><span class="va">None</span>,
    blank_index<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/ctc_ops.py"><code>python/ops/ctc_ops.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>This op implements the CTC loss as presented in the article:</p>
<p><a href="http://www.cs.toronto.edu/~graves/icml_2006.pdf">A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber. Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006, Pittsburgh, USA, pp. 369-376.</a></p>
<h4 id="notes">Notes:</h4>
<ul>
<li>Same as the &quot;Classic CTC&quot; in TensorFlow 1.x's tf.compat.v1.nn.ctc_loss setting of preprocess_collapse_repeated=False, ctc_merge_repeated=True</li>
<li>Labels may be supplied as either a dense, zero-padded tensor with a vector of label sequence lengths OR as a SparseTensor.</li>
<li>On TPU and GPU:
<ul>
<li>Only dense padded labels are supported.</li>
</ul></li>
<li>On CPU:
<ul>
<li>Caller may use SparseTensor or dense padded labels but calling with a SparseTensor will be significantly faster.</li>
</ul></li>
<li>Default blank label is 0 rather num_classes - 1, unless overridden by blank_index.</li>
</ul>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>labels</code></b>: tensor of shape [batch_size, max_label_seq_length] or SparseTensor</li>
<li><b><code>logits</code></b>: tensor of shape [frames, batch_size, num_labels], if logits_time_major == False, shape is [batch_size, frames, num_labels].</li>
<li><b><code>label_length</code></b>: tensor of shape [batch_size], None if labels is SparseTensor Length of reference label sequence in labels.</li>
<li><b><code>logit_length</code></b>: tensor of shape [batch_size] Length of input sequence in logits.</li>
<li><b><code>logits_time_major</code></b>: (optional) If True (default), logits is shaped [time, batch, logits]. If False, shape is [batch, time, logits]</li>
<li><b><code>unique</code></b>: (optional) Unique label indices as computed by ctc_unique_labels(labels). If supplied, enable a faster, memory efficient implementation on TPU.</li>
<li><b><code>blank_index</code></b>: (optional) Set the class index to use for the blank label. Negative values will start from num_classes, ie, -1 will reproduce the ctc_loss behavior of using num_classes - 1 for the blank symbol. There is some memory/performance overhead to switching from the default of 0 as an additional shifted copy of the logits may be created.</li>
<li><b><code>name</code></b>: A name for this <code>Op</code>. Defaults to &quot;ctc_loss_dense&quot;.</li>
</ul>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>loss</code></b>: tensor of shape [batch_size], negative log probabilities.</li>
</ul>
