<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.log_softmax" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.log_softmax" class="dashAnchor"></a><h1 id="tf.nn.log_softmax">tf.nn.log_softmax</h1>
<p>Computes log softmax activations.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v2.math.log_softmax</code></li>
<li><code>tf.compat.v2.nn.log_softmax</code></li>
<li><code>tf.math.log_softmax</code></li>
<li><code>tf.nn.log_softmax</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.nn.log_softmax(
    logits,
    axis<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/nn_ops.py"><code>python/ops/nn_ops.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>For each batch <code>i</code> and class <code>j</code> we have</p>
<pre><code>logsoftmax = logits - log(reduce_sum(exp(logits), axis))</code></pre>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>logits</code></b>: A non-empty <code>Tensor</code>. Must be one of the following types: <code>half</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>axis</code></b>: The dimension softmax would be performed on. The default is -1 which indicates the last dimension.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>InvalidArgumentError</code></b>: if <code>logits</code> is empty or <code>axis</code> is beyond the last dimension of <code>logits</code>.</li>
</ul>
