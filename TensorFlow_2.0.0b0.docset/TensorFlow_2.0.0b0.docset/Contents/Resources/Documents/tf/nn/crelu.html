<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.crelu" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.crelu" class="dashAnchor"></a><h1 id="tf.nn.crelu">tf.nn.crelu</h1>
<p>Computes Concatenated ReLU.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v2.nn.crelu</code></li>
<li><code>tf.nn.crelu</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.nn.crelu(
    features,
    axis<span class="op">=-</span><span class="dv">1</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/nn_ops.py"><code>python/ops/nn_ops.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the <em>negative</em> part of the activation. Note that as a result this non-linearity doubles the depth of the activations. Source: <a href="https://arxiv.org/abs/1603.05201">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units. W. Shang, et al.</a></p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>features</code></b>: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, or <code>int8</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
<li><b><code>axis</code></b>: The axis that the output values are concatenated along. Default is -1.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> with the same type as <code>features</code>.</p>
