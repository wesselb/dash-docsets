<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.gradients" /> <meta itemprop="path" content="Stable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.gradients" class="dashAnchor"></a><h1 id="tf.gradients">tf.gradients</h1>
<p>Constructs symbolic derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v2.gradients</code></li>
<li><code>tf.gradients</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">tf.gradients(
    ys,
    xs,
    grad_ys<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="st">&#39;gradients&#39;</span>,
    gate_gradients<span class="op">=</span><span class="va">False</span>,
    aggregation_method<span class="op">=</span><span class="va">None</span>,
    stop_gradients<span class="op">=</span><span class="va">None</span>,
    unconnected_gradients<span class="op">=</span>tf.UnconnectedGradients.NONE
)</code></pre></div>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/gradients_impl.py"><code>python/ops/gradients_impl.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p>
<p><code>gradients()</code> adds ops to the graph to output the derivatives of <code>ys</code> with respect to <code>xs</code>. It returns a list of <code>Tensor</code> of length <code>len(xs)</code> where each tensor is the <code>sum(dy/dx)</code> for y in <code>ys</code>.</p>
<p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each y in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of '1's of the shape of y for each y in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
<p><code>stop_gradients</code> is a <code>Tensor</code> or a list of tensors to be considered constant with respect to all <code>xs</code>. These tensors will not be backpropagated through, as though they had been explicitly disconnected using <code>stop_gradient</code>. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">a <span class="op">=</span> tf.constant(<span class="dv">0</span>.)
b <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> a
g <span class="op">=</span> tf.gradients(a <span class="op">+</span> b, [a, b], stop_gradients<span class="op">=</span>[a, b])</code></pre></div>
<p>Here the partial derivatives <code>g</code> evaluate to <code>[1.0, 1.0]</code>, compared to the total derivatives <code>tf.gradients(a + b, [a, b])</code>, which take into account the influence of <code>a</code> on <code>b</code> and evaluate to <code>[3.0, 1.0]</code>. Note that the above is equivalent to:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">a <span class="op">=</span> tf.stop_gradient(tf.constant(<span class="dv">0</span>.))
b <span class="op">=</span> tf.stop_gradient(<span class="dv">2</span> <span class="op">*</span> a)
g <span class="op">=</span> tf.gradients(a <span class="op">+</span> b, [a, b])</code></pre></div>
<p><code>stop_gradients</code> provides a way of stopping gradient after the graph has already been constructed, as compared to <a href="../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> which is used during graph construction. When the two approaches are combined, backpropagation stops at both <a href="../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> nodes and nodes in <code>stop_gradients</code>, whichever is encountered first.</p>
<p>All integer tensors are considered constant with respect to all <code>xs</code>, as if they were included in <code>stop_gradients</code>.</p>
<p><code>unconnected_gradients</code> determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the <code>'zero'</code> option. <a href="../tf/UnconnectedGradients.html"><code>tf.UnconnectedGradients</code></a> provides the following options and behaviors:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">a <span class="op">=</span> tf.ones([<span class="dv">1</span>, <span class="dv">2</span>])
b <span class="op">=</span> tf.ones([<span class="dv">3</span>, <span class="dv">1</span>])
g1 <span class="op">=</span> tf.gradients([b], [a], unnconnected_gradients<span class="op">=</span><span class="st">&#39;none&#39;</span>)
sess.run(g1)  <span class="co"># [None]</span>

g2 <span class="op">=</span> tf.gradients([b], [a], unconnected_gradients<span class="op">=</span><span class="st">&#39;zero&#39;</span>)
sess.run(g2)  <span class="co"># [array([[0., 0.]], dtype=float32)]</span></code></pre></div>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>ys</code></b>: A <code>Tensor</code> or list of tensors to be differentiated.</li>
<li><b><code>xs</code></b>: A <code>Tensor</code> or list of tensors to be used for differentiation.</li>
<li><b><code>grad_ys</code></b>: Optional. A <code>Tensor</code> or list of tensors the same size as <code>ys</code> and holding the gradients computed for each y in <code>ys</code>.</li>
<li><b><code>name</code></b>: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.</li>
<li><b><code>gate_gradients</code></b>: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.</li>
<li><b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class <code>AggregationMethod</code>.</li>
<li><b><code>stop_gradients</code></b>: Optional. A <code>Tensor</code> or list of tensors not to differentiate through.</li>
<li><b><code>unconnected_gradients</code></b>: Optional. Specifies the gradient value returned when the given input tensors are unconnected. Accepted values are constants defined in the class <a href="../tf/UnconnectedGradients.html"><code>tf.UnconnectedGradients</code></a> and the default value is <code>none</code>.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A list of <code>sum(dy/dx)</code> for each x in <code>xs</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>LookupError</code></b>: if one of the operations between <code>x</code> and <code>y</code> does not have a registered gradient function.</li>
<li><b><code>ValueError</code></b>: if the arguments are invalid.</li>
<li><b><code>RuntimeError</code></b>: if called in Eager mode.</li>
</ul>
