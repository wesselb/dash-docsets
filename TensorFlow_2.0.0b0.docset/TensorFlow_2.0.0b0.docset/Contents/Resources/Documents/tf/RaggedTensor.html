<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.RaggedTensor" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="dtype"/> <meta itemprop="property" content="flat_values"/> <meta itemprop="property" content="nested_row_splits"/> <meta itemprop="property" content="ragged_rank"/> <meta itemprop="property" content="row_splits"/> <meta itemprop="property" content="shape"/> <meta itemprop="property" content="values"/> <meta itemprop="property" content="__abs__"/> <meta itemprop="property" content="__add__"/> <meta itemprop="property" content="__and__"/> <meta itemprop="property" content="__bool__"/> <meta itemprop="property" content="__div__"/> <meta itemprop="property" content="__floordiv__"/> <meta itemprop="property" content="__ge__"/> <meta itemprop="property" content="__getitem__"/> <meta itemprop="property" content="__gt__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="__invert__"/> <meta itemprop="property" content="__le__"/> <meta itemprop="property" content="__lt__"/> <meta itemprop="property" content="__mod__"/> <meta itemprop="property" content="__mul__"/> <meta itemprop="property" content="__neg__"/> <meta itemprop="property" content="__nonzero__"/> <meta itemprop="property" content="__or__"/> <meta itemprop="property" content="__pow__"/> <meta itemprop="property" content="__radd__"/> <meta itemprop="property" content="__rand__"/> <meta itemprop="property" content="__rdiv__"/> <meta itemprop="property" content="__rfloordiv__"/> <meta itemprop="property" content="__rmod__"/> <meta itemprop="property" content="__rmul__"/> <meta itemprop="property" content="__ror__"/> <meta itemprop="property" content="__rpow__"/> <meta itemprop="property" content="__rsub__"/> <meta itemprop="property" content="__rtruediv__"/> <meta itemprop="property" content="__rxor__"/> <meta itemprop="property" content="__sub__"/> <meta itemprop="property" content="__truediv__"/> <meta itemprop="property" content="__xor__"/> <meta itemprop="property" content="bounding_shape"/> <meta itemprop="property" content="consumers"/> <meta itemprop="property" content="from_nested_row_lengths"/> <meta itemprop="property" content="from_nested_row_splits"/> <meta itemprop="property" content="from_nested_value_rowids"/> <meta itemprop="property" content="from_row_lengths"/> <meta itemprop="property" content="from_row_limits"/> <meta itemprop="property" content="from_row_splits"/> <meta itemprop="property" content="from_row_starts"/> <meta itemprop="property" content="from_sparse"/> <meta itemprop="property" content="from_tensor"/> <meta itemprop="property" content="from_value_rowids"/> <meta itemprop="property" content="nested_row_lengths"/> <meta itemprop="property" content="nrows"/> <meta itemprop="property" content="row_lengths"/> <meta itemprop="property" content="row_limits"/> <meta itemprop="property" content="row_starts"/> <meta itemprop="property" content="to_list"/> <meta itemprop="property" content="to_sparse"/> <meta itemprop="property" content="to_tensor"/> <meta itemprop="property" content="value_rowids"/> <meta itemprop="property" content="with_flat_values"/> <meta itemprop="property" content="with_row_splits_dtype"/> <meta itemprop="property" content="with_values"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.RaggedTensor" class="dashAnchor"></a><h1 id="tf.raggedtensor">tf.RaggedTensor</h1>
<h2 id="class-raggedtensor">Class <code>RaggedTensor</code></h2>
<p>Represents a ragged tensor.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.RaggedTensor</code></li>
<li>Class <code>tf.compat.v1.RaggedTensor</code></li>
<li>Class <code>tf.compat.v2.RaggedTensor</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/ops/ragged/ragged_tensor.py"><code>python/ops/ragged/ragged_tensor.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>A <code>RaggedTensor</code> is a tensor with one or more <em>ragged dimensions</em>, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of <code>rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code> is ragged, since the column slices (<code>rt[0, :]</code>, ..., <code>rt[4, :]</code>) have different lengths. Dimensions whose slices all have the same length are called <em>uniform dimensions</em>. The outermost dimension of a <code>RaggedTensor</code> is always uniform, since it consists of a single slice (and so there is no possibility for differing slice lengths).</p>
<p>The total number of dimensions in a <code>RaggedTensor</code> is called its <em>rank</em>, and the number of ragged dimensions in a <code>RaggedTensor</code> is called its <em>ragged-rank</em>. A <code>RaggedTensor</code>'s ragged-rank is fixed at graph creation time: it can't depend on the runtime values of <code>Tensor</code>s, and can't vary dynamically for different session runs.</p>
<h3 id="potentially-ragged-tensors">Potentially Ragged Tensors</h3>
<p>Many ops support both <code>Tensor</code>s and <code>RaggedTensor</code>s. The term &quot;potentially ragged tensor&quot; may be used to refer to a tensor that might be either a <code>Tensor</code> or a <code>RaggedTensor</code>. The ragged-rank of a <code>Tensor</code> is zero.</p>
<h3 id="documenting-raggedtensor-shapes">Documenting RaggedTensor Shapes</h3>
<p>When documenting the shape of a RaggedTensor, ragged dimensions can be indicated by enclosing them in parentheses. For example, the shape of a 3-D <code>RaggedTensor</code> that stores the fixed-size word embedding for each word in a sentence, for each sentence in a batch, could be written as <code>[num_sentences, (num_words), embedding_size]</code>. The parentheses around <code>(num_words)</code> indicate that dimension is ragged, and that the length of each element list in that dimension may vary for each item.</p>
<h3 id="component-tensors">Component Tensors</h3>
<p>Internally, a <code>RaggedTensor</code> consists of a concatenated list of values that are partitioned into variable-length rows. In particular, each <code>RaggedTensor</code> consists of:</p>
<ul>
<li><p>A <code>values</code> tensor, which concatenates the variable-length rows into a flattened list. For example, the <code>values</code> tensor for <code>[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code> is <code>[3, 1, 4, 1, 5, 9, 2, 6]</code>.</p></li>
<li><p>A <code>row_splits</code> vector, which indicates how those flattened values are divided into rows. In particular, the values for row <code>rt[i]</code> are stored in the slice <code>rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</code>.</p></li>
</ul>
<h4 id="example">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(tf.RaggedTensor.from_row_splits(
...     values<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">6</span>],
...     row_splits<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>]))
<span class="op">&lt;</span>tf.RaggedTensor [[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>], [], [<span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>], [<span class="dv">6</span>], []]<span class="op">&gt;</span></code></pre></div>
<h3 id="alternative-row-partitioning-schemes">Alternative Row-Partitioning Schemes</h3>
<p>In addition to <code>row_splits</code>, ragged tensors provide support for four other row-partitioning schemes:</p>
<ul>
<li><p><code>row_lengths</code>: a vector with shape <code>[nrows]</code>, which specifies the length of each row.</p></li>
<li><p><code>value_rowids</code> and <code>nrows</code>: <code>value_rowids</code> is a vector with shape <code>[nvals]</code>, corresponding one-to-one with <code>values</code>, which specifies each value's row index. In particular, the row <code>rt[row]</code> consists of the values <code>rt.values[j]</code> where <code>value_rowids[j]==row</code>. <code>nrows</code> is an integer scalar that specifies the number of rows in the <code>RaggedTensor</code>. (<code>nrows</code> is used to indicate trailing empty rows.)</p></li>
<li><p><code>row_starts</code>: a vector with shape <code>[nrows]</code>, which specifies the start offset of each row. Equivalent to <code>row_splits[:-1]</code>.</p></li>
<li><p><code>row_limits</code>: a vector with shape <code>[nrows]</code>, which specifies the stop offset of each row. Equivalent to <code>row_splits[1:]</code>.</p></li>
</ul>
<p>Example: The following ragged tensors are equivalent, and all represent the nested list <code>[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> values <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">6</span>]
<span class="op">&gt;&gt;&gt;</span> rt1 <span class="op">=</span> RaggedTensor.from_row_splits(values, row_splits<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>])
<span class="op">&gt;&gt;&gt;</span> rt2 <span class="op">=</span> RaggedTensor.from_row_lengths(values, row_lengths<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>])
<span class="op">&gt;&gt;&gt;</span> rt3 <span class="op">=</span> RaggedTensor.from_value_rowids(
...     values, value_rowids<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>], nrows<span class="op">=</span><span class="dv">5</span>)
<span class="op">&gt;&gt;&gt;</span> rt4 <span class="op">=</span> RaggedTensor.from_row_starts(values, row_starts<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>])
<span class="op">&gt;&gt;&gt;</span> rt5 <span class="op">=</span> RaggedTensor.from_row_limits(values, row_limits<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>])</code></pre></div>
<h3 id="multiple-ragged-dimensions">Multiple Ragged Dimensions</h3>
<p><code>RaggedTensor</code>s with multiple ragged dimensions can be defined by using a nested <code>RaggedTensor</code> for the <code>values</code> tensor. Each nested <code>RaggedTensor</code> adds a single ragged dimension.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> inner_rt <span class="op">=</span> RaggedTensor.from_row_splits(  <span class="co"># =rt1 from above</span>
...     values<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">6</span>], row_splits<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>])
<span class="op">&gt;&gt;&gt;</span> outer_rt <span class="op">=</span> RaggedTensor.from_row_splits(
...     values<span class="op">=</span>inner_rt, row_splits<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">5</span>])
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span> outer_rt.to_list()
[[[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>], [], [<span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>]], [], [[<span class="dv">6</span>], []]]
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span> outer_rt.ragged_rank
<span class="dv">2</span></code></pre></div>
<p>The factory function <code>RaggedTensor.from_nested_row_splits</code> may be used to construct a <code>RaggedTensor</code> with multiple ragged dimensions directly, by providing a list of <code>row_splits</code> tensors:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> RaggedTensor.from_nested_row_splits(
...     flat_values<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">6</span>],
...     nested_row_splits<span class="op">=</span>([<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">5</span>], [<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">8</span>])).to_list()
[[[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>], [], [<span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>]], [], [[<span class="dv">6</span>], []]]</code></pre></div>
<h3 id="uniform-inner-dimensions">Uniform Inner Dimensions</h3>
<p><code>RaggedTensor</code>s with uniform inner dimensions can be defined by using a multidimensional <code>Tensor</code> for <code>values</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> rt <span class="op">=</span> RaggedTensor.from_row_splits(values<span class="op">=</span>tf.ones([<span class="dv">5</span>, <span class="dv">3</span>]),
..                                    row_splits<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">5</span>])
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span> rt.to_list()
[[[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]],
 [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]]]
 <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span> rt.shape
 (<span class="dv">2</span>, ?, <span class="dv">3</span>)</code></pre></div>
<h3 id="raggedtensor-shape-restrictions">RaggedTensor Shape Restrictions</h3>
<p>The shape of a RaggedTensor is currently restricted to have the following form:</p>
<ul>
<li>A single uniform dimension</li>
<li>Followed by one or more ragged dimensions</li>
<li>Followed by zero or more uniform dimensions.</li>
</ul>
<p>This restriction follows from the fact that each nested <code>RaggedTensor</code> replaces the uniform outermost dimension of its <code>values</code> with a uniform dimension followed by a ragged dimension.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    values,
    row_splits,
    cached_row_lengths<span class="op">=</span><span class="va">None</span>,
    cached_value_rowids<span class="op">=</span><span class="va">None</span>,
    cached_nrows<span class="op">=</span><span class="va">None</span>,
    internal<span class="op">=</span><span class="va">False</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with a specified partitioning for <code>values</code>.</p>
<p>This constructor is private -- please use one of the following ops to build <code>RaggedTensor</code>s:</p>
<ul>
<li><a href="../tf/RaggedTensor.md#from_row_lengths"><code>tf.RaggedTensor.from_row_lengths</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_value_rowids"><code>tf.RaggedTensor.from_value_rowids</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_row_splits"><code>tf.RaggedTensor.from_row_splits</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_row_starts"><code>tf.RaggedTensor.from_row_starts</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_row_limits"><code>tf.RaggedTensor.from_row_limits</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_nested_row_splits"><code>tf.RaggedTensor.from_nested_row_splits</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_nested_row_lengths"><code>tf.RaggedTensor.from_nested_row_lengths</code></a></li>
<li><a href="../tf/RaggedTensor.md#from_nested_value_rowids"><code>tf.RaggedTensor.from_nested_value_rowids</code></a></li>
</ul>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor of any dtype and shape <code>[nvals, ...]</code>.</li>
<li><b><code>row_splits</code></b>: A 1-D integer tensor with shape <code>[nrows+1]</code>.</li>
<li><b><code>cached_row_lengths</code></b>: A 1-D integer tensor with shape <code>[nrows]</code></li>
<li><b><code>cached_value_rowids</code></b>: A 1-D integer tensor with shape <code>[nvals]</code>.</li>
<li><b><code>cached_nrows</code></b>: A 1-D integer scalar tensor.</li>
<li><b><code>internal</code></b>: True if the constructor is being called by one of the factory methods. If false, an exception will be raised.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If a row partitioning tensor has an inappropriate dtype.</li>
<li><b><code>TypeError</code></b>: If exactly one row partitioning argument was not specified.</li>
<li><b><code>ValueError</code></b>: If a row partitioning tensor has an inappropriate shape.</li>
<li><b><code>ValueError</code></b>: If multiple partitioning arguments are specified.</li>
<li><b><code>ValueError</code></b>: If nrows is specified but value_rowids is not None.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="dtype">
<code>dtype</code>
</h3>
<p>The <code>DType</code> of values in this tensor.</p>
<h3 id="flat_values">
<code>flat_values</code>
</h3>
<p>The innermost <code>values</code> tensor for this ragged tensor.</p>
<p>Concretely, if <code>rt.values</code> is a <code>Tensor</code>, then <code>rt.flat_values</code> is <code>rt.values</code>; otherwise, <code>rt.flat_values</code> is <code>rt.values.flat_values</code>.</p>
<p>Conceptually, <code>flat_values</code> is the tensor formed by flattening the outermost dimension and all of the ragged dimensions into a single dimension.</p>
<p><code>rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]</code> (where <code>nvals</code> is the number of items in the flattened dimensions).</p>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h4 id="example-1">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])   &gt;&gt;&gt; print rt.flat_values()   tf.Tensor([3, 1, 4, 1, 5, 9, 2, 6])</code></p>
<h3 id="nested_row_splits">
<code>nested_row_splits</code>
</h3>
<p>A tuple containing the row_splits for all ragged dimensions.</p>
<p><code>rt.nested_row_splits</code> is a tuple containing the <code>row_splits</code> tensors for all ragged dimensions in <code>rt</code>, ordered from outermost to innermost. In particular, <code>rt.nested_row_splits = (rt.row_splits,) + value_splits</code> where:</p>
<pre><code>* `value_splits = ()` if `rt.values` is a `Tensor`.
* `value_splits = rt.values.nested_row_splits` otherwise.</code></pre>
<h4 id="returns-1">Returns:</h4>
<p>A <code>tuple</code> of 1-D integer <code>Tensor</code>s.</p>
<h4 id="example-2">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])   &gt;&gt;&gt; for i, splits in enumerate(rt.nested_row_splits()):   ...   print('Splits for dimension %d: %s' % (i+1, splits))   Splits for dimension 1: [0, 1]   Splits for dimension 2: [0, 3, 3, 5]   Splits for dimension 3: [0, 4, 4, 7, 8, 8]</code></p>
<h3 id="ragged_rank">
<code>ragged_rank</code>
</h3>
<p>The number of ragged dimensions in this ragged tensor.</p>
<h4 id="returns-2">Returns:</h4>
<p>A Python <code>int</code> indicating the number of ragged dimensions in this ragged tensor. The outermost dimension is not considered ragged.</p>
<h3 id="row_splits">
<code>row_splits</code>
</h3>
<p>The row-split indices for this ragged tensor's <code>values</code>.</p>
<p><code>rt.row_splits</code> specifies where the values for each row begin and end in <code>rt.values</code>. In particular, the values for row <code>rt[i]</code> are stored in the slice <code>rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</code>.</p>
<h4 id="returns-3">Returns:</h4>
<p>A 1-D integer <code>Tensor</code> with shape <code>[self.nrows+1]</code>. The returned tensor is non-empty, and is sorted in ascending order. <code>self.row_splits[0]</code> is zero, and <code>self.row_splits[-1]</code> is equal to <code>self.values.shape[0]</code>.</p>
<h4 id="example-3">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; print rt.row_splits  # indices of row splits in rt.values   tf.Tensor([0, 4, 4, 7, 8, 8])</code></p>
<h3 id="shape">
<code>shape</code>
</h3>
<p>The statically known shape of this ragged tensor.</p>
<h4 id="returns-4">Returns:</h4>
<p>A <code>TensorShape</code> containing the statically known shape of this ragged tensor. Ragged dimensions have a size of <code>None</code>.</p>
<h4 id="examples">Examples:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> ragged.constant([[<span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">2</span>]]).shape
TensorShape([Dimension(<span class="dv">2</span>), Dimension(<span class="va">None</span>)])

<span class="op">&gt;&gt;&gt;</span> ragged.constant([[[<span class="dv">0</span>, <span class="dv">1</span>]], [[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]]], ragged_rank<span class="op">=</span><span class="dv">1</span>).shape
TensorShape([Dimension(<span class="dv">2</span>), Dimension(<span class="va">None</span>), Dimension(<span class="dv">2</span>)</code></pre></div>
<h3 id="values">
<code>values</code>
</h3>
<p>The concatenated rows for this ragged tensor.</p>
<p><code>rt.values</code> is a potentially ragged tensor formed by flattening the two outermost dimensions of <code>rt</code> into a single dimension.</p>
<p><code>rt.values.shape = [nvals] + rt.shape[2:]</code> (where <code>nvals</code> is the number of items in the outer two dimensions of <code>rt</code>).</p>
<p><code>rt.ragged_rank = self.ragged_rank - 1</code></p>
<h4 id="returns-5">Returns:</h4>
<p>A potentially ragged tensor.</p>
<h4 id="example-4">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; print rt.values   tf.Tensor([3, 1, 4, 1, 5, 9, 2, 6])</code></p>
<h2 id="methods">Methods</h2>
<h3 id="__abs__">
<code><strong>abs</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__abs__</span>(
    x,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Computes the absolute value of a tensor.</p>
<p>Given a tensor of integer or floating-point values, this operation returns a tensor of the same type, where each element contains the absolute value of the corresponding element in the input.</p>
<p>Given a tensor <code>x</code> of complex numbers, this operation returns a tensor of type <code>float32</code> or <code>float64</code> that is the absolute value of each element in <code>x</code>. All elements in <code>x</code> must be complex numbers of the form \(a + bj\). The absolute value is computed as \( \). For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant([[<span class="op">-</span><span class="fl">2.25</span> <span class="op">+</span> <span class="fl">4.</span>75j], [<span class="op">-</span><span class="fl">3.25</span> <span class="op">+</span> <span class="fl">5.</span>75j]])
tf.<span class="bu">abs</span>(x)  <span class="co"># [5.25594902, 6.60492229]</span></code></pre></div>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> or <code>SparseTensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code> or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>A <code>Tensor</code> or <code>SparseTensor</code> the same size, type, and sparsity as <code>x</code> with absolute values. Note, for <code>complex64</code> or <code>complex128</code> input, the returned <code>Tensor</code> will be of type <code>float32</code> or <code>float64</code>, respectively.</p>
<p>If <code>x</code> is a <code>SparseTensor</code>, returns <code>SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)</code></p>
<h3 id="__add__">
<code><strong>add</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__add__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>math.add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__and__">
<code><strong>and</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__and__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>math.logical_and</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__bool__">
<code><strong>bool</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__bool__(_)</code></pre></div>
<p>Dummy method to prevent a RaggedTensor from being used as a Python bool.</p>
<h3 id="__div__">
<code><strong>div</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__div__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.</p>
<p>NOTE: Prefer using the Tensor division operator or tf.divide which obey Python 3 division operator semantics.</p>
<p>This function divides <code>x</code> and <code>y</code>, forcing Python 2 semantics. That is, if <code>x</code> and <code>y</code> are both integers then the result will be an integer. This is in contrast to Python 3, where division with <code>/</code> is always a float while division with <code>//</code> is always an integer.</p>
<h4 id="args-4">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-9">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__floordiv__">
<code><strong>floordiv</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__floordiv__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.compat.v1.div(x,y)</code> for integers, but uses <code>tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-5">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-10">Returns:</h4>
<p><code>x / y</code> rounded down.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__ge__">
<code><strong>ge</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__ge__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of (x &gt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>math.greater_equal</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-6">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-11">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__getitem__">
<code><strong>getitem</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__getitem__</span>(key)</code></pre></div>
<p>Returns the specified piece of this RaggedTensor.</p>
<p>Supports multidimensional indexing and slicing, with one restriction: indexing into a ragged inner dimension is not allowed. This case is problematic because the indicated value may exist in some rows but not others. In such cases, it's not obvious whether we should (1) report an IndexError; (2) use a default value; or (3) skip that value and return a tensor with fewer rows than we started with. Following the guiding principles of Python (&quot;In the face of ambiguity, refuse the temptation to guess&quot;), we simply disallow this operation.</p>
<p>Any dimensions added by <code>array_ops.newaxis</code> will be ragged if the following dimension is ragged.</p>
<h4 id="args-7">Args:</h4>
<ul>
<li><b><code>self</code></b>: The RaggedTensor to slice.</li>
<li><p><b><code>key</code></b>: Indicates which piece of the RaggedTensor to return, using standard Python semantics (e.g., negative values index from the end). <code>key</code> may have any of the following types:</p></li>
<li><code>int</code> constant</li>
<li>Scalar integer <code>Tensor</code></li>
<li><code>slice</code> containing integer constants and/or scalar integer <code>Tensor</code>s</li>
<li><code>Ellipsis</code></li>
<li><a href="../tf.md#newaxis"><code>tf.newaxis</code></a></li>
<li><p><code>tuple</code> containing any of the above (for multidimentional indexing)</p></li>
</ul>
<h4 id="returns-12">Returns:</h4>
<p>A <code>Tensor</code> or <code>RaggedTensor</code> object. Values that include at least one ragged dimension are returned as <code>RaggedTensor</code>. Values that include no ragged dimensions are returned as <code>Tensor</code>. See above for examples of expressions that return <code>Tensor</code>s vs <code>RaggedTensor</code>s.</p>
<h4 id="raises-2">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>key</code> is out of bounds.</li>
<li><b><code>ValueError</code></b>: If <code>key</code> is not supported.</li>
<li><b><code>TypeError</code></b>: If the indices in <code>key</code> have an unsupported type.</li>
</ul>
<h4 id="examples-1">Examples:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="co"># A 2-D ragged tensor with 1 ragged dimension.</span>
<span class="op">&gt;&gt;&gt;</span> rt <span class="op">=</span> ragged.constant([[<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>, <span class="st">&#39;c&#39;</span>], [<span class="st">&#39;d&#39;</span>, <span class="st">&#39;e&#39;</span>], [<span class="st">&#39;f&#39;</span>], [<span class="st">&#39;g&#39;</span>]])
<span class="op">&gt;&gt;&gt;</span> rt[<span class="dv">0</span>].<span class="bu">eval</span>().tolist()       <span class="co"># First row (1-D `Tensor`)</span>
[<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>, <span class="st">&#39;c&#39;</span>]
<span class="op">&gt;&gt;&gt;</span> rt[:<span class="dv">3</span>].<span class="bu">eval</span>().tolist()      <span class="co"># First three rows (2-D RaggedTensor)</span>
[[<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>, <span class="st">&#39;c&#39;</span>], [<span class="st">&#39;d&#39;</span>, <span class="st">&#39;e&#39;</span>], <span class="st">&#39;[f&#39;</span>], [g<span class="st">&#39;]]</span>
<span class="st">&gt;&gt;&gt; rt[3, 0].eval().tolist()    # 1st element of 4th row (scalar)</span>
<span class="st">&#39;</span>g<span class="st">&#39;</span>

<span class="st">&gt;&gt;&gt; # A 3-D ragged tensor with 2 ragged dimensions.</span>
<span class="st">&gt;&gt;&gt; rt = ragged.constant([[[1, 2, 3], [4]],</span>
<span class="st">...                    [[5], [], [6]],</span>
<span class="st">...                    [[7]],</span>
<span class="st">...                    [[8, 9], [10]]])</span>
<span class="st">&gt;&gt;&gt; rt[1].eval().tolist()       # Second row (2-D RaggedTensor)</span>
<span class="st">[[5], [], [6]]</span>
<span class="st">&gt;&gt;&gt; rt[3, 0].eval().tolist()    # First element of fourth row (1-D Tensor)</span>
<span class="st">[8, 9]</span>
<span class="st">&gt;&gt;&gt; rt[:, 1:3].eval().tolist()  # Items 1-3 of each row (3-D RaggedTensor)</span>
<span class="st">[[[4]], [[], [6]], [], [[10]]]</span>
<span class="st">&gt;&gt;&gt; rt[:, -1:].eval().tolist()  # Last item of each row (3-D RaggedTensor)</span>
<span class="st">[[[4]], [[6]], [[7]], [[10]]]</span></code></pre></div>
<h3 id="__gt__">
<code><strong>gt</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__gt__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of (x &gt; y) element-wise.</p>
<p><em>NOTE</em>: <code>math.greater</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-8">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-13">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__invert__">
<code><strong>invert</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__invert__</span>(
    x,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of NOT x element-wise.</p>
<h4 id="args-9">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-14">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__le__">
<code><strong>le</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__le__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of (x &lt;= y) element-wise.</p>
<p><em>NOTE</em>: <code>math.less_equal</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-10">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-15">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__lt__">
<code><strong>lt</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__lt__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of (x &lt; y) element-wise.</p>
<p><em>NOTE</em>: <code>math.less</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-11">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>int64</code>, <code>bfloat16</code>, <code>uint16</code>, <code>half</code>, <code>uint32</code>, <code>uint64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-16">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__mod__">
<code><strong>mod</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__mod__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>math.floormod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-12">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-17">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__mul__">
<code><strong>mul</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__mul__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x * y element-wise.</p>
<p><em>NOTE</em>: <code>&lt;a href=&quot;../tf/math/multiply.md&quot;&gt;&lt;code&gt;tf.multiply&lt;/code&gt;&lt;/a&gt;</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-13">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-18">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__neg__">
<code><strong>neg</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__neg__</span>(
    x,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Computes numerical negative value element-wise.</p>
<p>I.e., \(y = -x\).</p>
<h4 id="args-14">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-19">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<p>If <code>x</code> is a <code>SparseTensor</code>, returns <code>SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)</code></p>
<h3 id="__nonzero__">
<code><strong>nonzero</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__nonzero__</span>(_)</code></pre></div>
<p>Dummy method to prevent a RaggedTensor from being used as a Python bool.</p>
<h3 id="__or__">
<code><strong>or</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__or__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>math.logical_or</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-15">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-20">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__pow__">
<code><strong>pow</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__pow__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant([[<span class="dv">2</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">3</span>]])
y <span class="op">=</span> tf.constant([[<span class="dv">8</span>, <span class="dv">16</span>], [<span class="dv">2</span>, <span class="dv">3</span>]])
tf.<span class="bu">pow</span>(x, y)  <span class="co"># [[256, 65536], [9, 27]]</span></code></pre></div>
<h4 id="args-16">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-21">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__radd__">
<code><strong>radd</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__radd__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x + y element-wise.</p>
<p><em>NOTE</em>: <code>math.add</code> supports broadcasting. <code>AddN</code> does not. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-17">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>, <code>string</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-22">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rand__">
<code><strong>rand</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rand__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of x AND y element-wise.</p>
<p><em>NOTE</em>: <code>math.logical_and</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-18">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-23">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rdiv__">
<code><strong>rdiv</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rdiv__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p>
<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.</p>
<p>NOTE: Prefer using the Tensor division operator or tf.divide which obey Python 3 division operator semantics.</p>
<p>This function divides <code>x</code> and <code>y</code>, forcing Python 2 semantics. That is, if <code>x</code> and <code>y</code> are both integers then the result will be an integer. This is in contrast to Python 3, where division with <code>/</code> is always a float while division with <code>//</code> is always an integer.</p>
<h4 id="args-19">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-24">Returns:</h4>
<p><code>x / y</code> returns the quotient of x and y.</p>
<h3 id="__rfloordiv__">
<code><strong>rfloordiv</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rfloordiv__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides <code>x / y</code> elementwise, rounding toward the most negative integer.</p>
<p>The same as <code>tf.compat.v1.div(x,y)</code> for integers, but uses <code>tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code>x // y</code> floor division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>.</p>
<p><code>x</code> and <code>y</code> must have the same type, and the result will have the same type as well.</p>
<h4 id="args-20">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of real numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of real numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-25">Returns:</h4>
<p><code>x / y</code> rounded down.</p>
<h4 id="raises-3">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If the inputs are complex.</li>
</ul>
<h3 id="__rmod__">
<code><strong>rmod</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rmod__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns element-wise remainder of division. When <code>x &lt; 0</code> xor <code>y &lt; 0</code> is</p>
<p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code>floor(x / y) * y + mod(x, y) = x</code>.</p>
<p><em>NOTE</em>: <code>math.floormod</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-21">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>, <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-26">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rmul__">
<code><strong>rmul</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rmul__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x * y element-wise.</p>
<p><em>NOTE</em>: <code>&lt;a href=&quot;../tf/math/multiply.md&quot;&gt;&lt;code&gt;tf.multiply&lt;/code&gt;&lt;/a&gt;</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-22">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-27">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__ror__">
<code><strong>ror</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__ror__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the truth value of x OR y element-wise.</p>
<p><em>NOTE</em>: <code>math.logical_or</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-23">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>bool</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-28">Returns:</h4>
<p>A <code>Tensor</code> of type <code>bool</code>.</p>
<h3 id="__rpow__">
<code><strong>rpow</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rpow__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Computes the power of one value to another.</p>
<p>Given a tensor <code>x</code> and a tensor <code>y</code>, this operation computes \(x^y\) for corresponding elements in <code>x</code> and <code>y</code>. For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant([[<span class="dv">2</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">3</span>]])
y <span class="op">=</span> tf.constant([[<span class="dv">8</span>, <span class="dv">16</span>], [<span class="dv">2</span>, <span class="dv">3</span>]])
tf.<span class="bu">pow</span>(x, y)  <span class="co"># [[256, 65536], [9, 27]]</span></code></pre></div>
<h4 id="args-24">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type <code>float16</code>, <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, or <code>complex128</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-29">Returns:</h4>
<p>A <code>Tensor</code>.</p>
<h3 id="__rsub__">
<code><strong>rsub</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rsub__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-25">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-30">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__rtruediv__">
<code><strong>rtruediv</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rtruediv__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides x / y elementwise (using Python 3 division operator semantics).</p>
<p>NOTE: Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.</p>
<p>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal <code>x / y</code> division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>. If you want integer division that rounds down, use <code>x // y</code> or <a href="../tf/math/floordiv.html"><code>tf.math.floordiv</code></a>.</p>
<p><code>x</code> and <code>y</code> must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to <code>float32</code> for <code>int8</code> and <code>int16</code> and <code>float64</code> for <code>int32</code> and <code>int64</code> (matching the behavior of Numpy).</p>
<h4 id="args-26">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-31">Returns:</h4>
<p><code>x / y</code> evaluated in floating point.</p>
<h4 id="raises-4">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>x</code> and <code>y</code> have different dtypes.</li>
</ul>
<h3 id="__rxor__">
<code><strong>rxor</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__rxor__</span>(
    x,
    y,
    name<span class="op">=</span><span class="st">&#39;LogicalXor&#39;</span>
)</code></pre></div>
<p>Logical XOR function.</p>
<p>x ^ y = (x | y) &amp; ~(x &amp; y)</p>
<p>Inputs are tensor and if the tensors contains more than one element, an element-wise logical XOR is computed.</p>
<h4 id="usage">Usage:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant([<span class="va">False</span>, <span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>], dtype <span class="op">=</span> tf.<span class="bu">bool</span>)
y <span class="op">=</span> tf.constant([<span class="va">False</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">True</span>], dtype <span class="op">=</span> tf.<span class="bu">bool</span>)
z <span class="op">=</span> tf.logical_xor(x, y, name<span class="op">=</span><span class="st">&quot;LogicalXor&quot;</span>)
<span class="co">#  here z = [False  True  True False]</span></code></pre></div>
<h4 id="args-27">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> type bool.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type bool.</li>
</ul>
<h4 id="returns-32">Returns:</h4>
<p>A <code>Tensor</code> of type bool with the same size as that of x or y.</p>
<h3 id="__sub__">
<code><strong>sub</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__sub__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns x - y element-wise.</p>
<p><em>NOTE</em>: <code>Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p>
<h4 id="args-28">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>bfloat16</code>, <code>half</code>, <code>float32</code>, <code>float64</code>, <code>uint8</code>, <code>int8</code>, <code>uint16</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>complex64</code>, <code>complex128</code>.</li>
<li><b><code>y</code></b>: A <code>Tensor</code>. Must have the same type as <code>x</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-33">Returns:</h4>
<p>A <code>Tensor</code>. Has the same type as <code>x</code>.</p>
<h3 id="__truediv__">
<code><strong>truediv</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__truediv__</span>(
    x,
    y,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Divides x / y elementwise (using Python 3 division operator semantics).</p>
<p>NOTE: Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.</p>
<p>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal <code>x / y</code> division in Python 3 and in Python 2.7 with <code>from __future__ import division</code>. If you want integer division that rounds down, use <code>x // y</code> or <a href="../tf/math/floordiv.html"><code>tf.math.floordiv</code></a>.</p>
<p><code>x</code> and <code>y</code> must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to <code>float32</code> for <code>int8</code> and <code>int16</code> and <code>float64</code> for <code>int32</code> and <code>int64</code> (matching the behavior of Numpy).</p>
<h4 id="args-29">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code> numerator of numeric type.</li>
<li><b><code>y</code></b>: <code>Tensor</code> denominator of numeric type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns-34">Returns:</h4>
<p><code>x / y</code> evaluated in floating point.</p>
<h4 id="raises-5">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>x</code> and <code>y</code> have different dtypes.</li>
</ul>
<h3 id="__xor__">
<code><strong>xor</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__xor__</span>(
    x,
    y,
    name<span class="op">=</span><span class="st">&#39;LogicalXor&#39;</span>
)</code></pre></div>
<p>Logical XOR function.</p>
<p>x ^ y = (x | y) &amp; ~(x &amp; y)</p>
<p>Inputs are tensor and if the tensors contains more than one element, an element-wise logical XOR is computed.</p>
<h4 id="usage-1">Usage:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.constant([<span class="va">False</span>, <span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>], dtype <span class="op">=</span> tf.<span class="bu">bool</span>)
y <span class="op">=</span> tf.constant([<span class="va">False</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">True</span>], dtype <span class="op">=</span> tf.<span class="bu">bool</span>)
z <span class="op">=</span> tf.logical_xor(x, y, name<span class="op">=</span><span class="st">&quot;LogicalXor&quot;</span>)
<span class="co">#  here z = [False  True  True False]</span></code></pre></div>
<h4 id="args-30">Args:</h4>
<ul>
<li><b><code>x</code></b>: A <code>Tensor</code> type bool.</li>
<li><b><code>y</code></b>: A <code>Tensor</code> of type bool.</li>
</ul>
<h4 id="returns-35">Returns:</h4>
<p>A <code>Tensor</code> of type bool with the same size as that of x or y.</p>
<h3 id="bounding_shape">
<code>bounding_shape</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">bounding_shape(
    axis<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>,
    out_type<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the tight bounding box shape for this <code>RaggedTensor</code>.</p>
<h4 id="args-31">Args:</h4>
<ul>
<li><b><code>axis</code></b>: An integer scalar or vector indicating which axes to return the bounding box for. If not specified, then the full bounding box is returned.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
<li><b><code>out_type</code></b>: <code>dtype</code> for the returned tensor. Defaults to <code>self.row_splits.dtype</code>.</li>
</ul>
<h4 id="returns-36">Returns:</h4>
<p>An integer <code>Tensor</code> (<code>dtype=self.row_splits.dtype</code>). If <code>axis</code> is not specified, then <code>output</code> is a vector with <code>output.shape=[self.shape.ndims]</code>. If <code>axis</code> is a scalar, then the <code>output</code> is a scalar. If <code>axis</code> is a vector, then <code>output</code> is a vector, where <code>output[i]</code> is the bounding size for dimension <code>axis[i]</code>.</p>
<h4 id="example-5">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[1, 2, 3, 4], [5], [], [6, 7, 8, 9], [10]])   &gt;&gt;&gt; rt.bounding_shape()   [5, 4]</code></p>
<h3 id="consumers">
<code>consumers</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">consumers()</code></pre></div>
<h3 id="from_nested_row_lengths">
<code>from_nested_row_lengths</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_nested_row_lengths(
    cls,
    flat_values,
    nested_row_lengths,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> from a nested list of <code>row_lengths</code> tensors.</p>
<h4 id="equivalent-to">Equivalent to:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> flat_values
<span class="cf">for</span> row_lengths <span class="kw">in</span> <span class="bu">reversed</span>(nested_row_lengths):
  result <span class="op">=</span> from_row_lengths(result, row_lengths)</code></pre></div>
<h4 id="args-32">Args:</h4>
<ul>
<li><b><code>flat_values</code></b>: A potentially ragged tensor.</li>
<li><b><code>nested_row_lengths</code></b>: A list of 1-D integer tensors. The <code>i</code>th tensor is used as the <code>row_lengths</code> for the <code>i</code>th ragged dimension.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-37">Returns:</h4>
<p>A <code>RaggedTensor</code> (or <code>flat_values</code> if <code>nested_row_lengths</code> is empty).</p>
<h3 id="from_nested_row_splits">
<code>from_nested_row_splits</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_nested_row_splits(
    cls,
    flat_values,
    nested_row_splits,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> from a nested list of <code>row_splits</code> tensors.</p>
<h4 id="equivalent-to-1">Equivalent to:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> flat_values
<span class="cf">for</span> row_splits <span class="kw">in</span> <span class="bu">reversed</span>(nested_row_splits):
  result <span class="op">=</span> from_row_splits(result, row_splits)</code></pre></div>
<h4 id="args-33">Args:</h4>
<ul>
<li><b><code>flat_values</code></b>: A potentially ragged tensor.</li>
<li><b><code>nested_row_splits</code></b>: A list of 1-D integer tensors. The <code>i</code>th tensor is used as the <code>row_splits</code> for the <code>i</code>th ragged dimension.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-38">Returns:</h4>
<p>A <code>RaggedTensor</code> (or <code>flat_values</code> if <code>nested_row_splits</code> is empty).</p>
<h3 id="from_nested_value_rowids">
<code>from_nested_value_rowids</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_nested_value_rowids(
    cls,
    flat_values,
    nested_value_rowids,
    nested_nrows<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> from a nested list of <code>value_rowids</code> tensors.</p>
<h4 id="equivalent-to-2">Equivalent to:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> flat_values
<span class="cf">for</span> (rowids, nrows) <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">zip</span>(nested_value_rowids, nested_nrows)):
  result <span class="op">=</span> from_value_rowids(result, rowids, nrows)</code></pre></div>
<h4 id="args-34">Args:</h4>
<ul>
<li><b><code>flat_values</code></b>: A potentially ragged tensor.</li>
<li><b><code>nested_value_rowids</code></b>: A list of 1-D integer tensors. The <code>i</code>th tensor is used as the <code>value_rowids</code> for the <code>i</code>th ragged dimension.</li>
<li><b><code>nested_nrows</code></b>: A list of integer scalars. The <code>i</code>th scalar is used as the <code>nrows</code> for the <code>i</code>th ragged dimension.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-39">Returns:</h4>
<p>A <code>RaggedTensor</code> (or <code>flat_values</code> if <code>nested_value_rowids</code> is empty).</p>
<h4 id="raises-6">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>len(nested_values_rowids) != len(nested_nrows)</code>.</li>
</ul>
<h3 id="from_row_lengths">
<code>from_row_lengths</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_row_lengths(
    cls,
    values,
    row_lengths,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with rows partitioned by <code>row_lengths</code>.</p>
<p>The returned <code>RaggedTensor</code> corresponds with the python list defined by:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> [[values.pop(<span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(length)]
          <span class="cf">for</span> length <span class="kw">in</span> row_lengths]</code></pre></div>
<h4 id="args-35">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor with shape <code>[nvals, ...]</code>.</li>
<li><b><code>row_lengths</code></b>: A 1-D integer tensor with shape <code>[nrows]</code>. Must be nonnegative. <code>sum(row_lengths)</code> must be <code>nvals</code>.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-40">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = values.rank + 1</code>. <code>result.ragged_rank = values.ragged_rank + 1</code>.</p>
<h4 id="example-6">Example:</h4>
<p><code>python   &gt;&gt;&gt; print(tf.RaggedTensor.from_row_lengths(   ...     values=[3, 1, 4, 1, 5, 9, 2, 6],   ...     row_lengths=[4, 0, 3, 1, 0]))   &lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []])&gt;</code></p>
<h3 id="from_row_limits">
<code>from_row_limits</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_row_limits(
    cls,
    values,
    row_limits,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with rows partitioned by <code>row_limits</code>.</p>
<p>Equivalent to: <code>from_row_splits(values, concat([0, row_limits]))</code>.</p>
<h4 id="args-36">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor with shape <code>[nvals, ...]</code>.</li>
<li><b><code>row_limits</code></b>: A 1-D integer tensor with shape <code>[nrows]</code>. Must be sorted in ascending order. If <code>nrows&gt;0</code>, then <code>row_limits[-1]</code> must be <code>nvals</code>.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-41">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = values.rank + 1</code>. <code>result.ragged_rank = values.ragged_rank + 1</code>.</p>
<h4 id="example-7">Example:</h4>
<p><code>python   &gt;&gt;&gt; print(tf.RaggedTensor.from_row_limits(   ...     values=[3, 1, 4, 1, 5, 9, 2, 6],   ...     row_limits=[4, 4, 7, 8, 8]))   &lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</code></p>
<h3 id="from_row_splits">
<code>from_row_splits</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_row_splits(
    cls,
    values,
    row_splits,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with rows partitioned by <code>row_splits</code>.</p>
<p>The returned <code>RaggedTensor</code> corresponds with the python list defined by:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> [values[row_splits[i]:row_splits[i <span class="op">+</span> <span class="dv">1</span>]]
          <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(row_splits) <span class="op">-</span> <span class="dv">1</span>)]</code></pre></div>
<h4 id="args-37">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor with shape <code>[nvals, ...]</code>.</li>
<li><b><code>row_splits</code></b>: A 1-D integer tensor with shape <code>[nrows+1]</code>. Must not be empty, and must be sorted in ascending order. <code>row_splits[0]</code> must be zero and <code>row_splits[-1]</code> must be <code>nvals</code>.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-42">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = values.rank + 1</code>. <code>result.ragged_rank = values.ragged_rank + 1</code>.</p>
<h4 id="raises-7">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>row_splits</code> is an empty list.</li>
</ul>
<h4 id="example-8">Example:</h4>
<p><code>python   &gt;&gt;&gt; print(tf.RaggedTensor.from_row_splits(   ...     values=[3, 1, 4, 1, 5, 9, 2, 6],   ...     row_splits=[0, 4, 4, 7, 8, 8]))   &lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</code></p>
<h3 id="from_row_starts">
<code>from_row_starts</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_row_starts(
    cls,
    values,
    row_starts,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with rows partitioned by <code>row_starts</code>.</p>
<p>Equivalent to: <code>from_row_splits(values, concat([row_starts, nvals]))</code>.</p>
<h4 id="args-38">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor with shape <code>[nvals, ...]</code>.</li>
<li><b><code>row_starts</code></b>: A 1-D integer tensor with shape <code>[nrows]</code>. Must be nonnegative and sorted in ascending order. If <code>nrows&gt;0</code>, then <code>row_starts[0]</code> must be zero.</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-43">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = values.rank + 1</code>. <code>result.ragged_rank = values.ragged_rank + 1</code>.</p>
<h4 id="example-9">Example:</h4>
<p><code>python   &gt;&gt;&gt; print(tf.RaggedTensor.from_row_starts(   ...     values=[3, 1, 4, 1, 5, 9, 2, 6],   ...     row_starts=[0, 4, 4, 7, 8]))   &lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</code></p>
<h3 id="from_sparse">
<code>from_sparse</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_sparse(
    cls,
    st_input,
    name<span class="op">=</span><span class="va">None</span>,
    row_splits_dtype<span class="op">=</span>tf.dtypes.int64
)</code></pre></div>
<p>Converts a 2D <a href="../tf/sparse/SparseTensor.html"><code>tf.SparseTensor</code></a> to a <code>RaggedTensor</code>.</p>
<p>Each row of the <code>output</code> <code>RaggedTensor</code> will contain the explicit values from the same row in <code>st_input</code>. <code>st_input</code> must be ragged-right. If not it is not ragged-right, then an error will be generated.</p>
<h4 id="example-10">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> st <span class="op">=</span> SparseTensor(indices<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">2</span>], [<span class="dv">0</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">3</span>, <span class="dv">0</span>]],
...                   values<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],
...                   dense_shape<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">3</span>])
<span class="op">&gt;&gt;&gt;</span> rt.RaggedTensor.from_sparse(st).<span class="bu">eval</span>().tolist()
[[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>], [], [<span class="dv">5</span>]]</code></pre></div>
<p>Currently, only two-dimensional <code>SparseTensors</code> are supported.</p>
<h4 id="args-39">Args:</h4>
<ul>
<li><b><code>st_input</code></b>: The sparse tensor to convert. Must have rank 2.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensors (optional).</li>
<li><b><code>row_splits_dtype</code></b>: <code>dtype</code> for the returned <code>RaggedTensor</code>'s <code>row_splits</code> tensor. One of <a href="../tf.md#int32"><code>tf.int32</code></a> or <a href="../tf.md#int64"><code>tf.int64</code></a>.</li>
</ul>
<h4 id="returns-44">Returns:</h4>
<p>A <code>RaggedTensor</code> with the same values as <code>st_input</code>. <code>output.ragged_rank = rank(st_input) - 1</code>. <code>output.shape = [st_input.dense_shape[0], None]</code>.</p>
<h4 id="raises-8">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If the number of dimensions in <code>st_input</code> is not known statically, or is not two.</li>
</ul>
<h3 id="from_tensor">
<code>from_tensor</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_tensor(
    cls,
    tensor,
    lengths<span class="op">=</span><span class="va">None</span>,
    padding<span class="op">=</span><span class="va">None</span>,
    ragged_rank<span class="op">=</span><span class="dv">1</span>,
    name<span class="op">=</span><span class="va">None</span>,
    row_splits_dtype<span class="op">=</span>tf.dtypes.int64
)</code></pre></div>
<p>Converts a <a href="../tf/Tensor.html"><code>tf.Tensor</code></a> into a <code>RaggedTensor</code>.</p>
<p>The set of absent/default values may be specified using a vector of lengths or a padding value (but not both). If <code>lengths</code> is specified, then the output tensor will satisfy <code>output[row] = tensor[row][:lengths[row]]</code>. If 'lengths' is a list of lists or tuple of lists, those lists will be used as nested row lengths. If <code>padding</code> is specified, then any row <em>suffix</em> consisting entirely of <code>padding</code> will be excluded from the returned <code>RaggedTensor</code>. If neither <code>lengths</code> nor <code>padding</code> is specified, then the returned <code>RaggedTensor</code> will have no absent/default values.</p>
<h4 id="examples-2">Examples:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> dt <span class="op">=</span> tf.constant([[<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>], [<span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">0</span>]])
<span class="op">&gt;&gt;&gt;</span> tf.RaggedTensor.from_tensor(dt)
<span class="op">&lt;</span>tf.RaggedTensor [[<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>], [<span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">0</span>]]<span class="op">&gt;</span>
<span class="op">&gt;&gt;&gt;</span> tf.RaggedTensor.from_tensor(dt, lengths<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>])
<span class="op">&lt;</span>tf.RaggedTensor [[<span class="dv">5</span>], [], [<span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">0</span>]]<span class="op">&gt;</span>

<span class="op">&gt;&gt;&gt;</span> tf.RaggedTensor.from_tensor(dt, padding<span class="op">=</span><span class="dv">0</span>)
<span class="op">&lt;</span>tf.RaggedTensor [[<span class="dv">5</span>, <span class="dv">7</span>], [<span class="dv">0</span>, <span class="dv">3</span>], [<span class="dv">6</span>]]<span class="op">&gt;</span>

<span class="op">&gt;&gt;&gt;</span> dt <span class="op">=</span> tf.constant([[[<span class="dv">5</span>, <span class="dv">0</span>], [<span class="dv">7</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>]],
                      [[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">3</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>]],
                      [[<span class="dv">6</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>]]])
<span class="op">&gt;&gt;&gt;</span> tf.RaggedTensor.from_tensor(dt, lengths<span class="op">=</span>([<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>]))
<span class="op">&lt;</span>tf.RaggedTensor [[[<span class="dv">5</span>], [<span class="dv">7</span>]], [], [[<span class="dv">6</span>, <span class="dv">0</span>], [], [<span class="dv">0</span>]]]<span class="op">&gt;</span></code></pre></div>
<h4 id="args-40">Args:</h4>
<ul>
<li><b><code>tensor</code></b>: The <code>Tensor</code> to convert. Must have rank <code>ragged_rank + 1</code> or higher.</li>
<li><b><code>lengths</code></b>: An optional set of row lengths, specified using a 1-D integer <code>Tensor</code> whose length is equal to <code>tensor.shape[0]</code> (the number of rows in <code>tensor</code>). If specified, then <code>output[row]</code> will contain <code>tensor[row][:lengths[row]]</code>. Negative lengths are treated as zero. You may optionally pass a list or tuple of lengths to this argument, which will be used as nested row lengths to construct a ragged tensor with multiple ragged dimensions.</li>
<li><b><code>padding</code></b>: An optional padding value. If specified, then any row suffix consisting entirely of <code>padding</code> will be excluded from the returned RaggedTensor. <code>padding</code> is a <code>Tensor</code> with the same dtype as <code>tensor</code> and with <code>shape=tensor.shape[ragged_rank + 1:]</code>.</li>
<li><b><code>ragged_rank</code></b>: Integer specifying the ragged rank for the returned <code>RaggedTensor</code>. Must be greater than zero.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensors (optional).</li>
<li><b><code>row_splits_dtype</code></b>: <code>dtype</code> for the returned <code>RaggedTensor</code>'s <code>row_splits</code> tensor. One of <a href="../tf.md#int32"><code>tf.int32</code></a> or <a href="../tf.md#int64"><code>tf.int64</code></a>.</li>
</ul>
<h4 id="returns-45">Returns:</h4>
<p>A <code>RaggedTensor</code> with the specified <code>ragged_rank</code>. The shape of the returned ragged tensor is compatible with the shape of <code>tensor</code>.</p>
<h4 id="raises-9">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If both <code>lengths</code> and <code>padding</code> are specified.</li>
</ul>
<h3 id="from_value_rowids">
<code>from_value_rowids</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="at">@classmethod</span>
from_value_rowids(
    cls,
    values,
    value_rowids,
    nrows<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>,
    validate<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Creates a <code>RaggedTensor</code> with rows partitioned by <code>value_rowids</code>.</p>
<p>The returned <code>RaggedTensor</code> corresponds with the python list defined by:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> [[values[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(values)) <span class="cf">if</span> value_rowids[i] <span class="op">==</span> row]
          <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(nrows)]</code></pre></div>
<h4 id="args-41">Args:</h4>
<ul>
<li><b><code>values</code></b>: A potentially ragged tensor with shape <code>[nvals, ...]</code>.</li>
<li><b><code>value_rowids</code></b>: A 1-D integer tensor with shape <code>[nvals]</code>, which corresponds one-to-one with <code>values</code>, and specifies each value's row index. Must be nonnegative, and must be sorted in ascending order.</li>
<li><b><code>nrows</code></b>: An integer scalar specifying the number of rows. This should be specified if the <code>RaggedTensor</code> may containing empty training rows. Must be greater than <code>value_rowids[-1]</code> (or zero if <code>value_rowids</code> is empty). Defaults to <code>value_rowids[-1]</code> (or zero if <code>value_rowids</code> is empty).</li>
<li><b><code>name</code></b>: A name prefix for the RaggedTensor (optional).</li>
<li><b><code>validate</code></b>: If true, then use assertions to check that the arguments form a valid <code>RaggedTensor</code>.</li>
</ul>
<h4 id="returns-46">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = values.rank + 1</code>. <code>result.ragged_rank = values.ragged_rank + 1</code>.</p>
<h4 id="raises-10">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>nrows</code> is incompatible with <code>value_rowids</code>.</li>
</ul>
<h4 id="example-11">Example:</h4>
<p><code>python   &gt;&gt;&gt; print(tf.RaggedTensor.from_value_rowids(   ...     values=[3, 1, 4, 1, 5, 9, 2, 6],   ...     value_rowids=[0, 0, 0, 0, 2, 2, 2, 3],   ...     nrows=5))   &lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;</code></p>
<h3 id="nested_row_lengths">
<code>nested_row_lengths</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nested_row_lengths(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Returns a tuple containing the row_lengths for all ragged dimensions.</p>
<p><code>rtnested_row_lengths()</code> is a tuple containing the <code>row_lengths</code> tensors for all ragged dimensions in <code>rt</code>, ordered from outermost to innermost.</p>
<h4 id="args-42">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name prefix for the returned tensors (optional).</li>
</ul>
<h4 id="returns-47">Returns:</h4>
<p>A <code>tuple</code> of 1-D integer <code>Tensors</code>. The length of the tuple is equal to <code>self.ragged_rank</code>.</p>
<h3 id="nrows">
<code>nrows</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nrows(
    out_type<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the number of rows in this ragged tensor.</p>
<p>I.e., the size of the outermost dimension of the tensor.</p>
<h4 id="args-43">Args:</h4>
<ul>
<li><b><code>out_type</code></b>: <code>dtype</code> for the returned tensor. Defaults to <code>self.row_splits.dtype</code>.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
</ul>
<h4 id="returns-48">Returns:</h4>
<p>A scalar <code>Tensor</code> with dtype <code>out_type</code>.</p>
<h4 id="example-12">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; rt.nrows()  # rt has 5 rows.   5</code></p>
<h3 id="row_lengths">
<code>row_lengths</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">row_lengths(
    axis<span class="op">=</span><span class="dv">1</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the lengths of the rows in this ragged tensor.</p>
<p><code>rt.row_lengths()[i]</code> indicates the number of values in the <code>i</code>th row of <code>rt</code>.</p>
<h4 id="args-44">Args:</h4>
<ul>
<li><b><code>axis</code></b>: An integer constant indicating the axis whose row lengths should be returned.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
</ul>
<h4 id="returns-49">Returns:</h4>
<p>A potentially ragged integer Tensor with shape <code>self.shape[:axis]</code>.</p>
<h4 id="raises-11">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>axis</code> is out of bounds.</li>
</ul>
<h4 id="example-13">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[[3, 1, 4], [1]], [], [[5, 9], [2]], [[6]], []])   &gt;&gt;&gt; rt.row_lengths(rt)  # lengths of rows in rt   tf.Tensor([2, 0, 2, 1, 0])   &gt;&gt;&gt; rt.row_lengths(axis=2)  # lengths of axis=2 rows.   &lt;tf.RaggedTensor [[3, 1], [], [2, 1], [1], []]&gt;</code></p>
<h3 id="row_limits">
<code>row_limits</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">row_limits(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Returns the limit indices for rows in this ragged tensor.</p>
<p>These indices specify where the values for each row end in <code>self.values</code>. <code>rt.row_limits(self)</code> is equal to <code>rt.row_splits[:-1]</code>.</p>
<h4 id="args-45">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
</ul>
<h4 id="returns-50">Returns:</h4>
<p>A 1-D integer Tensor with shape <code>[nrows]</code>. The returned tensor is nonnegative, and is sorted in ascending order.</p>
<h4 id="example-14">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; rt.values   tf.Tensor([3, 1, 4, 1, 5, 9, 2, 6])   &gt;&gt;&gt; rt.row_limits()  # indices of row limits in rt.values   tf.Tensor([4, 4, 7, 8, 8])</code></p>
<h3 id="row_starts">
<code>row_starts</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">row_starts(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Returns the start indices for rows in this ragged tensor.</p>
<p>These indices specify where the values for each row begin in <code>self.values</code>. <code>rt.row_starts()</code> is equal to <code>rt.row_splits[:-1]</code>.</p>
<h4 id="args-46">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
</ul>
<h4 id="returns-51">Returns:</h4>
<p>A 1-D integer Tensor with shape <code>[nrows]</code>. The returned tensor is nonnegative, and is sorted in ascending order.</p>
<h4 id="example-15">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; rt.values   tf.Tensor([3, 1, 4, 1, 5, 9, 2, 6])   &gt;&gt;&gt; rt.row_starts()  # indices of row starts in rt.values   tf.Tensor([0, 4, 4, 7, 8])</code></p>
<h3 id="to_list">
<code>to_list</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">to_list()</code></pre></div>
<p>Returns a nested Python <code>list</code> with the values for this <code>RaggedTensor</code>.</p>
<p>Requires that <code>rt</code> was constructed in eager execution mode.</p>
<h4 id="returns-52">Returns:</h4>
<p>A nested Python <code>list</code>.</p>
<h3 id="to_sparse">
<code>to_sparse</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">to_sparse(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Converts this <code>RaggedTensor</code> into a <a href="../tf/sparse/SparseTensor.html"><code>tf.SparseTensor</code></a>.</p>
<h4 id="example-16">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> rt <span class="op">=</span> ragged.constant([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>], [], [<span class="dv">5</span>, <span class="dv">6</span>]])
<span class="op">&gt;&gt;&gt;</span> rt.to_sparse().<span class="bu">eval</span>()
SparseTensorValue(indices<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">2</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">3</span>, <span class="dv">0</span>], [<span class="dv">3</span>, <span class="dv">1</span>]],
                  values<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>],
                  dense_shape<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">3</span>])</code></pre></div>
<h4 id="args-47">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name prefix for the returned tensors (optional).</li>
</ul>
<h4 id="returns-53">Returns:</h4>
<p>A SparseTensor with the same values as <code>self</code>.</p>
<h3 id="to_tensor">
<code>to_tensor</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">to_tensor(
    default_value<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Converts this <code>RaggedTensor</code> into a <a href="../tf/Tensor.html"><code>tf.Tensor</code></a>.</p>
<h4 id="example-17">Example:</h4>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> rt <span class="op">=</span> ragged.constant([[<span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">7</span>], [], [<span class="dv">6</span>, <span class="dv">5</span>], [<span class="dv">4</span>]])
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span> rt.to_tensor()
[[<span class="dv">9</span> <span class="dv">8</span> <span class="dv">7</span>]
 [<span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span>]
 [<span class="dv">6</span> <span class="dv">5</span> <span class="dv">0</span>]
 [<span class="dv">4</span> <span class="dv">0</span> <span class="dv">0</span>]]</code></pre></div>
<h4 id="args-48">Args:</h4>
<ul>
<li><b><code>default_value</code></b>: Value to set for indices not specified in <code>self</code>. Defaults to zero. <code>default_value</code> must be broadcastable to <code>self.shape[self.ragged_rank + 1:]</code>.</li>
<li><b><code>name</code></b>: A name prefix for the returned tensors (optional).</li>
</ul>
<h4 id="returns-54">Returns:</h4>
<p>A <code>Tensor</code> with shape <code>ragged.bounding_shape(self)</code> and the values specified by the non-empty values in <code>self</code>. Empty values are assigned <code>default_value</code>.</p>
<h3 id="value_rowids">
<code>value_rowids</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">value_rowids(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Returns the row indices for the <code>values</code> in this ragged tensor.</p>
<p><code>rt.value_rowids()</code> corresponds one-to-one with the outermost dimension of <code>rt.values</code>, and specifies the row containing each value. In particular, the row <code>rt[row]</code> consists of the values <code>rt.values[j]</code> where <code>rt.value_rowids()[j] == row</code>.</p>
<h4 id="args-49">Args:</h4>
<ul>
<li><b><code>name</code></b>: A name prefix for the returned tensor (optional).</li>
</ul>
<h4 id="returns-55">Returns:</h4>
<p>A 1-D integer <code>Tensor</code> with shape <code>self.values.shape[:1]</code>. The returned tensor is nonnegative, and is sorted in ascending order.</p>
<h4 id="example-18">Example:</h4>
<p><code>python   &gt;&gt;&gt; rt = ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])   &gt;&gt;&gt; rt.values   tf.Tensor([3, 1, 4, 1, 5, 9, 2, 6])   &gt;&gt;&gt; rt.value_rowids()   tf.Tensor([0, 0, 0, 0, 2, 2, 2, 3])  # corresponds 1:1 with rt.values</code></p>
<h3 id="with_flat_values">
<code>with_flat_values</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">with_flat_values(new_values)</code></pre></div>
<p>Returns a copy of <code>self</code> with <code>flat_values</code> replaced by <code>new_value</code>.</p>
<p>Preserves cached row-partitioning tensors such as <code>self.cached_nrows</code> and <code>self.cached_value_rowids</code> if they have values.</p>
<h4 id="args-50">Args:</h4>
<ul>
<li><b><code>new_values</code></b>: Potentially ragged tensor that should replace <code>self.flat_values</code>. Must have <code>rank &gt; 0</code>, and must have the same number of rows as <code>self.flat_values</code>.</li>
</ul>
<h4 id="returns-56">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = self.ragged_rank + new_values.rank</code>. <code>result.ragged_rank = self.ragged_rank + new_values.ragged_rank</code>.</p>
<h3 id="with_row_splits_dtype">
<code>with_row_splits_dtype</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">with_row_splits_dtype(dtype)</code></pre></div>
<p>Returns a copy of this RaggedTensor with the given <code>row_splits</code> dtype.</p>
<p>For RaggedTensors with multiple ragged dimensions, the <code>row_splits</code> for all nested <code>RaggedTensor</code> objects are cast to the given dtype.</p>
<h4 id="args-51">Args:</h4>
<ul>
<li><b><code>dtype</code></b>: The dtype for <code>row_splits</code>. One of <a href="../tf.md#int32"><code>tf.int32</code></a> or <a href="../tf.md#int64"><code>tf.int64</code></a>.</li>
</ul>
<h4 id="returns-57">Returns:</h4>
<p>A copy of this RaggedTensor, with the <code>row_splits</code> cast to the given type.</p>
<h3 id="with_values">
<code>with_values</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">with_values(new_values)</code></pre></div>
<p>Returns a copy of <code>self</code> with <code>values</code> replaced by <code>new_value</code>.</p>
<p>Preserves cached row-partitioning tensors such as <code>self.cached_nrows</code> and <code>self.cached_value_rowids</code> if they have values.</p>
<h4 id="args-52">Args:</h4>
<ul>
<li><b><code>new_values</code></b>: Potentially ragged tensor to use as the <code>values</code> for the returned <code>RaggedTensor</code>. Must have <code>rank &gt; 0</code>, and must have the same number of rows as <code>self.values</code>.</li>
</ul>
<h4 id="returns-58">Returns:</h4>
<p>A <code>RaggedTensor</code>. <code>result.rank = 1 + new_values.rank</code>. <code>result.ragged_rank = 1 + new_values.ragged_rank</code></p>
