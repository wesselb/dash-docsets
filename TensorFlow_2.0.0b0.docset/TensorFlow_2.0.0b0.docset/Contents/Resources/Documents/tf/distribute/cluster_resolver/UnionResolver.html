<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.distribute.cluster_resolver.UnionResolver" /> <meta itemprop="path" content="Stable" /> <meta itemprop="property" content="environment"/> <meta itemprop="property" content="rpc_layer"/> <meta itemprop="property" content="task_id"/> <meta itemprop="property" content="task_type"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="cluster_spec"/> <meta itemprop="property" content="master"/> <meta itemprop="property" content="num_accelerators"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.distribute.cluster_resolver.UnionResolver" class="dashAnchor"></a><h1 id="tf.distribute.cluster_resolver.unionresolver">tf.distribute.cluster_resolver.UnionResolver</h1>
<h2 id="class-unionresolver">Class <code>UnionResolver</code></h2>
<p>Performs a union on underlying ClusterResolvers.</p>
<p>Inherits From: <a href="../../../tf/distribute/cluster_resolver/ClusterResolver.html"><code>ClusterResolver</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.distribute.cluster_resolver.UnionResolver</code></li>
<li>Class <code>tf.compat.v2.distribute.cluster_resolver.UnionResolver</code></li>
<li>Class <code>tf.distribute.cluster_resolver.UnionResolver</code></li>
</ul>
<p>Defined in <a href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py"><code>python/distribute/cluster_resolver/cluster_resolver.py</code></a>.</p>
<!-- Placeholder for "Used in" -->
<p>This class performs a union given two or more existing ClusterResolvers. It merges the underlying ClusterResolvers, and returns one unified ClusterSpec when cluster_spec is called. The details of the merge function is documented in the cluster_spec function.</p>
<p>For additional Cluster Resolver properties such as task type, task index, rpc layer, environment, etc..., we will return the value from the first ClusterResolver in the union.</p>
<h2 id="__init__">
<code><strong>init</strong></code>
</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Initializes a UnionClusterResolver with other ClusterResolvers.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>*args</code></b>: <code>ClusterResolver</code> objects to be unionized.</li>
<li><b><code>**kwargs</code></b>: rpc_layer - (Optional) Override value for the RPC layer used by TensorFlow. task_type - (Optional) Override value for the current task type. task_id - (Optional) Override value for the current task index.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If any argument is not a subclass of <code>ClusterResolvers</code>.</li>
<li><b><code>ValueError</code></b>: If there are no arguments passed.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="environment">
<code>environment</code>
</h3>
<p>Returns the current environment which TensorFlow is running in.</p>
<p>There are two possible return values, &quot;google&quot; (when TensorFlow is running in a Google-internal environment) or an empty string (when TensorFlow is running elsewhere).</p>
<p>If you are implementing a ClusterResolver that works in both the Google environment and the open-source world (for instance, a TPU ClusterResolver or similar), you will have to return the appropriate string depending on the environment, which you will have to detect.</p>
<p>Otherwise, if you are implementing a ClusterResolver that will only work in open-source TensorFlow, you do not need to implement this property.</p>
<h3 id="rpc_layer">
<code>rpc_layer</code>
</h3>
<h3 id="task_id">
<code>task_id</code>
</h3>
<h3 id="task_type">
<code>task_type</code>
</h3>
<h2 id="methods">Methods</h2>
<h3 id="cluster_spec">
<code>cluster_spec</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">cluster_spec()</code></pre></div>
<p>Returns a union of all the ClusterSpecs from the ClusterResolvers.</p>
<h4 id="returns">Returns:</h4>
<p>A ClusterSpec containing host information merged from all the underlying ClusterResolvers.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>KeyError</code></b>: If there are conflicting keys detected when merging two or more dictionaries, this exception is raised.</li>
</ul>
<p>Note: If there are multiple ClusterResolvers exposing ClusterSpecs with the same job name, we will merge the list/dict of workers.</p>
<p>If <em>all</em> underlying ClusterSpecs expose the set of workers as lists, we will concatenate the lists of workers, starting with the list of workers from the first ClusterResolver passed into the constructor.</p>
<p>If <em>any</em> of the ClusterSpecs expose the set of workers as a dict, we will treat all the sets of workers as dicts (even if they are returned as lists) and will only merge them into a dict if there is no conflicting keys. If there is a conflicting key, we will raise a <code>KeyError</code>.</p>
<h3 id="master">
<code>master</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">master(
    task_type<span class="op">=</span><span class="va">None</span>,
    task_id<span class="op">=</span><span class="va">None</span>,
    rpc_layer<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the master address to use when creating a session.</p>
<p>This usually returns the master from the first ClusterResolver passed in, but you can override this by specifying the task_type and task_id.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the master.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the master.</li>
<li><b><code>rpc_layer</code></b>: (Optional) The RPC protocol for the given cluster.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>The name or URL of the session master.</p>
<h3 id="num_accelerators">
<code>num_accelerators</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">num_accelerators(
    task_type<span class="op">=</span><span class="va">None</span>,
    task_id<span class="op">=</span><span class="va">None</span>,
    config_proto<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Returns the number of accelerator cores per worker.</p>
<p>This returns the number of accelerator cores (such as GPUs and TPUs) available per worker.</p>
<p>Optionally, we allow callers to specify the task_type, and task_id, for if they want to target a specific TensorFlow process to query the number of accelerators. This is to support heterogenous environments, where the number of accelerators cores per host is different.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the machine we want to query.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the machine we want to query.</li>
<li><b><code>config_proto</code></b>: (Optional) Configuration for starting a new session to query how many accelerator cores it has.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>A map of accelerator types to number of cores.</p>
