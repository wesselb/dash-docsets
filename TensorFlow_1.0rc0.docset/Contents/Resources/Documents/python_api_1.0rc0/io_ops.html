<!-- This file is machine generated: DO NOT EDIT! -->
<h1 id="inputs-and-readers">Inputs and Readers</h1>
<p>Note: Functions taking <code>Tensor</code> arguments can also take anything accepted by <a href="framework.md#convert_to_tensor"><code>tf.convert_to_tensor</code></a>.</p>
<p>[TOC]</p>
<h2 id="placeholders">Placeholders</h2>
<p>TensorFlow provides a placeholder operation that must be fed with data on execution. For more info, see the section on <a href="../../how_tos/reading_data/index.md#feeding">Feeding data</a>.</p>
<hr />
<h3 id="tf.placeholderdtype-shapenone-namenone"><a name="//apple_ref/cpp/Function/placeholder" class="dashAnchor"></a><code id="placeholder">tf.placeholder(dtype, shape=None, name=None)</code></h3>
<p>Inserts a placeholder for a tensor that will be always fed.</p>
<p><strong>Important</strong>: This tensor will produce an error if evaluated. Its value must be fed using the <code>feed_dict</code> optional argument to <code>Session.run()</code>, <code>Tensor.eval()</code>, or <code>Operation.run()</code>.</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>(<span class="dv">1024</span>, <span class="dv">1024</span>))
y <span class="op">=</span> tf.matmul(x, x)

<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
  <span class="bu">print</span>(sess.run(y))  <span class="co"># ERROR: will fail because x was not fed.</span>

  rand_array <span class="op">=</span> np.random.rand(<span class="dv">1024</span>, <span class="dv">1024</span>)
  <span class="bu">print</span>(sess.run(y, feed_dict<span class="op">=</span>{x: rand_array}))  <span class="co"># Will succeed.</span></code></pre></div>
<h5 id="args">Args:</h5>
<ul>
<li><b><code>dtype</code></b>: The type of elements in the tensor to be fed.</li>
<li><b><code>shape</code></b>: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns">Returns:</h5>
<p>A <code>Tensor</code> that may be used as a handle for feeding a value, but not evaluated directly.</p>
<hr />
<h3 id="tf.placeholder_with_defaultinput-shape-namenone"><a name="//apple_ref/cpp/Function/placeholder_with_default" class="dashAnchor"></a><code id="placeholder_with_default">tf.placeholder_with_default(input, shape, name=None)</code></h3>
<p>A placeholder op that passes through <code>input</code> when its output is not fed.</p>
<h5 id="args-1">Args:</h5>
<ul>
<li><b><code>input</code></b>: A <code>Tensor</code>. The default value to produce when <code>output</code> is not fed.</li>
<li><b><code>shape</code></b>: A <code>tf.TensorShape</code> or list of <code>ints</code>. The (possibly partial) shape of the tensor.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-1">Returns:</h5>
<p>A <code>Tensor</code>. Has the same type as <code>input</code>. A placeholder tensor that defaults to <code>input</code> if it is not fed.</p>
<p>For feeding <code>SparseTensor</code>s which are composite type, there is a convenience function:</p>
<hr />
<h3 id="tf.sparse_placeholderdtype-shapenone-namenone"><a name="//apple_ref/cpp/Function/sparse_placeholder" class="dashAnchor"></a><code id="sparse_placeholder">tf.sparse_placeholder(dtype, shape=None, name=None)</code></h3>
<p>Inserts a placeholder for a sparse tensor that will be always fed.</p>
<p><strong>Important</strong>: This sparse tensor will produce an error if evaluated. Its value must be fed using the <code>feed_dict</code> optional argument to <code>Session.run()</code>, <code>Tensor.eval()</code>, or <code>Operation.run()</code>.</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> tf.sparse_placeholder(tf.float32)
y <span class="op">=</span> tf.sparse_reduce_sum(x)

<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
  <span class="bu">print</span>(sess.run(y))  <span class="co"># ERROR: will fail because x was not fed.</span>

  indices <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">0</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>np.int64)
  values <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">2.0</span>], dtype<span class="op">=</span>np.float32)
  shape <span class="op">=</span> np.array([<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">2</span>], dtype<span class="op">=</span>np.int64)
  <span class="bu">print</span>(sess.run(y, feed_dict<span class="op">=</span>{
    x: tf.SparseTensorValue(indices, values, shape)}))  <span class="co"># Will succeed.</span>
  <span class="bu">print</span>(sess.run(y, feed_dict<span class="op">=</span>{
    x: (indices, values, shape)}))  <span class="co"># Will succeed.</span>

  sp <span class="op">=</span> tf.SparseTensor(indices<span class="op">=</span>indices, values<span class="op">=</span>values, dense_shape<span class="op">=</span>shape)
  sp_value <span class="op">=</span> sp.<span class="bu">eval</span>(session)
  <span class="bu">print</span>(sess.run(y, feed_dict<span class="op">=</span>{x: sp_value}))  <span class="co"># Will succeed.</span></code></pre></div>
<h5 id="args-2">Args:</h5>
<ul>
<li><b><code>dtype</code></b>: The type of <code>values</code> elements in the tensor to be fed.</li>
<li><b><code>shape</code></b>: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a sparse tensor of any shape.</li>
<li><b><code>name</code></b>: A name for prefixing the operations (optional).</li>
</ul>
<h5 id="returns-2">Returns:</h5>
<p>A <code>SparseTensor</code> that may be used as a handle for feeding a value, but not evaluated directly.</p>
<h2 id="readers">Readers</h2>
<p>TensorFlow provides a set of Reader classes for reading data formats. For more information on inputs and readers, see <a href="../../how_tos/reading_data/index.md">Reading data</a>.</p>
<hr />
<h3 id="class-tf.readerbase"><a name="//apple_ref/cpp/Class/ReaderBase" class="dashAnchor"></a><code id="ReaderBase">class tf.ReaderBase</code></h3>
<p>Base class for different Reader types, that produce a record every step.</p>
<p>Conceptually, Readers convert string 'work units' into records (key, value pairs). Typically the 'work units' are filenames and the records are extracted from the contents of those files. We want a single record produced per step, but a work unit can correspond to many records.</p>
<p>Therefore we introduce some decoupling using a queue. The queue contains the work units and the Reader dequeues from the queue when it is asked to produce a record (via Read()) but it has finished the last work unit. - - -</p>
<h4 id="tf.readerbase.__init__reader_ref-supports_serializefalse"><code id="ReaderBase.__init__">tf.ReaderBase.__init__(reader_ref, supports_serialize=False)</code></h4>
<p>Creates a new ReaderBase.</p>
<h5 id="args-3">Args:</h5>
<ul>
<li><b><code>reader_ref</code></b>: The operation that implements the reader.</li>
<li><b><code>supports_serialize</code></b>: True if the reader implementation can serialize its state.</li>
</ul>
<hr />
<h4 id="tf.readerbase.num_records_producednamenone"><code id="ReaderBase.num_records_produced">tf.ReaderBase.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-4">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-3">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.readerbase.num_work_units_completednamenone"><code id="ReaderBase.num_work_units_completed">tf.ReaderBase.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-5">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-4">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.readerbase.readqueue-namenone"><code id="ReaderBase.read">tf.ReaderBase.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-6">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-5">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.readerbase.read_up_toqueue-num_records-namenone"><code id="ReaderBase.read_up_to">tf.ReaderBase.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-7">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-6">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.readerbase.reader_ref"><code id="ReaderBase.reader_ref">tf.ReaderBase.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.readerbase.resetnamenone"><code id="ReaderBase.reset">tf.ReaderBase.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-8">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-7">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.readerbase.restore_statestate-namenone"><code id="ReaderBase.restore_state">tf.ReaderBase.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-9">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-8">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.readerbase.serialize_statenamenone"><code id="ReaderBase.serialize_state">tf.ReaderBase.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-10">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-9">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.readerbase.supports_serialize"><code id="ReaderBase.supports_serialize">tf.ReaderBase.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<hr />
<h3 id="class-tf.textlinereader"><a name="//apple_ref/cpp/Class/TextLineReader" class="dashAnchor"></a><code id="TextLineReader">class tf.TextLineReader</code></h3>
<p>A Reader that outputs the lines of a file delimited by newlines.</p>
<p>Newlines are stripped from the output. See ReaderBase for supported methods. - - -</p>
<h4 id="tf.textlinereader.__init__skip_header_linesnone-namenone"><code id="TextLineReader.__init__">tf.TextLineReader.__init__(skip_header_lines=None, name=None)</code></h4>
<p>Create a TextLineReader.</p>
<h5 id="args-11">Args:</h5>
<ul>
<li><b><code>skip_header_lines</code></b>: An optional int. Defaults to 0. Number of lines to skip from the beginning of every file.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<hr />
<h4 id="tf.textlinereader.num_records_producednamenone"><code id="TextLineReader.num_records_produced">tf.TextLineReader.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-12">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-10">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.textlinereader.num_work_units_completednamenone"><code id="TextLineReader.num_work_units_completed">tf.TextLineReader.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-13">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-11">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.textlinereader.readqueue-namenone"><code id="TextLineReader.read">tf.TextLineReader.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-14">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-12">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.textlinereader.read_up_toqueue-num_records-namenone"><code id="TextLineReader.read_up_to">tf.TextLineReader.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-15">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-13">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.textlinereader.reader_ref"><code id="TextLineReader.reader_ref">tf.TextLineReader.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.textlinereader.resetnamenone"><code id="TextLineReader.reset">tf.TextLineReader.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-16">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-14">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.textlinereader.restore_statestate-namenone"><code id="TextLineReader.restore_state">tf.TextLineReader.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-17">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-15">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.textlinereader.serialize_statenamenone"><code id="TextLineReader.serialize_state">tf.TextLineReader.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-18">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-16">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.textlinereader.supports_serialize"><code id="TextLineReader.supports_serialize">tf.TextLineReader.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<hr />
<h3 id="class-tf.wholefilereader"><a name="//apple_ref/cpp/Class/WholeFileReader" class="dashAnchor"></a><code id="WholeFileReader">class tf.WholeFileReader</code></h3>
<p>A Reader that outputs the entire contents of a file as a value.</p>
<p>To use, enqueue filenames in a Queue. The output of Read will be a filename (key) and the contents of that file (value).</p>
<p>See ReaderBase for supported methods. - - -</p>
<h4 id="tf.wholefilereader.__init__namenone"><code id="WholeFileReader.__init__">tf.WholeFileReader.__init__(name=None)</code></h4>
<p>Create a WholeFileReader.</p>
<h5 id="args-19">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<hr />
<h4 id="tf.wholefilereader.num_records_producednamenone"><code id="WholeFileReader.num_records_produced">tf.WholeFileReader.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-20">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-17">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.wholefilereader.num_work_units_completednamenone"><code id="WholeFileReader.num_work_units_completed">tf.WholeFileReader.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-21">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-18">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.wholefilereader.readqueue-namenone"><code id="WholeFileReader.read">tf.WholeFileReader.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-22">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-19">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.wholefilereader.read_up_toqueue-num_records-namenone"><code id="WholeFileReader.read_up_to">tf.WholeFileReader.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-23">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-20">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.wholefilereader.reader_ref"><code id="WholeFileReader.reader_ref">tf.WholeFileReader.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.wholefilereader.resetnamenone"><code id="WholeFileReader.reset">tf.WholeFileReader.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-24">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-21">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.wholefilereader.restore_statestate-namenone"><code id="WholeFileReader.restore_state">tf.WholeFileReader.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-25">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-22">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.wholefilereader.serialize_statenamenone"><code id="WholeFileReader.serialize_state">tf.WholeFileReader.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-26">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-23">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.wholefilereader.supports_serialize"><code id="WholeFileReader.supports_serialize">tf.WholeFileReader.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<hr />
<h3 id="class-tf.identityreader"><a name="//apple_ref/cpp/Class/IdentityReader" class="dashAnchor"></a><code id="IdentityReader">class tf.IdentityReader</code></h3>
<p>A Reader that outputs the queued work as both the key and value.</p>
<p>To use, enqueue strings in a Queue. Read will take the front work string and output (work, work).</p>
<p>See ReaderBase for supported methods. - - -</p>
<h4 id="tf.identityreader.__init__namenone"><code id="IdentityReader.__init__">tf.IdentityReader.__init__(name=None)</code></h4>
<p>Create a IdentityReader.</p>
<h5 id="args-27">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<hr />
<h4 id="tf.identityreader.num_records_producednamenone"><code id="IdentityReader.num_records_produced">tf.IdentityReader.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-28">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-24">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.identityreader.num_work_units_completednamenone"><code id="IdentityReader.num_work_units_completed">tf.IdentityReader.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-29">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-25">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.identityreader.readqueue-namenone"><code id="IdentityReader.read">tf.IdentityReader.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-30">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-26">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.identityreader.read_up_toqueue-num_records-namenone"><code id="IdentityReader.read_up_to">tf.IdentityReader.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-31">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-27">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.identityreader.reader_ref"><code id="IdentityReader.reader_ref">tf.IdentityReader.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.identityreader.resetnamenone"><code id="IdentityReader.reset">tf.IdentityReader.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-32">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-28">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.identityreader.restore_statestate-namenone"><code id="IdentityReader.restore_state">tf.IdentityReader.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-33">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-29">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.identityreader.serialize_statenamenone"><code id="IdentityReader.serialize_state">tf.IdentityReader.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-34">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-30">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.identityreader.supports_serialize"><code id="IdentityReader.supports_serialize">tf.IdentityReader.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<hr />
<h3 id="class-tf.tfrecordreader"><a name="//apple_ref/cpp/Class/TFRecordReader" class="dashAnchor"></a><code id="TFRecordReader">class tf.TFRecordReader</code></h3>
<p>A Reader that outputs the records from a TFRecords file.</p>
<p>See ReaderBase for supported methods. - - -</p>
<h4 id="tf.tfrecordreader.__init__namenone-optionsnone"><code id="TFRecordReader.__init__">tf.TFRecordReader.__init__(name=None, options=None)</code></h4>
<p>Create a TFRecordReader.</p>
<h5 id="args-35">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
<li><b><code>options</code></b>: A TFRecordOptions object (optional).</li>
</ul>
<hr />
<h4 id="tf.tfrecordreader.num_records_producednamenone"><code id="TFRecordReader.num_records_produced">tf.TFRecordReader.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-36">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-31">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.tfrecordreader.num_work_units_completednamenone"><code id="TFRecordReader.num_work_units_completed">tf.TFRecordReader.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-37">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-32">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.tfrecordreader.readqueue-namenone"><code id="TFRecordReader.read">tf.TFRecordReader.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-38">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-33">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.tfrecordreader.read_up_toqueue-num_records-namenone"><code id="TFRecordReader.read_up_to">tf.TFRecordReader.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-39">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-34">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.tfrecordreader.reader_ref"><code id="TFRecordReader.reader_ref">tf.TFRecordReader.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.tfrecordreader.resetnamenone"><code id="TFRecordReader.reset">tf.TFRecordReader.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-40">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-35">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.tfrecordreader.restore_statestate-namenone"><code id="TFRecordReader.restore_state">tf.TFRecordReader.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-41">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-36">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.tfrecordreader.serialize_statenamenone"><code id="TFRecordReader.serialize_state">tf.TFRecordReader.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-42">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-37">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.tfrecordreader.supports_serialize"><code id="TFRecordReader.supports_serialize">tf.TFRecordReader.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<hr />
<h3 id="class-tf.fixedlengthrecordreader"><a name="//apple_ref/cpp/Class/FixedLengthRecordReader" class="dashAnchor"></a><code id="FixedLengthRecordReader">class tf.FixedLengthRecordReader</code></h3>
<p>A Reader that outputs fixed-length records from a file.</p>
<p>See ReaderBase for supported methods. - - -</p>
<h4 id="tf.fixedlengthrecordreader.__init__record_bytes-header_bytesnone-footer_bytesnone-namenone"><code id="FixedLengthRecordReader.__init__">tf.FixedLengthRecordReader.__init__(record_bytes, header_bytes=None, footer_bytes=None, name=None)</code></h4>
<p>Create a FixedLengthRecordReader.</p>
<h5 id="args-43">Args:</h5>
<ul>
<li><b><code>record_bytes</code></b>: An int.</li>
<li><b><code>header_bytes</code></b>: An optional int. Defaults to 0.</li>
<li><b><code>footer_bytes</code></b>: An optional int. Defaults to 0.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<hr />
<h4 id="tf.fixedlengthrecordreader.num_records_producednamenone"><code id="FixedLengthRecordReader.num_records_produced">tf.FixedLengthRecordReader.num_records_produced(name=None)</code></h4>
<p>Returns the number of records this reader has produced.</p>
<p>This is the same as the number of Read executions that have succeeded.</p>
<h5 id="args-44">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-38">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.num_work_units_completednamenone"><code id="FixedLengthRecordReader.num_work_units_completed">tf.FixedLengthRecordReader.num_work_units_completed(name=None)</code></h4>
<p>Returns the number of work units this reader has finished processing.</p>
<h5 id="args-45">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-39">Returns:</h5>
<p>An int64 Tensor.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.readqueue-namenone"><code id="FixedLengthRecordReader.read">tf.FixedLengthRecordReader.read(queue, name=None)</code></h4>
<p>Returns the next record (key, value pair) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</p>
<h5 id="args-46">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-40">Returns:</h5>
<p>A tuple of Tensors (key, value).</p>
<ul>
<li><b><code>key</code></b>: A string scalar Tensor.</li>
<li><b><code>value</code></b>: A string scalar Tensor.</li>
</ul>
<hr />
<h4 id="tf.fixedlengthrecordreader.read_up_toqueue-num_records-namenone"><code id="FixedLengthRecordReader.read_up_to">tf.FixedLengthRecordReader.read_up_to(queue, num_records, name=None)</code></h4>
<p>Returns up to num_records (key, value pairs) produced by a reader.</p>
<p>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</p>
<h5 id="args-47">Args:</h5>
<ul>
<li><b><code>queue</code></b>: A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.</li>
<li><b><code>num_records</code></b>: Number of records to read.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-41">Returns:</h5>
<p>A tuple of Tensors (keys, values).</p>
<ul>
<li><b><code>keys</code></b>: A 1-D string Tensor.</li>
<li><b><code>values</code></b>: A 1-D string Tensor.</li>
</ul>
<hr />
<h4 id="tf.fixedlengthrecordreader.reader_ref"><code id="FixedLengthRecordReader.reader_ref">tf.FixedLengthRecordReader.reader_ref</code></h4>
<p>Op that implements the reader.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.resetnamenone"><code id="FixedLengthRecordReader.reset">tf.FixedLengthRecordReader.reset(name=None)</code></h4>
<p>Restore a reader to its initial clean state.</p>
<h5 id="args-48">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-42">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.restore_statestate-namenone"><code id="FixedLengthRecordReader.restore_state">tf.FixedLengthRecordReader.restore_state(state, name=None)</code></h4>
<p>Restore a reader to a previously saved state.</p>
<p>Not all Readers support being restored, so this can produce an Unimplemented error.</p>
<h5 id="args-49">Args:</h5>
<ul>
<li><b><code>state</code></b>: A string Tensor. Result of a SerializeState of a Reader with matching type.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-43">Returns:</h5>
<p>The created Operation.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.serialize_statenamenone"><code id="FixedLengthRecordReader.serialize_state">tf.FixedLengthRecordReader.serialize_state(name=None)</code></h4>
<p>Produce a string tensor that encodes the state of a reader.</p>
<p>Not all Readers support being serialized, so this can produce an Unimplemented error.</p>
<h5 id="args-50">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-44">Returns:</h5>
<p>A string Tensor.</p>
<hr />
<h4 id="tf.fixedlengthrecordreader.supports_serialize"><code id="FixedLengthRecordReader.supports_serialize">tf.FixedLengthRecordReader.supports_serialize</code></h4>
<p>Whether the Reader implementation can serialize its state.</p>
<h2 id="converting">Converting</h2>
<p>TensorFlow provides several operations that you can use to convert various data formats into tensors.</p>
<hr />
<h3 id="tf.decode_csvrecords-record_defaults-field_delimnone-namenone"><a name="//apple_ref/cpp/Function/decode_csv" class="dashAnchor"></a><code id="decode_csv">tf.decode_csv(records, record_defaults, field_delim=None, name=None)</code></h3>
<p>Convert CSV records to tensors. Each column maps to one tensor.</p>
<p>RFC 4180 format is expected for the CSV records. (https://tools.ietf.org/html/rfc4180) Note that we allow leading and trailing spaces with int or float field.</p>
<h5 id="args-51">Args:</h5>
<ul>
<li><b><code>records</code></b>: A <code>Tensor</code> of type <code>string</code>. Each string is a record/row in the csv and all records should have the same format.</li>
<li><b><code>record_defaults</code></b>: A list of <code>Tensor</code> objects with types from: <code>float32</code>, <code>int32</code>, <code>int64</code>, <code>string</code>. One tensor per column of the input record, with either a scalar default value for that column or empty if the column is required.</li>
<li><b><code>field_delim</code></b>: An optional <code>string</code>. Defaults to <code>&quot;,&quot;</code>. delimiter to separate fields in a record.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-45">Returns:</h5>
<p>A list of <code>Tensor</code> objects. Has the same type as <code>record_defaults</code>. Each tensor will have the same shape as records.</p>
<hr />
<h3 id="tf.decode_rawbytes-out_type-little_endiannone-namenone"><a name="//apple_ref/cpp/Function/decode_raw" class="dashAnchor"></a><code id="decode_raw">tf.decode_raw(bytes, out_type, little_endian=None, name=None)</code></h3>
<p>Reinterpret the bytes of a string as a vector of numbers.</p>
<h5 id="args-52">Args:</h5>
<ul>
<li><b><code>bytes</code></b>: A <code>Tensor</code> of type <code>string</code>. All the elements must have the same length.</li>
<li><b><code>out_type</code></b>: A <code>tf.DType</code> from: <code>tf.half, tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.int64</code>.</li>
<li><b><code>little_endian</code></b>: An optional <code>bool</code>. Defaults to <code>True</code>. Whether the input <code>bytes</code> are in little-endian order. Ignored for <code>out_type</code> values that are stored in a single byte like <code>uint8</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-46">Returns:</h5>
<p>A <code>Tensor</code> of type <code>out_type</code>. A Tensor with one more dimension than the input <code>bytes</code>. The added dimension will have size equal to the length of the elements of <code>bytes</code> divided by the number of bytes to represent <code>out_type</code>.</p>
<hr />
<h3 id="example-protocol-buffer">Example protocol buffer</h3>
<p>TensorFlow's <a href="../../how_tos/reading_data/index.md#standard-tensorflow-format">recommended format for training examples</a> is serialized <code>Example</code> protocol buffers, <a href="https://www.tensorflow.org/code/tensorflow/core/example/example.proto">described here</a>. They contain <code>Features</code>, <a href="https://www.tensorflow.org/code/tensorflow/core/example/feature.proto">described here</a>.</p>
<hr />
<h3 id="class-tf.varlenfeature"><a name="//apple_ref/cpp/Class/VarLenFeature" class="dashAnchor"></a><code id="VarLenFeature">class tf.VarLenFeature</code></h3>
<p>Configuration for parsing a variable-length input feature.</p>
<p>Fields: dtype: Data type of input. - - -</p>
<h4 id="tf.varlenfeature.__getnewargs__"><code id="VarLenFeature.__getnewargs__">tf.VarLenFeature.__getnewargs__()</code></h4>
<p>Return self as a plain tuple. Used by copy and pickle.</p>
<hr />
<h4 id="tf.varlenfeature.__getstate__"><code id="VarLenFeature.__getstate__">tf.VarLenFeature.__getstate__()</code></h4>
<p>Exclude the OrderedDict from pickling</p>
<hr />
<h4 id="tf.varlenfeature.__new___cls-dtype"><code id="VarLenFeature.__new__">tf.VarLenFeature.__new__(_cls, dtype)</code></h4>
<p>Create new instance of VarLenFeature(dtype,)</p>
<hr />
<h4 id="tf.varlenfeature.__repr__"><code id="VarLenFeature.__repr__">tf.VarLenFeature.__repr__()</code></h4>
<p>Return a nicely formatted representation string</p>
<hr />
<h4 id="tf.varlenfeature.dtype"><code id="VarLenFeature.dtype">tf.VarLenFeature.dtype</code></h4>
<p>Alias for field number 0</p>
<hr />
<h3 id="class-tf.fixedlenfeature"><a name="//apple_ref/cpp/Class/FixedLenFeature" class="dashAnchor"></a><code id="FixedLenFeature">class tf.FixedLenFeature</code></h3>
<p>Configuration for parsing a fixed-length input feature.</p>
<p>To treat sparse input as dense, provide a <code>default_value</code>; otherwise, the parse functions will fail on any examples missing this feature.</p>
<p>Fields: shape: Shape of input data. dtype: Data type of input. default_value: Value to be used if an example is missing this feature. It must be compatible with <code>dtype</code>. - - -</p>
<h4 id="tf.fixedlenfeature.__getnewargs__"><code id="FixedLenFeature.__getnewargs__">tf.FixedLenFeature.__getnewargs__()</code></h4>
<p>Return self as a plain tuple. Used by copy and pickle.</p>
<hr />
<h4 id="tf.fixedlenfeature.__getstate__"><code id="FixedLenFeature.__getstate__">tf.FixedLenFeature.__getstate__()</code></h4>
<p>Exclude the OrderedDict from pickling</p>
<hr />
<h4 id="tf.fixedlenfeature.__new___cls-shape-dtype-default_valuenone"><code id="FixedLenFeature.__new__">tf.FixedLenFeature.__new__(_cls, shape, dtype, default_value=None)</code></h4>
<p>Create new instance of FixedLenFeature(shape, dtype, default_value)</p>
<hr />
<h4 id="tf.fixedlenfeature.__repr__"><code id="FixedLenFeature.__repr__">tf.FixedLenFeature.__repr__()</code></h4>
<p>Return a nicely formatted representation string</p>
<hr />
<h4 id="tf.fixedlenfeature.default_value"><code id="FixedLenFeature.default_value">tf.FixedLenFeature.default_value</code></h4>
<p>Alias for field number 2</p>
<hr />
<h4 id="tf.fixedlenfeature.dtype"><code id="FixedLenFeature.dtype">tf.FixedLenFeature.dtype</code></h4>
<p>Alias for field number 1</p>
<hr />
<h4 id="tf.fixedlenfeature.shape"><code id="FixedLenFeature.shape">tf.FixedLenFeature.shape</code></h4>
<p>Alias for field number 0</p>
<hr />
<h3 id="class-tf.fixedlensequencefeature"><a name="//apple_ref/cpp/Class/FixedLenSequenceFeature" class="dashAnchor"></a><code id="FixedLenSequenceFeature">class tf.FixedLenSequenceFeature</code></h3>
<p>Configuration for a dense input feature in a sequence item.</p>
<p>To treat a sparse input as dense, provide <code>allow_missing=True</code>; otherwise, the parse functions will fail on any examples missing this feature.</p>
<p>Fields: shape: Shape of input data. dtype: Data type of input. allow_missing: Whether to allow this feature to be missing from a feature list item. - - -</p>
<h4 id="tf.fixedlensequencefeature.__getnewargs__"><code id="FixedLenSequenceFeature.__getnewargs__">tf.FixedLenSequenceFeature.__getnewargs__()</code></h4>
<p>Return self as a plain tuple. Used by copy and pickle.</p>
<hr />
<h4 id="tf.fixedlensequencefeature.__getstate__"><code id="FixedLenSequenceFeature.__getstate__">tf.FixedLenSequenceFeature.__getstate__()</code></h4>
<p>Exclude the OrderedDict from pickling</p>
<hr />
<h4 id="tf.fixedlensequencefeature.__new___cls-shape-dtype-allow_missingfalse"><code id="FixedLenSequenceFeature.__new__">tf.FixedLenSequenceFeature.__new__(_cls, shape, dtype, allow_missing=False)</code></h4>
<p>Create new instance of FixedLenSequenceFeature(shape, dtype, allow_missing)</p>
<hr />
<h4 id="tf.fixedlensequencefeature.__repr__"><code id="FixedLenSequenceFeature.__repr__">tf.FixedLenSequenceFeature.__repr__()</code></h4>
<p>Return a nicely formatted representation string</p>
<hr />
<h4 id="tf.fixedlensequencefeature.allow_missing"><code id="FixedLenSequenceFeature.allow_missing">tf.FixedLenSequenceFeature.allow_missing</code></h4>
<p>Alias for field number 2</p>
<hr />
<h4 id="tf.fixedlensequencefeature.dtype"><code id="FixedLenSequenceFeature.dtype">tf.FixedLenSequenceFeature.dtype</code></h4>
<p>Alias for field number 1</p>
<hr />
<h4 id="tf.fixedlensequencefeature.shape"><code id="FixedLenSequenceFeature.shape">tf.FixedLenSequenceFeature.shape</code></h4>
<p>Alias for field number 0</p>
<hr />
<h3 id="class-tf.sparsefeature"><a name="//apple_ref/cpp/Class/SparseFeature" class="dashAnchor"></a><code id="SparseFeature">class tf.SparseFeature</code></h3>
<p>Configuration for parsing a sparse input feature.</p>
<p>Fields: index_key: Name of index feature. The underlying feature's type must be <code>int64</code> and its length must always match that of the <code>value_key</code> feature. value_key: Name of value feature. The underlying feature's type must be <code>dtype</code> and its length must always match that of the <code>index_key</code> feature. dtype: Data type of the <code>value_key</code> feature. size: A Python int to specify a dimension of the dense shape. Each value in the <code>index_key</code> feature must be in <code>[0, size)</code>. already_sorted: A Python boolean to specify whether the values in <code>index_key</code> are already sorted. If so skip sorting. False by default (optional). - - -</p>
<h4 id="tf.sparsefeature.__getnewargs__"><code id="SparseFeature.__getnewargs__">tf.SparseFeature.__getnewargs__()</code></h4>
<p>Return self as a plain tuple. Used by copy and pickle.</p>
<hr />
<h4 id="tf.sparsefeature.__getstate__"><code id="SparseFeature.__getstate__">tf.SparseFeature.__getstate__()</code></h4>
<p>Exclude the OrderedDict from pickling</p>
<hr />
<h4 id="tf.sparsefeature.__new___cls-index_key-value_key-dtype-size-already_sortedfalse"><code id="SparseFeature.__new__">tf.SparseFeature.__new__(_cls, index_key, value_key, dtype, size, already_sorted=False)</code></h4>
<p>Create new instance of SparseFeature(index_key, value_key, dtype, size, already_sorted)</p>
<hr />
<h4 id="tf.sparsefeature.__repr__"><code id="SparseFeature.__repr__">tf.SparseFeature.__repr__()</code></h4>
<p>Return a nicely formatted representation string</p>
<hr />
<h4 id="tf.sparsefeature.already_sorted"><code id="SparseFeature.already_sorted">tf.SparseFeature.already_sorted</code></h4>
<p>Alias for field number 4</p>
<hr />
<h4 id="tf.sparsefeature.dtype"><code id="SparseFeature.dtype">tf.SparseFeature.dtype</code></h4>
<p>Alias for field number 2</p>
<hr />
<h4 id="tf.sparsefeature.index_key"><code id="SparseFeature.index_key">tf.SparseFeature.index_key</code></h4>
<p>Alias for field number 0</p>
<hr />
<h4 id="tf.sparsefeature.size"><code id="SparseFeature.size">tf.SparseFeature.size</code></h4>
<p>Alias for field number 3</p>
<hr />
<h4 id="tf.sparsefeature.value_key"><code id="SparseFeature.value_key">tf.SparseFeature.value_key</code></h4>
<p>Alias for field number 1</p>
<hr />
<h3 id="tf.parse_exampleserialized-features-namenone-example_namesnone"><a name="//apple_ref/cpp/Function/parse_example" class="dashAnchor"></a><code id="parse_example">tf.parse_example(serialized, features, name=None, example_names=None)</code></h3>
<p>Parses <code>Example</code> protos into a <code>dict</code> of tensors.</p>
<p>Parses a number of serialized <a href="https://www.tensorflow.org/code/tensorflow/core/example/example.proto"><code>Example</code></a> protos given in <code>serialized</code>.</p>
<p><code>example_names</code> may contain descriptive names for the corresponding serialized protos. These may be useful for debugging purposes, but they have no effect on the output. If not <code>None</code>, <code>example_names</code> must be the same length as <code>serialized</code>.</p>
<p>This op parses serialized examples into a dictionary mapping keys to <code>Tensor</code> and <code>SparseTensor</code> objects. <code>features</code> is a dict from keys to <code>VarLenFeature</code>, <code>SparseFeature</code>, and <code>FixedLenFeature</code> objects. Each <code>VarLenFeature</code> and <code>SparseFeature</code> is mapped to a <code>SparseTensor</code>, and each <code>FixedLenFeature</code> is mapped to a <code>Tensor</code>.</p>
<p>Each <code>VarLenFeature</code> maps to a <code>SparseTensor</code> of the specified type representing a ragged matrix. Its indices are <code>[batch, index]</code> where <code>batch</code> is the batch entry the value is from in <code>serialized</code>, and <code>index</code> is the value's index in the list of values associated with that feature and example.</p>
<p>Each <code>SparseFeature</code> maps to a <code>SparseTensor</code> of the specified type representing a sparse matrix of shape <code>(serialized.size(), SparseFeature.size)</code>. Its indices are <code>[batch, index]</code> where <code>batch</code> is the batch entry the value is from in <code>serialized</code>, and <code>index</code> is the value's index is given by the values in the <code>SparseFeature.index_key</code> feature column.</p>
<p>Each <code>FixedLenFeature</code> <code>df</code> maps to a <code>Tensor</code> of the specified type (or <code>tf.float32</code> if not specified) and shape <code>(serialized.size(),) + df.shape</code>.</p>
<p><code>FixedLenFeature</code> entries with a <code>default_value</code> are optional. With no default value, we will fail if that <code>Feature</code> is missing from any example in <code>serialized</code>.</p>
<p>Examples:</p>
<p>For example, if one expects a <code>tf.float32</code> sparse feature <code>ft</code> and three serialized <code>Example</code>s are provided:</p>
<pre><code>serialized = [
  features
    { feature { key: &quot;ft&quot; value { float_list { value: [1.0, 2.0] } } } },
  features
    { feature []},
  features
    { feature { key: &quot;ft&quot; value { float_list { value: [3.0] } } }
]</code></pre>
<p>then the output will look like:</p>
<pre><code>{&quot;ft&quot;: SparseTensor(indices=[[0, 0], [0, 1], [2, 0]],
                    values=[1.0, 2.0, 3.0],
                    dense_shape=(3, 2)) }</code></pre>
<p>Given two <code>Example</code> input protos in <code>serialized</code>:</p>
<pre><code>[
  features {
    feature { key: &quot;kw&quot; value { bytes_list { value: [ &quot;knit&quot;, &quot;big&quot; ] } } }
    feature { key: &quot;gps&quot; value { float_list { value: [] } } }
  },
  features {
    feature { key: &quot;kw&quot; value { bytes_list { value: [ &quot;emmy&quot; ] } } }
    feature { key: &quot;dank&quot; value { int64_list { value: [ 42 ] } } }
    feature { key: &quot;gps&quot; value { } }
  }
]</code></pre>
<p>And arguments</p>
<pre><code>example_names: [&quot;input0&quot;, &quot;input1&quot;],
features: {
    &quot;kw&quot;: VarLenFeature(tf.string),
    &quot;dank&quot;: VarLenFeature(tf.int64),
    &quot;gps&quot;: VarLenFeature(tf.float32),
}</code></pre>
<p>Then the output is a dictionary:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">{
  <span class="st">&quot;kw&quot;</span>: SparseTensor(
      indices<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>]],
      values<span class="op">=</span>[<span class="st">&quot;knit&quot;</span>, <span class="st">&quot;big&quot;</span>, <span class="st">&quot;emmy&quot;</span>]
      dense_shape<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">2</span>]),
  <span class="co">&quot;dank&quot;</span>: SparseTensor(
      indices<span class="op">=</span>[[<span class="dv">1</span>, <span class="dv">0</span>]],
      values<span class="op">=</span>[<span class="dv">42</span>],
      dense_shape<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">1</span>]),
  <span class="co">&quot;gps&quot;</span>: SparseTensor(
      indices<span class="op">=</span>[],
      values<span class="op">=</span>[],
      dense_shape<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">0</span>]),
}</code></pre></div>
<p>For dense results in two serialized <code>Example</code>s:</p>
<pre><code>[
  features {
    feature { key: &quot;age&quot; value { int64_list { value: [ 0 ] } } }
    feature { key: &quot;gender&quot; value { bytes_list { value: [ &quot;f&quot; ] } } }
   },
   features {
    feature { key: &quot;age&quot; value { int64_list { value: [] } } }
    feature { key: &quot;gender&quot; value { bytes_list { value: [ &quot;f&quot; ] } } }
  }
]</code></pre>
<p>We can use arguments:</p>
<pre><code>example_names: [&quot;input0&quot;, &quot;input1&quot;],
features: {
    &quot;age&quot;: FixedLenFeature([], dtype=tf.int64, default_value=-1),
    &quot;gender&quot;: FixedLenFeature([], dtype=tf.string),
}</code></pre>
<p>And the expected output is:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">{
  <span class="st">&quot;age&quot;</span>: [[<span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>]],
  <span class="co">&quot;gender&quot;</span>: [[<span class="st">&quot;f&quot;</span>], [<span class="st">&quot;f&quot;</span>]],
}</code></pre></div>
<p>Given two <code>Example</code> input protos in <code>serialized</code>:</p>
<pre><code>[
  features {
    feature { key: &quot;val&quot; value { float_list { value: [ 0.5, -1.0 ] } } }
    feature { key: &quot;ix&quot; value { int64_list { value: [ 3, 20 ] } } }
  },
  features {
    feature { key: &quot;val&quot; value { float_list { value: [ 0.0 ] } } }
    feature { key: &quot;ix&quot; value { int64_list { value: [ 42 ] } } }
  }
]</code></pre>
<p>And arguments</p>
<pre><code>example_names: [&quot;input0&quot;, &quot;input1&quot;],
features: {
    &quot;sparse&quot;: SparseFeature(
        index_key=&quot;ix&quot;, value_key=&quot;val&quot;, dtype=tf.float32, size=100),
}</code></pre>
<p>Then the output is a dictionary:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">{
  <span class="st">&quot;sparse&quot;</span>: SparseTensor(
      indices<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">3</span>], [<span class="dv">0</span>, <span class="dv">20</span>], [<span class="dv">1</span>, <span class="dv">42</span>]],
      values<span class="op">=</span>[<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.0</span>]
      dense_shape<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">100</span>]),
}</code></pre></div>
<h5 id="args-53">Args:</h5>
<ul>
<li><b><code>serialized</code></b>: A vector (1-D Tensor) of strings, a batch of binary serialized <code>Example</code> protos.</li>
<li><b><code>features</code></b>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code>, <code>VarLenFeature</code>, and <code>SparseFeature</code> values.</li>
<li><b><code>name</code></b>: A name for this operation (optional).</li>
<li><b><code>example_names</code></b>: A vector (1-D Tensor) of strings (optional), the names of the serialized protos in the batch.</li>
</ul>
<h5 id="returns-47">Returns:</h5>
<p>A <code>dict</code> mapping feature keys to <code>Tensor</code> and <code>SparseTensor</code> values.</p>
<h5 id="raises">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if any feature is invalid.</li>
</ul>
<hr />
<h3 id="tf.parse_single_exampleserialized-features-namenone-example_namesnone"><a name="//apple_ref/cpp/Function/parse_single_example" class="dashAnchor"></a><code id="parse_single_example">tf.parse_single_example(serialized, features, name=None, example_names=None)</code></h3>
<p>Parses a single <code>Example</code> proto.</p>
<p>Similar to <code>parse_example</code>, except:</p>
<p>For dense tensors, the returned <code>Tensor</code> is identical to the output of <code>parse_example</code>, except there is no batch dimension, the output shape is the same as the shape given in <code>dense_shape</code>.</p>
<p>For <code>SparseTensor</code>s, the first (batch) column of the indices matrix is removed (the indices matrix is a column vector), the values vector is unchanged, and the first (<code>batch_size</code>) entry of the shape vector is removed (it is now a single element vector).</p>
<p>One might see performance advantages by batching <code>Example</code> protos with <code>parse_example</code> instead of using this function directly.</p>
<h5 id="args-54">Args:</h5>
<ul>
<li><b><code>serialized</code></b>: A scalar string Tensor, a single serialized Example. See <code>_parse_single_example_raw</code> documentation for more details.</li>
<li><b><code>features</code></b>: A <code>dict</code> mapping feature keys to <code>FixedLenFeature</code> or <code>VarLenFeature</code> values.</li>
<li><b><code>name</code></b>: A name for this operation (optional).</li>
<li><b><code>example_names</code></b>: (Optional) A scalar string Tensor, the associated name. See <code>_parse_single_example_raw</code> documentation for more details.</li>
</ul>
<h5 id="returns-48">Returns:</h5>
<p>A <code>dict</code> mapping feature keys to <code>Tensor</code> and <code>SparseTensor</code> values.</p>
<h5 id="raises-1">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if any feature is invalid.</li>
</ul>
<hr />
<h3 id="tf.parse_tensorserialized-out_type-namenone"><a name="//apple_ref/cpp/Function/parse_tensor" class="dashAnchor"></a><code id="parse_tensor">tf.parse_tensor(serialized, out_type, name=None)</code></h3>
<p>Transforms a serialized tensorflow.TensorProto proto into a Tensor.</p>
<h5 id="args-55">Args:</h5>
<ul>
<li><b><code>serialized</code></b>: A <code>Tensor</code> of type <code>string</code>. A scalar string containing a serialized TensorProto proto.</li>
<li><b><code>out_type</code></b>: A <code>tf.DType</code>. The type of the serialized tensor. The provided type must match the type of the serialized tensor and no implicit conversion will take place.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-49">Returns:</h5>
<p>A <code>Tensor</code> of type <code>out_type</code>. A Tensor of type <code>out_type</code>.</p>
<hr />
<h3 id="tf.decode_json_examplejson_examples-namenone"><a name="//apple_ref/cpp/Function/decode_json_example" class="dashAnchor"></a><code id="decode_json_example">tf.decode_json_example(json_examples, name=None)</code></h3>
<p>Convert JSON-encoded Example records to binary protocol buffer strings.</p>
<p>This op translates a tensor containing Example records, encoded using the <a href="https://developers.google.com/protocol-buffers/docs/proto3#json">standard JSON mapping</a>, into a tensor containing the same records encoded as binary protocol buffers. The resulting tensor can then be fed to any of the other Example-parsing ops.</p>
<h5 id="args-56">Args:</h5>
<ul>
<li><b><code>json_examples</code></b>: A <code>Tensor</code> of type <code>string</code>. Each string is a JSON object serialized according to the JSON mapping of the Example proto.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-50">Returns:</h5>
<p>A <code>Tensor</code> of type <code>string</code>. Each string is a binary Example protocol buffer corresponding to the respective element of <code>json_examples</code>.</p>
<h2 id="queues">Queues</h2>
<p>TensorFlow provides several implementations of 'Queues', which are structures within the TensorFlow computation graph to stage pipelines of tensors together. The following describe the basic Queue interface and some implementations. To see an example use, see <a href="../../how_tos/threading_and_queues/index.md">Threading and Queues</a>.</p>
<hr />
<h3 id="class-tf.queuebase"><a name="//apple_ref/cpp/Class/QueueBase" class="dashAnchor"></a><code id="QueueBase">class tf.QueueBase</code></h3>
<p>Base class for queue implementations.</p>
<p>A queue is a TensorFlow data structure that stores tensors across multiple steps, and exposes operations that enqueue and dequeue tensors.</p>
<p>Each queue element is a tuple of one or more tensors, where each tuple component has a static dtype, and may have a static shape. The queue implementations support versions of enqueue and dequeue that handle single elements, versions that support enqueuing and dequeuing a batch of elements at once.</p>
<p>See <a href="#FIFOQueue"><code>tf.FIFOQueue</code></a> and <a href="#RandomShuffleQueue"><code>tf.RandomShuffleQueue</code></a> for concrete implementations of this class, and instructions on how to create them.</p>
<hr />
<h4 id="tf.queuebase.enqueuevals-namenone"><code id="QueueBase.enqueue">tf.QueueBase.enqueue(vals, name=None)</code></h4>
<p>Enqueues one element to this queue.</p>
<p>If the queue is full when this operation executes, it will block until the element has been enqueued.</p>
<p>At runtime, this operation may raise an error if the queue is <a href="#QueueBase.close">closed</a> before or during its execution. If the queue is closed before this operation runs, <code>tf.errors.CancelledError</code> will be raised. If this operation is blocked, and either (i) the queue is closed by a close operation with <code>cancel_pending_enqueues=True</code>, or (ii) the session is <a href="../../api_docs/python/client.md#Session.close">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>
<h5 id="args-57">Args:</h5>
<ul>
<li><b><code>vals</code></b>: A tensor, a list or tuple of tensors, or a dictionary containing the values to enqueue.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-51">Returns:</h5>
<p>The operation that enqueues a new tuple of tensors to the queue.</p>
<hr />
<h4 id="tf.queuebase.enqueue_manyvals-namenone"><code id="QueueBase.enqueue_many">tf.QueueBase.enqueue_many(vals, name=None)</code></h4>
<p>Enqueues zero or more elements to this queue.</p>
<p>This operation slices each component tensor along the 0th dimension to make multiple queue elements. All of the tensors in <code>vals</code> must have the same size in the 0th dimension.</p>
<p>If the queue is full when this operation executes, it will block until all of the elements have been enqueued.</p>
<p>At runtime, this operation may raise an error if the queue is <a href="#QueueBase.close">closed</a> before or during its execution. If the queue is closed before this operation runs, <code>tf.errors.CancelledError</code> will be raised. If this operation is blocked, and either (i) the queue is closed by a close operation with <code>cancel_pending_enqueues=True</code>, or (ii) the session is <a href="../../api_docs/python/client.md#Session.close">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>
<h5 id="args-58">Args:</h5>
<ul>
<li><b><code>vals</code></b>: A tensor, a list or tuple of tensors, or a dictionary from which the queue elements are taken.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-52">Returns:</h5>
<p>The operation that enqueues a batch of tuples of tensors to the queue.</p>
<hr />
<h4 id="tf.queuebase.dequeuenamenone"><code id="QueueBase.dequeue">tf.QueueBase.dequeue(name=None)</code></h4>
<p>Dequeues one element from this queue.</p>
<p>If the queue is empty when this operation executes, it will block until there is an element to dequeue.</p>
<p>At runtime, this operation may raise an error if the queue is <a href="#QueueBase.close">closed</a> before or during its execution. If the queue is closed, the queue is empty, and there are no pending enqueue operations that can fulfill this request, <code>tf.errors.OutOfRangeError</code> will be raised. If the session is <a href="../../api_docs/python/client.md#Session.close">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>
<h5 id="args-59">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-53">Returns:</h5>
<p>The tuple of tensors that was dequeued.</p>
<hr />
<h4 id="tf.queuebase.dequeue_manyn-namenone"><code id="QueueBase.dequeue_many">tf.QueueBase.dequeue_many(n, name=None)</code></h4>
<p>Dequeues and concatenates <code>n</code> elements from this queue.</p>
<p>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. All of the components in the dequeued tuple will have size <code>n</code> in the 0th dimension.</p>
<p>If the queue is closed and there are less than <code>n</code> elements left, then an <code>OutOfRange</code> exception is raised.</p>
<p>At runtime, this operation may raise an error if the queue is <a href="#QueueBase.close">closed</a> before or during its execution. If the queue is closed, the queue contains fewer than <code>n</code> elements, and there are no pending enqueue operations that can fulfill this request, <code>tf.errors.OutOfRangeError</code> will be raised. If the session is <a href="../../api_docs/python/client.md#Session.close">closed</a>, <code>tf.errors.CancelledError</code> will be raised.</p>
<h5 id="args-60">Args:</h5>
<ul>
<li><b><code>n</code></b>: A scalar <code>Tensor</code> containing the number of elements to dequeue.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-54">Returns:</h5>
<p>The tuple of concatenated tensors that was dequeued.</p>
<hr />
<h4 id="tf.queuebase.sizenamenone"><code id="QueueBase.size">tf.QueueBase.size(name=None)</code></h4>
<p>Compute the number of elements in this queue.</p>
<h5 id="args-61">Args:</h5>
<ul>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-55">Returns:</h5>
<p>A scalar tensor containing the number of elements in this queue.</p>
<hr />
<h4 id="tf.queuebase.closecancel_pending_enqueuesfalse-namenone"><code id="QueueBase.close">tf.QueueBase.close(cancel_pending_enqueues=False, name=None)</code></h4>
<p>Closes this queue.</p>
<p>This operation signals that no more elements will be enqueued in the given queue. Subsequent <code>enqueue</code> and <code>enqueue_many</code> operations will fail. Subsequent <code>dequeue</code> and <code>dequeue_many</code> operations will continue to succeed if sufficient elements remain in the queue. Subsequent <code>dequeue</code> and <code>dequeue_many</code> operations that would block will fail immediately.</p>
<p>If <code>cancel_pending_enqueues</code> is <code>True</code>, all pending requests will also be cancelled.</p>
<h5 id="args-62">Args:</h5>
<ul>
<li><b><code>cancel_pending_enqueues</code></b>: (Optional.) A boolean, defaulting to <code>False</code> (described above).</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-56">Returns:</h5>
<p>The operation that closes the queue.</p>
<h4 id="other-methods">Other Methods</h4>
<hr />
<h4 id="tf.queuebase.__init__dtypes-shapes-names-queue_ref"><code id="QueueBase.__init__">tf.QueueBase.__init__(dtypes, shapes, names, queue_ref)</code></h4>
<p>Constructs a queue object from a queue reference.</p>
<p>The two optional lists, <code>shapes</code> and <code>names</code>, must be of the same length as <code>dtypes</code> if provided. The values at a given index <code>i</code> indicate the shape and name to use for the corresponding queue component in <code>dtypes</code>.</p>
<h5 id="args-63">Args:</h5>
<ul>
<li><b><code>dtypes</code></b>: A list of types. The length of dtypes must equal the number of tensors in each element.</li>
<li><b><code>shapes</code></b>: Constraints on the shapes of tensors in an element: A list of shape tuples or None. This list is the same length as dtypes. If the shape of any tensors in the element are constrained, all must be; shapes can be None if the shapes should not be constrained.</li>
<li><b><code>names</code></b>: Optional list of names. If provided, the <code>enqueue()</code> and <code>dequeue()</code> methods will use dictionaries with these names as keys. Must be None or a list or tuple of the same length as <code>dtypes</code>.</li>
<li><b><code>queue_ref</code></b>: The queue reference, i.e. the output of the queue op.</li>
</ul>
<h5 id="raises-2">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If one of the arguments is invalid.</li>
</ul>
<hr />
<h4 id="tf.queuebase.dequeue_up_ton-namenone"><code id="QueueBase.dequeue_up_to">tf.QueueBase.dequeue_up_to(n, name=None)</code></h4>
<p>Dequeues and concatenates <code>n</code> elements from this queue.</p>
<p><strong>Note</strong> This operation is not supported by all queues. If a queue does not support DequeueUpTo, then a <code>tf.errors.UnimplementedError</code> is raised.</p>
<p>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. If the queue has not been closed, all of the components in the dequeued tuple will have size <code>n</code> in the 0th dimension.</p>
<p>If the queue is closed and there are more than <code>0</code> but fewer than <code>n</code> elements remaining, then instead of raising a <code>tf.errors.OutOfRangeError</code> like <a href="#QueueBase.dequeue_many"><code>dequeue_many</code></a>, less than <code>n</code> elements are returned immediately. If the queue is closed and there are <code>0</code> elements left in the queue, then a <code>tf.errors.OutOfRangeError</code> is raised just like in <code>dequeue_many</code>. Otherwise the behavior is identical to <code>dequeue_many</code>.</p>
<h5 id="args-64">Args:</h5>
<ul>
<li><b><code>n</code></b>: A scalar <code>Tensor</code> containing the number of elements to dequeue.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-57">Returns:</h5>
<p>The tuple of concatenated tensors that was dequeued.</p>
<hr />
<h4 id="tf.queuebase.dtypes"><code id="QueueBase.dtypes">tf.QueueBase.dtypes</code></h4>
<p>The list of dtypes for each component of a queue element.</p>
<hr />
<h4 id="tf.queuebase.from_listindex-queues"><code id="QueueBase.from_list">tf.QueueBase.from_list(index, queues)</code></h4>
<p>Create a queue using the queue reference from <code>queues[index]</code>.</p>
<h5 id="args-65">Args:</h5>
<ul>
<li><b><code>index</code></b>: An integer scalar tensor that determines the input that gets selected.</li>
<li><b><code>queues</code></b>: A list of <code>QueueBase</code> objects.</li>
</ul>
<h5 id="returns-58">Returns:</h5>
<p>A <code>QueueBase</code> object.</p>
<h5 id="raises-3">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: When <code>queues</code> is not a list of <code>QueueBase</code> objects, or when the data types of <code>queues</code> are not all the same.</li>
</ul>
<hr />
<h4 id="tf.queuebase.name"><code id="QueueBase.name">tf.QueueBase.name</code></h4>
<p>The name of the underlying queue.</p>
<hr />
<h4 id="tf.queuebase.names"><code id="QueueBase.names">tf.QueueBase.names</code></h4>
<p>The list of names for each component of a queue element.</p>
<hr />
<h4 id="tf.queuebase.queue_ref"><code id="QueueBase.queue_ref">tf.QueueBase.queue_ref</code></h4>
<p>The underlying queue reference.</p>
<hr />
<h4 id="tf.queuebase.shapes"><code id="QueueBase.shapes">tf.QueueBase.shapes</code></h4>
<p>The list of shapes for each component of a queue element.</p>
<hr />
<h3 id="class-tf.fifoqueue"><a name="//apple_ref/cpp/Class/FIFOQueue" class="dashAnchor"></a><code id="FIFOQueue">class tf.FIFOQueue</code></h3>
<p>A queue implementation that dequeues elements in first-in first-out order.</p>
<p>See <a href="#QueueBase"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>
<hr />
<h4 id="tf.fifoqueue.__init__capacity-dtypes-shapesnone-namesnone-shared_namenone-namefifo_queue"><code id="FIFOQueue.__init__">tf.FIFOQueue.__init__(capacity, dtypes, shapes=None, names=None, shared_name=None, name='fifo_queue')</code></h4>
<p>Creates a queue that dequeues elements in a first-in first-out order.</p>
<p>A <code>FIFOQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p>
<p>A <code>FIFOQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are optionally described by the <code>shapes</code> argument.</p>
<p>If the <code>shapes</code> argument is specified, each component of a queue element must have the respective fixed shape. If it is unspecified, different queue elements may have different shapes, but the use of <code>dequeue_many</code> is disallowed.</p>
<h5 id="args-66">Args:</h5>
<ul>
<li><b><code>capacity</code></b>: An integer. The upper bound on the number of elements that may be stored in this queue.</li>
<li><b><code>dtypes</code></b>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li>
<li><b><code>shapes</code></b>: (Optional.) A list of fully-defined <code>TensorShape</code> objects with the same length as <code>dtypes</code>, or <code>None</code>.</li>
<li><b><code>names</code></b>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li>
<li><b><code>shared_name</code></b>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the queue operation.</li>
</ul>
<hr />
<h3 id="class-tf.paddingfifoqueue"><a name="//apple_ref/cpp/Class/PaddingFIFOQueue" class="dashAnchor"></a><code id="PaddingFIFOQueue">class tf.PaddingFIFOQueue</code></h3>
<p>A FIFOQueue that supports batching variable-sized tensors by padding.</p>
<p>A <code>PaddingFIFOQueue</code> may contain components with dynamic shape, while also supporting <code>dequeue_many</code>. See the constructor for more details.</p>
<p>See <a href="#QueueBase"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>
<hr />
<h4 id="tf.paddingfifoqueue.__init__capacity-dtypes-shapes-namesnone-shared_namenone-namepadding_fifo_queue"><code id="PaddingFIFOQueue.__init__">tf.PaddingFIFOQueue.__init__(capacity, dtypes, shapes, names=None, shared_name=None, name='padding_fifo_queue')</code></h4>
<p>Creates a queue that dequeues elements in a first-in first-out order.</p>
<p>A <code>PaddingFIFOQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p>
<p>A <code>PaddingFIFOQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are described by the <code>shapes</code> argument.</p>
<p>The <code>shapes</code> argument must be specified; each component of a queue element must have the respective shape. Shapes of fixed rank but variable size are allowed by setting any shape dimension to None. In this case, the inputs' shape may vary along the given dimension, and <code>dequeue_many</code> will pad the given dimension with zeros up to the maximum shape of all elements in the given batch.</p>
<h5 id="args-67">Args:</h5>
<ul>
<li><b><code>capacity</code></b>: An integer. The upper bound on the number of elements that may be stored in this queue.</li>
<li><b><code>dtypes</code></b>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li>
<li><b><code>shapes</code></b>: A list of <code>TensorShape</code> objects, with the same length as <code>dtypes</code>. Any dimension in the <code>TensorShape</code> containing value <code>None</code> is dynamic and allows values to be enqueued with variable size in that dimension.</li>
<li><b><code>names</code></b>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li>
<li><b><code>shared_name</code></b>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the queue operation.</li>
</ul>
<h5 id="raises-4">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If shapes is not a list of shapes, or the lengths of dtypes and shapes do not match, or if names is specified and the lengths of dtypes and names do not match.</li>
</ul>
<hr />
<h3 id="class-tf.randomshufflequeue"><a name="//apple_ref/cpp/Class/RandomShuffleQueue" class="dashAnchor"></a><code id="RandomShuffleQueue">class tf.RandomShuffleQueue</code></h3>
<p>A queue implementation that dequeues elements in a random order.</p>
<p>See <a href="#QueueBase"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>
<hr />
<h4 id="tf.randomshufflequeue.__init__capacity-min_after_dequeue-dtypes-shapesnone-namesnone-seednone-shared_namenone-namerandom_shuffle_queue"><code id="RandomShuffleQueue.__init__">tf.RandomShuffleQueue.__init__(capacity, min_after_dequeue, dtypes, shapes=None, names=None, seed=None, shared_name=None, name='random_shuffle_queue')</code></h4>
<p>Create a queue that dequeues elements in a random order.</p>
<p>A <code>RandomShuffleQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p>
<p>A <code>RandomShuffleQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>dtypes</code>, and whose shapes are optionally described by the <code>shapes</code> argument.</p>
<p>If the <code>shapes</code> argument is specified, each component of a queue element must have the respective fixed shape. If it is unspecified, different queue elements may have different shapes, but the use of <code>dequeue_many</code> is disallowed.</p>
<p>The <code>min_after_dequeue</code> argument allows the caller to specify a minimum number of elements that will remain in the queue after a <code>dequeue</code> or <code>dequeue_many</code> operation completes, to ensure a minimum level of mixing of elements. This invariant is maintained by blocking those operations until sufficient elements have been enqueued. The <code>min_after_dequeue</code> argument is ignored after the queue has been closed.</p>
<h5 id="args-68">Args:</h5>
<ul>
<li><b><code>capacity</code></b>: An integer. The upper bound on the number of elements that may be stored in this queue.</li>
<li><b><code>min_after_dequeue</code></b>: An integer (described above).</li>
<li><b><code>dtypes</code></b>: A list of <code>DType</code> objects. The length of <code>dtypes</code> must equal the number of tensors in each queue element.</li>
<li><b><code>shapes</code></b>: (Optional.) A list of fully-defined <code>TensorShape</code> objects with the same length as <code>dtypes</code>, or <code>None</code>.</li>
<li><b><code>names</code></b>: (Optional.) A list of string naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified the dequeue methods return a dictionary with the names as keys.</li>
<li><b><code>seed</code></b>: A Python integer. Used to create a random seed. See <a href="../../api_docs/python/constant_op.md#set_random_seed"><code>set_random_seed</code></a> for behavior.</li>
<li><b><code>shared_name</code></b>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the queue operation.</li>
</ul>
<hr />
<h3 id="class-tf.priorityqueue"><a name="//apple_ref/cpp/Class/PriorityQueue" class="dashAnchor"></a><code id="PriorityQueue">class tf.PriorityQueue</code></h3>
<p>A queue implementation that dequeues elements in prioritized order.</p>
<p>See <a href="#QueueBase"><code>tf.QueueBase</code></a> for a description of the methods on this class.</p>
<hr />
<h4 id="tf.priorityqueue.__init__capacity-types-shapesnone-namesnone-shared_namenone-namepriority_queue"><code id="PriorityQueue.__init__">tf.PriorityQueue.__init__(capacity, types, shapes=None, names=None, shared_name=None, name='priority_queue')</code></h4>
<p>Creates a queue that dequeues elements in a first-in first-out order.</p>
<p>A <code>PriorityQueue</code> has bounded capacity; supports multiple concurrent producers and consumers; and provides exactly-once delivery.</p>
<p>A <code>PriorityQueue</code> holds a list of up to <code>capacity</code> elements. Each element is a fixed-length tuple of tensors whose dtypes are described by <code>types</code>, and whose shapes are optionally described by the <code>shapes</code> argument.</p>
<p>If the <code>shapes</code> argument is specified, each component of a queue element must have the respective fixed shape. If it is unspecified, different queue elements may have different shapes, but the use of <code>dequeue_many</code> is disallowed.</p>
<p>Enqueues and Dequeues to the <code>PriorityQueue</code> must include an additional tuple entry at the beginning: the <code>priority</code>. The priority must be an int64 scalar (for <code>enqueue</code>) or an int64 vector (for <code>enqueue_many</code>).</p>
<h5 id="args-69">Args:</h5>
<ul>
<li><b><code>capacity</code></b>: An integer. The upper bound on the number of elements that may be stored in this queue.</li>
<li><b><code>types</code></b>: A list of <code>DType</code> objects. The length of <code>types</code> must equal the number of tensors in each queue element, except the first priority element. The first tensor in each element is the priority, which must be type int64.</li>
<li><b><code>shapes</code></b>: (Optional.) A list of fully-defined <code>TensorShape</code> objects, with the same length as <code>types</code>, or <code>None</code>.</li>
<li><b><code>names</code></b>: (Optional.) A list of strings naming the components in the queue with the same length as <code>dtypes</code>, or <code>None</code>. If specified, the dequeue methods return a dictionary with the names as keys.</li>
<li><b><code>shared_name</code></b>: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the queue operation.</li>
</ul>
<h2 id="conditional-accumulators">Conditional Accumulators</h2>
<hr />
<h3 id="class-tf.conditionalaccumulatorbase"><a name="//apple_ref/cpp/Class/ConditionalAccumulatorBase" class="dashAnchor"></a><code id="ConditionalAccumulatorBase">class tf.ConditionalAccumulatorBase</code></h3>
<p>A conditional accumulator for aggregating gradients.</p>
<p>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</p>
<p>Extraction of the average gradient is blocked until the required number of gradients has been accumulated. - - -</p>
<h4 id="tf.conditionalaccumulatorbase.__init__dtype-shape-accumulator_ref"><code id="ConditionalAccumulatorBase.__init__">tf.ConditionalAccumulatorBase.__init__(dtype, shape, accumulator_ref)</code></h4>
<p>Creates a new ConditionalAccumulator.</p>
<h5 id="args-70">Args:</h5>
<ul>
<li><b><code>dtype</code></b>: Datatype of the accumulated gradients.</li>
<li><b><code>shape</code></b>: Shape of the accumulated gradients.</li>
<li><b><code>accumulator_ref</code></b>: A handle to the conditional accumulator, created by sub- classes</li>
</ul>
<hr />
<h4 id="tf.conditionalaccumulatorbase.accumulator_ref"><code id="ConditionalAccumulatorBase.accumulator_ref">tf.ConditionalAccumulatorBase.accumulator_ref</code></h4>
<p>The underlying accumulator reference.</p>
<hr />
<h4 id="tf.conditionalaccumulatorbase.dtype"><code id="ConditionalAccumulatorBase.dtype">tf.ConditionalAccumulatorBase.dtype</code></h4>
<p>The datatype of the gradients accumulated by this accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulatorbase.name"><code id="ConditionalAccumulatorBase.name">tf.ConditionalAccumulatorBase.name</code></h4>
<p>The name of the underlying accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulatorbase.num_accumulatednamenone"><code id="ConditionalAccumulatorBase.num_accumulated">tf.ConditionalAccumulatorBase.num_accumulated(name=None)</code></h4>
<p>Number of gradients that have currently been aggregated in accumulator.</p>
<h5 id="args-71">Args:</h5>
<ul>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-59">Returns:</h5>
<p>Number of accumulated gradients currently in accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulatorbase.set_global_stepnew_global_step-namenone"><code id="ConditionalAccumulatorBase.set_global_step">tf.ConditionalAccumulatorBase.set_global_step(new_global_step, name=None)</code></h4>
<p>Sets the global time step of the accumulator.</p>
<p>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</p>
<h5 id="args-72">Args:</h5>
<ul>
<li><b><code>new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-60">Returns:</h5>
<p>Operation that sets the accumulator's time step.</p>
<hr />
<h3 id="class-tf.conditionalaccumulator"><a name="//apple_ref/cpp/Class/ConditionalAccumulator" class="dashAnchor"></a><code id="ConditionalAccumulator">class tf.ConditionalAccumulator</code></h3>
<p>A conditional accumulator for aggregating gradients.</p>
<p>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</p>
<p>Extraction of the average gradient is blocked until the required number of gradients has been accumulated. - - -</p>
<h4 id="tf.conditionalaccumulator.__init__dtype-shapenone-shared_namenone-nameconditional_accumulator"><code id="ConditionalAccumulator.__init__">tf.ConditionalAccumulator.__init__(dtype, shape=None, shared_name=None, name='conditional_accumulator')</code></h4>
<p>Creates a new ConditionalAccumulator.</p>
<h5 id="args-73">Args:</h5>
<ul>
<li><b><code>dtype</code></b>: Datatype of the accumulated gradients.</li>
<li><b><code>shape</code></b>: Shape of the accumulated gradients.</li>
<li><b><code>shared_name</code></b>: Optional. If non-empty, this accumulator will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the accumulator.</li>
</ul>
<hr />
<h4 id="tf.conditionalaccumulator.accumulator_ref"><code id="ConditionalAccumulator.accumulator_ref">tf.ConditionalAccumulator.accumulator_ref</code></h4>
<p>The underlying accumulator reference.</p>
<hr />
<h4 id="tf.conditionalaccumulator.apply_gradgrad-local_step0-namenone"><code id="ConditionalAccumulator.apply_grad">tf.ConditionalAccumulator.apply_grad(grad, local_step=0, name=None)</code></h4>
<p>Attempts to apply a gradient to the accumulator.</p>
<p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p>
<h5 id="args-74">Args:</h5>
<ul>
<li><b><code>grad</code></b>: The gradient tensor to be applied.</li>
<li><b><code>local_step</code></b>: Time step at which the gradient was computed.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-61">Returns:</h5>
<p>The operation that (conditionally) applies a gradient to the accumulator.</p>
<h5 id="raises-5">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If grad is of the wrong shape</li>
</ul>
<hr />
<h4 id="tf.conditionalaccumulator.dtype"><code id="ConditionalAccumulator.dtype">tf.ConditionalAccumulator.dtype</code></h4>
<p>The datatype of the gradients accumulated by this accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulator.name"><code id="ConditionalAccumulator.name">tf.ConditionalAccumulator.name</code></h4>
<p>The name of the underlying accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulator.num_accumulatednamenone"><code id="ConditionalAccumulator.num_accumulated">tf.ConditionalAccumulator.num_accumulated(name=None)</code></h4>
<p>Number of gradients that have currently been aggregated in accumulator.</p>
<h5 id="args-75">Args:</h5>
<ul>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-62">Returns:</h5>
<p>Number of accumulated gradients currently in accumulator.</p>
<hr />
<h4 id="tf.conditionalaccumulator.set_global_stepnew_global_step-namenone"><code id="ConditionalAccumulator.set_global_step">tf.ConditionalAccumulator.set_global_step(new_global_step, name=None)</code></h4>
<p>Sets the global time step of the accumulator.</p>
<p>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</p>
<h5 id="args-76">Args:</h5>
<ul>
<li><b><code>new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-63">Returns:</h5>
<p>Operation that sets the accumulator's time step.</p>
<hr />
<h4 id="tf.conditionalaccumulator.take_gradnum_required-namenone"><code id="ConditionalAccumulator.take_grad">tf.ConditionalAccumulator.take_grad(num_required, name=None)</code></h4>
<p>Attempts to extract the average gradient from the accumulator.</p>
<p>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</p>
<p>Once successful, the following actions are also triggered: - Counter of accumulated gradients is reset to 0. - Aggregated gradient is reset to 0 tensor. - Accumulator's internal time step is incremented by 1.</p>
<h5 id="args-77">Args:</h5>
<ul>
<li><b><code>num_required</code></b>: Number of gradients that needs to have been aggregated</li>
<li><b><code>name</code></b>: Optional name for the operation</li>
</ul>
<h5 id="returns-64">Returns:</h5>
<p>A tensor holding the value of the average gradient.</p>
<h5 id="raises-6">Raises:</h5>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If num_required &lt; 1</li>
</ul>
<hr />
<h3 id="class-tf.sparseconditionalaccumulator"><a name="//apple_ref/cpp/Class/SparseConditionalAccumulator" class="dashAnchor"></a><code id="SparseConditionalAccumulator">class tf.SparseConditionalAccumulator</code></h3>
<p>A conditional accumulator for aggregating sparse gradients.</p>
<p>Sparse gradients are represented by IndexedSlices.</p>
<p>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</p>
<p>Extraction of the average gradient is blocked until the required number of gradients has been accumulated.</p>
<p>Args: dtype: Datatype of the accumulated gradients. shape: Shape of the accumulated gradients. shared_name: Optional. If non-empty, this accumulator will be shared under the given name across multiple sessions. name: Optional name for the accumulator. - - -</p>
<h4 id="tf.sparseconditionalaccumulator.__init__dtype-shapenone-shared_namenone-namesparse_conditional_accumulator"><code id="SparseConditionalAccumulator.__init__">tf.SparseConditionalAccumulator.__init__(dtype, shape=None, shared_name=None, name='sparse_conditional_accumulator')</code></h4>
<hr />
<h4 id="tf.sparseconditionalaccumulator.accumulator_ref"><code id="SparseConditionalAccumulator.accumulator_ref">tf.SparseConditionalAccumulator.accumulator_ref</code></h4>
<p>The underlying accumulator reference.</p>
<hr />
<h4 id="tf.sparseconditionalaccumulator.apply_gradgrad_indices-grad_values-grad_shapenone-local_step0-namenone"><code id="SparseConditionalAccumulator.apply_grad">tf.SparseConditionalAccumulator.apply_grad(grad_indices, grad_values, grad_shape=None, local_step=0, name=None)</code></h4>
<p>Attempts to apply a sparse gradient to the accumulator.</p>
<p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p>
<p>A sparse gradient is represented by its indices, values and possibly empty or None shape. Indices must be a vector representing the locations of non-zero entries in the tensor. Values are the non-zero slices of the gradient, and must have the same first dimension as indices, i.e., the nnz represented by indices and values must be consistent. Shape, if not empty or None, must be consistent with the accumulator's shape (if also provided).</p>
<h5 id="example">Example:</h5>
<p>A tensor [[0, 0], [0. 1], [2, 3]] can be represented</p>
<ul>
<li><b><code>indices</code></b>: [1,2]</li>
<li><b><code>values</code></b>: [[0,1],[2,3]]</li>
<li><b><code>shape</code></b>: [3, 2]</li>
</ul>
<h5 id="args-78">Args:</h5>
<ul>
<li><b><code>grad_indices</code></b>: Indices of the sparse gradient to be applied.</li>
<li><b><code>grad_values</code></b>: Values of the sparse gradient to be applied.</li>
<li><b><code>grad_shape</code></b>: Shape of the sparse gradient to be applied.</li>
<li><b><code>local_step</code></b>: Time step at which the gradient was computed.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-65">Returns:</h5>
<p>The operation that (conditionally) applies a gradient to the accumulator.</p>
<h5 id="raises-7">Raises:</h5>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If grad is of the wrong shape</li>
</ul>
<hr />
<h4 id="tf.sparseconditionalaccumulator.apply_indexed_slices_gradgrad-local_step0-namenone"><code id="SparseConditionalAccumulator.apply_indexed_slices_grad">tf.SparseConditionalAccumulator.apply_indexed_slices_grad(grad, local_step=0, name=None)</code></h4>
<p>Attempts to apply a gradient to the accumulator.</p>
<p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p>
<h5 id="args-79">Args:</h5>
<ul>
<li><b><code>grad</code></b>: The gradient IndexedSlices to be applied.</li>
<li><b><code>local_step</code></b>: Time step at which the gradient was computed.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-66">Returns:</h5>
<p>The operation that (conditionally) applies a gradient to the accumulator.</p>
<h5 id="raises-8">Raises:</h5>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If grad is of the wrong shape</li>
</ul>
<hr />
<h4 id="tf.sparseconditionalaccumulator.dtype"><code id="SparseConditionalAccumulator.dtype">tf.SparseConditionalAccumulator.dtype</code></h4>
<p>The datatype of the gradients accumulated by this accumulator.</p>
<hr />
<h4 id="tf.sparseconditionalaccumulator.name"><code id="SparseConditionalAccumulator.name">tf.SparseConditionalAccumulator.name</code></h4>
<p>The name of the underlying accumulator.</p>
<hr />
<h4 id="tf.sparseconditionalaccumulator.num_accumulatednamenone"><code id="SparseConditionalAccumulator.num_accumulated">tf.SparseConditionalAccumulator.num_accumulated(name=None)</code></h4>
<p>Number of gradients that have currently been aggregated in accumulator.</p>
<h5 id="args-80">Args:</h5>
<ul>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-67">Returns:</h5>
<p>Number of accumulated gradients currently in accumulator.</p>
<hr />
<h4 id="tf.sparseconditionalaccumulator.set_global_stepnew_global_step-namenone"><code id="SparseConditionalAccumulator.set_global_step">tf.SparseConditionalAccumulator.set_global_step(new_global_step, name=None)</code></h4>
<p>Sets the global time step of the accumulator.</p>
<p>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</p>
<h5 id="args-81">Args:</h5>
<ul>
<li><b><code>new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h5 id="returns-68">Returns:</h5>
<p>Operation that sets the accumulator's time step.</p>
<hr />
<h4 id="tf.sparseconditionalaccumulator.take_gradnum_required-namenone"><code id="SparseConditionalAccumulator.take_grad">tf.SparseConditionalAccumulator.take_grad(num_required, name=None)</code></h4>
<p>Attempts to extract the average gradient from the accumulator.</p>
<p>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</p>
<p>Once successful, the following actions are also triggered: - Counter of accumulated gradients is reset to 0. - Aggregated gradient is reset to 0 tensor. - Accumulator's internal time step is incremented by 1.</p>
<h5 id="args-82">Args:</h5>
<ul>
<li><b><code>num_required</code></b>: Number of gradients that needs to have been aggregated</li>
<li><b><code>name</code></b>: Optional name for the operation</li>
</ul>
<h5 id="returns-69">Returns:</h5>
<p>A tuple of indices, values, and shape representing the average gradient.</p>
<h5 id="raises-9">Raises:</h5>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If num_required &lt; 1</li>
</ul>
<hr />
<h4 id="tf.sparseconditionalaccumulator.take_indexed_slices_gradnum_required-namenone"><code id="SparseConditionalAccumulator.take_indexed_slices_grad">tf.SparseConditionalAccumulator.take_indexed_slices_grad(num_required, name=None)</code></h4>
<p>Attempts to extract the average gradient from the accumulator.</p>
<p>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</p>
<p>Once successful, the following actions are also triggered: - Counter of accumulated gradients is reset to 0. - Aggregated gradient is reset to 0 tensor. - Accumulator's internal time step is incremented by 1.</p>
<h5 id="args-83">Args:</h5>
<ul>
<li><b><code>num_required</code></b>: Number of gradients that needs to have been aggregated</li>
<li><b><code>name</code></b>: Optional name for the operation</li>
</ul>
<h5 id="returns-70">Returns:</h5>
<p>An IndexedSlices holding the value of the average gradient.</p>
<h5 id="raises-10">Raises:</h5>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If num_required &lt; 1</li>
</ul>
<h2 id="dealing-with-the-filesystem">Dealing with the filesystem</h2>
<hr />
<h3 id="tf.matching_filespattern-namenone"><a name="//apple_ref/cpp/Function/matching_files" class="dashAnchor"></a><code id="matching_files">tf.matching_files(pattern, name=None)</code></h3>
<p>Returns the set of files matching a pattern.</p>
<p>Note that this routine only supports wildcard characters in the basename portion of the pattern, not in the directory portion.</p>
<h5 id="args-84">Args:</h5>
<ul>
<li><b><code>pattern</code></b>: A <code>Tensor</code> of type <code>string</code>. A (scalar) shell wildcard pattern.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-71">Returns:</h5>
<p>A <code>Tensor</code> of type <code>string</code>. A vector of matching filenames.</p>
<hr />
<h3 id="tf.read_filefilename-namenone"><a name="//apple_ref/cpp/Function/read_file" class="dashAnchor"></a><code id="read_file">tf.read_file(filename, name=None)</code></h3>
<p>Reads and outputs the entire contents of the input filename.</p>
<h5 id="args-85">Args:</h5>
<ul>
<li><b><code>filename</code></b>: A <code>Tensor</code> of type <code>string</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-72">Returns:</h5>
<p>A <code>Tensor</code> of type <code>string</code>.</p>
<hr />
<h3 id="tf.write_filefilename-contents-namenone"><a name="//apple_ref/cpp/Function/write_file" class="dashAnchor"></a><code id="write_file">tf.write_file(filename, contents, name=None)</code></h3>
<p>Writes contents to the file at input filename. Creates file if not existing.</p>
<h5 id="args-86">Args:</h5>
<ul>
<li><b><code>filename</code></b>: A <code>Tensor</code> of type <code>string</code>. scalar. The name of the file to which we write the contents.</li>
<li><b><code>contents</code></b>: A <code>Tensor</code> of type <code>string</code>. scalar. The content to be written to the output file.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h5 id="returns-73">Returns:</h5>
<p>The created Operation.</p>
<h2 id="input-pipeline">Input pipeline</h2>
<p>TensorFlow functions for setting up an input-prefetching pipeline. Please see the <a href="../../how_tos/reading_data/index.md">reading data how-to</a> for context.</p>
<h3 id="beginning-of-an-input-pipeline">Beginning of an input pipeline</h3>
<p>The &quot;producer&quot; functions add a queue to the graph and a corresponding <code>QueueRunner</code> for running the subgraph that fills that queue.</p>
<hr />
<h3 id="tf.train.match_filenames_oncepattern-namenone"><a name="//apple_ref/cpp/Function/match_filenames_once" class="dashAnchor"></a><code id="match_filenames_once">tf.train.match_filenames_once(pattern, name=None)</code></h3>
<p>Save the list of files matching pattern, so it is only computed once.</p>
<h5 id="args-87">Args:</h5>
<ul>
<li><b><code>pattern</code></b>: A file pattern (glob).</li>
<li><b><code>name</code></b>: A name for the operations (optional).</li>
</ul>
<h5 id="returns-74">Returns:</h5>
<p>A variable that is initialized to the list of files matching pattern.</p>
<hr />
<h3 id="tf.train.limit_epochstensor-num_epochsnone-namenone"><a name="//apple_ref/cpp/Function/limit_epochs" class="dashAnchor"></a><code id="limit_epochs">tf.train.limit_epochs(tensor, num_epochs=None, name=None)</code></h3>
<p>Returns tensor <code>num_epochs</code> times and then raises an <code>OutOfRange</code> error.</p>
<p>Note: creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-88">Args:</h5>
<ul>
<li><b><code>tensor</code></b>: Any <code>Tensor</code>.</li>
<li><b><code>num_epochs</code></b>: A positive integer (optional). If specified, limits the number of steps the output tensor may be evaluated.</li>
<li><b><code>name</code></b>: A name for the operations (optional).</li>
</ul>
<h5 id="returns-75">Returns:</h5>
<p>tensor or <code>OutOfRange</code>.</p>
<h5 id="raises-11">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if <code>num_epochs</code> is invalid.</li>
</ul>
<hr />
<h3 id="tf.train.input_producerinput_tensor-element_shapenone-num_epochsnone-shuffletrue-seednone-capacity32-shared_namenone-summary_namenone-namenone-cancel_opnone"><a name="//apple_ref/cpp/Function/input_producer" class="dashAnchor"></a><code id="input_producer">tf.train.input_producer(input_tensor, element_shape=None, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, summary_name=None, name=None, cancel_op=None)</code></h3>
<p>Output the rows of <code>input_tensor</code> to a queue for an input pipeline.</p>
<p>Note: if <code>num_epochs</code> is not <code>None</code>, this function creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-89">Args:</h5>
<ul>
<li><b><code>input_tensor</code></b>: A tensor with the rows to produce. Must be at least one-dimensional. Must either have a fully-defined shape, or <code>element_shape</code> must be defined.</li>
<li><b><code>element_shape</code></b>: (Optional.) A <code>TensorShape</code> representing the shape of a row of <code>input_tensor</code>, if it cannot be inferred.</li>
<li><b><code>num_epochs</code></b>: (Optional.) An integer. If specified <code>input_producer</code> produces each row of <code>input_tensor</code> <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>input_producer</code> can cycle through the rows of <code>input_tensor</code> an unlimited number of times.</li>
<li><b><code>shuffle</code></b>: (Optional.) A boolean. If true, the rows are randomly shuffled within each epoch.</li>
<li><b><code>seed</code></b>: (Optional.) An integer. The seed to use if <code>shuffle</code> is true.</li>
<li><b><code>capacity</code></b>: (Optional.) The capacity of the queue to be used for buffering the input.</li>
<li><b><code>shared_name</code></b>: (Optional.) If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>summary_name</code></b>: (Optional.) If set, a scalar summary for the current queue size will be generated, using this name as part of the tag.</li>
<li><b><code>name</code></b>: (Optional.) A name for queue.</li>
<li><b><code>cancel_op</code></b>: (Optional.) Cancel op for the queue</li>
</ul>
<h5 id="returns-76">Returns:</h5>
<p>A queue with the output rows. A <code>QueueRunner</code> for the queue is added to the current <code>QUEUE_RUNNER</code> collection of the current graph.</p>
<h5 id="raises-12">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the shape of the input cannot be inferred from the arguments.</li>
</ul>
<hr />
<h3 id="tf.train.range_input_producerlimit-num_epochsnone-shuffletrue-seednone-capacity32-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/range_input_producer" class="dashAnchor"></a><code id="range_input_producer">tf.train.range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></h3>
<p>Produces the integers from 0 to limit-1 in a queue.</p>
<p>Note: if <code>num_epochs</code> is not <code>None</code>, this function creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-90">Args:</h5>
<ul>
<li><b><code>limit</code></b>: An int32 scalar tensor.</li>
<li><b><code>num_epochs</code></b>: An integer (optional). If specified, <code>range_input_producer</code> produces each integer <code>num_epochs</code> times before generating an OutOfRange error. If not specified, <code>range_input_producer</code> can cycle through the integers an unlimited number of times.</li>
<li><b><code>shuffle</code></b>: Boolean. If true, the integers are randomly shuffled within each epoch.</li>
<li><b><code>seed</code></b>: An integer (optional). Seed used if shuffle == True.</li>
<li><b><code>capacity</code></b>: An integer. Sets the queue capacity.</li>
<li><b><code>shared_name</code></b>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: A name for the operations (optional).</li>
</ul>
<h5 id="returns-77">Returns:</h5>
<p>A Queue with the output integers. A <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>
<hr />
<h3 id="tf.train.slice_input_producertensor_list-num_epochsnone-shuffletrue-seednone-capacity32-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/slice_input_producer" class="dashAnchor"></a><code id="slice_input_producer">tf.train.slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)</code></h3>
<p>Produces a slice of each <code>Tensor</code> in <code>tensor_list</code>.</p>
<p>Implemented using a Queue -- a <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>
<h5 id="args-91">Args:</h5>
<ul>
<li><b><code>tensor_list</code></b>: A list of <code>Tensor</code> objects. Every <code>Tensor</code> in <code>tensor_list</code> must have the same size in the first dimension.</li>
<li><b><code>num_epochs</code></b>: An integer (optional). If specified, <code>slice_input_producer</code> produces each slice <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>slice_input_producer</code> can cycle through the slices an unlimited number of times.</li>
<li><b><code>shuffle</code></b>: Boolean. If true, the integers are randomly shuffled within each epoch.</li>
<li><b><code>seed</code></b>: An integer (optional). Seed used if shuffle == True.</li>
<li><b><code>capacity</code></b>: An integer. Sets the queue capacity.</li>
<li><b><code>shared_name</code></b>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: A name for the operations (optional).</li>
</ul>
<h5 id="returns-78">Returns:</h5>
<p>A list of tensors, one for each element of <code>tensor_list</code>. If the tensor in <code>tensor_list</code> has shape <code>[N, a, b, .., z]</code>, then the corresponding output tensor will have shape <code>[a, b, ..., z]</code>.</p>
<h5 id="raises-13">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if <code>slice_input_producer</code> produces nothing from <code>tensor_list</code>.</li>
</ul>
<hr />
<h3 id="tf.train.string_input_producerstring_tensor-num_epochsnone-shuffletrue-seednone-capacity32-shared_namenone-namenone-cancel_opnone"><a name="//apple_ref/cpp/Function/string_input_producer" class="dashAnchor"></a><code id="string_input_producer">tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None, cancel_op=None)</code></h3>
<p>Output strings (e.g. filenames) to a queue for an input pipeline.</p>
<p>Note: if <code>num_epochs</code> is not <code>None</code>, this function creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-92">Args:</h5>
<ul>
<li><b><code>string_tensor</code></b>: A 1-D string tensor with the strings to produce.</li>
<li><b><code>num_epochs</code></b>: An integer (optional). If specified, <code>string_input_producer</code> produces each string from <code>string_tensor</code> <code>num_epochs</code> times before generating an <code>OutOfRange</code> error. If not specified, <code>string_input_producer</code> can cycle through the strings in <code>string_tensor</code> an unlimited number of times.</li>
<li><b><code>shuffle</code></b>: Boolean. If true, the strings are randomly shuffled within each epoch.</li>
<li><b><code>seed</code></b>: An integer (optional). Seed used if shuffle == True.</li>
<li><b><code>capacity</code></b>: An integer. Sets the queue capacity.</li>
<li><b><code>shared_name</code></b>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: A name for the operations (optional).</li>
<li><b><code>cancel_op</code></b>: Cancel op for the queue (optional).</li>
</ul>
<h5 id="returns-79">Returns:</h5>
<p>A queue with the output strings. A <code>QueueRunner</code> for the Queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>
<h5 id="raises-14">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the string_tensor is a null Python list. At runtime, will fail with an assertion if string_tensor becomes a null tensor.</li>
</ul>
<h3 id="batching-at-the-end-of-an-input-pipeline">Batching at the end of an input pipeline</h3>
<p>These functions add a queue to the graph to assemble a batch of examples, with possible shuffling. They also add a <code>QueueRunner</code> for running the subgraph that fills that queue.</p>
<p>Use <a href="#batch"><code>batch</code></a> or <a href="#batch_join"><code>batch_join</code></a> for batching examples that have already been well shuffled. Use <a href="#shuffle_batch"><code>shuffle_batch</code></a> or <a href="#shuffle_batch_join"><code>shuffle_batch_join</code></a> for examples that would benefit from additional shuffling.</p>
<p>Use <a href="#batch"><code>batch</code></a> or <a href="#shuffle_batch"><code>shuffle_batch</code></a> if you want a single thread producing examples to batch, or if you have a single subgraph producing examples but you want to run it in <em>N</em> threads (where you increase <em>N</em> until it can keep the queue full). Use <a href="#batch_join"><code>batch_join</code></a> or <a href="#shuffle_batch_join"><code>shuffle_batch_join</code></a> if you have <em>N</em> different subgraphs producing examples to batch and you want them run by <em>N</em> threads. Use <code>maybe_*</code> to enqueue conditionally.</p>
<hr />
<h3 id="tf.train.batchtensors-batch_size-num_threads1-capacity32-enqueue_manyfalse-shapesnone-dynamic_padfalse-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/batch" class="dashAnchor"></a><code id="batch">tf.train.batch(tensors, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Creates batches of tensors in <code>tensors</code>.</p>
<p>The argument <code>tensors</code> can be a list or a dictionary of tensors. The value returned by the function will be of the same type as <code>tensors</code>.</p>
<p>This function is implemented using a queue. A <code>QueueRunner</code> for the queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>
<p>If <code>enqueue_many</code> is <code>False</code>, <code>tensors</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p>
<p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>. The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p>
<p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p>
<p><em>N.B.:</em> If <code>dynamic_pad</code> is <code>False</code>, you must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p>
<p>If <code>dynamic_pad</code> is <code>True</code>, it is sufficient that the <em>rank</em> of the tensors is known, but individual dimensions may have shape <code>None</code>. In this case, for each enqueue the dimensions with value <code>None</code> may have a variable length; upon dequeue, the output tensors will be padded on the right to the maximum shape of the tensors in the current minibatch. For numbers, this padding takes value 0. For strings, this padding is the empty string. See <code>PaddingFIFOQueue</code> for more info.</p>
<p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>
<p>Note: if <code>num_epochs</code> is not <code>None</code>, this function creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-93">Args:</h5>
<ul>
<li><b><code>tensors</code></b>: The list or dictionary of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: The new batch size pulled from the queue.</li>
<li><b><code>num_threads</code></b>: The number of threads enqueuing <code>tensors</code>.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensors</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors</code>.</li>
<li><b><code>dynamic_pad</code></b>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-80">Returns:</h5>
<p>A list or dictionary of tensors with the same types as <code>tensors</code> (except if the input is a list of one element, then it returns a tensor, not a list).</p>
<h5 id="raises-15">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li>
</ul>
<hr />
<h3 id="tf.train.maybe_batchtensors-keep_input-batch_size-num_threads1-capacity32-enqueue_manyfalse-shapesnone-dynamic_padfalse-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/maybe_batch" class="dashAnchor"></a><code id="maybe_batch">tf.train.maybe_batch(tensors, keep_input, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Conditionally creates batches of tensors based on <code>keep_input</code>.</p>
<p>See docstring in <code>batch</code> for more details.</p>
<h5 id="args-94">Args:</h5>
<ul>
<li><b><code>tensors</code></b>: The list or dictionary of tensors to enqueue.</li>
<li><b><code>keep_input</code></b>: A <code>bool</code> scalar Tensor. This tensor controls whether the input is added to the queue or not. If it evaluates <code>True</code>, then <code>tensors</code> are added to the queue; otherwise they are dropped. This tensor essentially acts as a filtering mechanism.</li>
<li><b><code>batch_size</code></b>: The new batch size pulled from the queue.</li>
<li><b><code>num_threads</code></b>: The number of threads enqueuing <code>tensors</code>.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensors</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors</code>.</li>
<li><b><code>dynamic_pad</code></b>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-81">Returns:</h5>
<p>A list or dictionary of tensors with the same types as <code>tensors</code>.</p>
<h5 id="raises-16">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li>
</ul>
<hr />
<h3 id="tf.train.batch_jointensors_list-batch_size-capacity32-enqueue_manyfalse-shapesnone-dynamic_padfalse-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/batch_join" class="dashAnchor"></a><code id="batch_join">tf.train.batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Runs a list of tensors to fill a queue to create batches of examples.</p>
<p>The <code>tensors_list</code> argument is a list of tuples of tensors, or a list of dictionaries of tensors. Each element in the list is treated similarly to the <code>tensors</code> argument of <code>tf.train.batch()</code>.</p>
<p>Enqueues a different list of tensors in different threads. Implemented using a queue -- a <code>QueueRunner</code> for the queue is added to the current <code>Graph</code>'s <code>QUEUE_RUNNER</code> collection.</p>
<p><code>len(tensors_list)</code> threads will be started, with thread <code>i</code> enqueuing the tensors from <code>tensors_list[i]</code>. <code>tensors_list[i1][j]</code> must match <code>tensors_list[i2][j]</code> in type and shape, except in the first dimension if <code>enqueue_many</code> is true.</p>
<p>If <code>enqueue_many</code> is <code>False</code>, each <code>tensors_list[i]</code> is assumed to represent a single example. An input tensor <code>x</code> will be output as a tensor with shape <code>[batch_size] + x.shape</code>.</p>
<p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors_list[i]</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors_list[i]</code> should have the same size in the first dimension. The slices of any input tensor <code>x</code> are treated as examples, and the output tensors will have shape <code>[batch_size] + x.shape[1:]</code>.</p>
<p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p>
<p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p>
<p><em>N.B.:</em> If <code>dynamic_pad</code> is <code>False</code>, you must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors_list</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p>
<p>If <code>dynamic_pad</code> is <code>True</code>, it is sufficient that the <em>rank</em> of the tensors is known, but individual dimensions may have value <code>None</code>. In this case, for each enqueue the dimensions with value <code>None</code> may have a variable length; upon dequeue, the output tensors will be padded on the right to the maximum shape of the tensors in the current minibatch. For numbers, this padding takes value 0. For strings, this padding is the empty string. See <code>PaddingFIFOQueue</code> for more info.</p>
<p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>
<h5 id="args-95">Args:</h5>
<ul>
<li><b><code>tensors_list</code></b>: A list of tuples or dictionaries of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: An integer. The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list_list[i]</code>.</li>
<li><b><code>dynamic_pad</code></b>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-82">Returns:</h5>
<p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>
<h5 id="raises-17">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensor_list_list</code>.</li>
</ul>
<hr />
<h3 id="tf.train.maybe_batch_jointensors_list-keep_input-batch_size-capacity32-enqueue_manyfalse-shapesnone-dynamic_padfalse-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/maybe_batch_join" class="dashAnchor"></a><code id="maybe_batch_join">tf.train.maybe_batch_join(tensors_list, keep_input, batch_size, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Runs a list of tensors to conditionally fill a queue to create batches.</p>
<p>See docstring in <code>batch_join</code> for more details.</p>
<h5 id="args-96">Args:</h5>
<ul>
<li><b><code>tensors_list</code></b>: A list of tuples or dictionaries of tensors to enqueue.</li>
<li><b><code>keep_input</code></b>: A <code>bool</code> scalar Tensor. This tensor controls whether the input is added to the queue or not. If it evaluates <code>True</code>, then <code>tensors</code> are added to the queue; otherwise they are dropped. This tensor essentially acts as a filtering mechanism.</li>
<li><b><code>batch_size</code></b>: An integer. The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list_list[i]</code>.</li>
<li><b><code>dynamic_pad</code></b>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-83">Returns:</h5>
<p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>
<h5 id="raises-18">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensor_list_list</code>.</li>
</ul>
<hr />
<h3 id="tf.train.shuffle_batchtensors-batch_size-capacity-min_after_dequeue-num_threads1-seednone-enqueue_manyfalse-shapesnone-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/shuffle_batch" class="dashAnchor"></a><code id="shuffle_batch">tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Creates batches by randomly shuffling tensors.</p>
<p>This function adds the following to the current <code>Graph</code>:</p>
<ul>
<li>A shuffling queue into which tensors from <code>tensors</code> are enqueued.</li>
<li>A <code>dequeue_many</code> operation to create batches from the queue.</li>
<li>A <code>QueueRunner</code> to <code>QUEUE_RUNNER</code> collection, to enqueue the tensors from <code>tensors</code>.</li>
</ul>
<p>If <code>enqueue_many</code> is <code>False</code>, <code>tensors</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p>
<p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>.</p>
<p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p>
<p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Creates batches of 32 images and 32 labels.</span>
image_batch, label_batch <span class="op">=</span> tf.train.shuffle_batch(
      [single_image, single_label],
      batch_size<span class="op">=</span><span class="dv">32</span>,
      num_threads<span class="op">=</span><span class="dv">4</span>,
      capacity<span class="op">=</span><span class="dv">50000</span>,
      min_after_dequeue<span class="op">=</span><span class="dv">10000</span>)</code></pre></div>
<p><em>N.B.:</em> You must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</p>
<p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>
<p>Note: if <code>num_epochs</code> is not <code>None</code>, this function creates local counter <code>epochs</code>. Use <code>local_variables_initializer()</code> to initialize local variables.</p>
<h5 id="args-97">Args:</h5>
<ul>
<li><b><code>tensors</code></b>: The list or dictionary of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>min_after_dequeue</code></b>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li>
<li><b><code>num_threads</code></b>: The number of threads enqueuing <code>tensor_list</code>.</li>
<li><b><code>seed</code></b>: Seed for the random shuffling within the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list</code>.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-84">Returns:</h5>
<p>A list or dictionary of tensors with the types as <code>tensors</code>.</p>
<h5 id="raises-19">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li>
</ul>
<hr />
<h3 id="tf.train.maybe_shuffle_batchtensors-batch_size-capacity-min_after_dequeue-keep_input-num_threads1-seednone-enqueue_manyfalse-shapesnone-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/maybe_shuffle_batch" class="dashAnchor"></a><code id="maybe_shuffle_batch">tf.train.maybe_shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, keep_input, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Creates batches by randomly shuffling conditionally-enqueued tensors.</p>
<p>See docstring in <code>shuffle_batch</code> for more details.</p>
<h5 id="args-98">Args:</h5>
<ul>
<li><b><code>tensors</code></b>: The list or dictionary of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>min_after_dequeue</code></b>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li>
<li><b><code>keep_input</code></b>: A <code>bool</code> scalar Tensor. This tensor controls whether the input is added to the queue or not. If it evaluates <code>True</code>, then <code>tensors</code> are added to the queue; otherwise they are dropped. This tensor essentially acts as a filtering mechanism.</li>
<li><b><code>num_threads</code></b>: The number of threads enqueuing <code>tensor_list</code>.</li>
<li><b><code>seed</code></b>: Seed for the random shuffling within the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list</code>.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (Optional) If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-85">Returns:</h5>
<p>A list or dictionary of tensors with the types as <code>tensors</code>.</p>
<h5 id="raises-20">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li>
</ul>
<hr />
<h3 id="tf.train.shuffle_batch_jointensors_list-batch_size-capacity-min_after_dequeue-seednone-enqueue_manyfalse-shapesnone-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/shuffle_batch_join" class="dashAnchor"></a><code id="shuffle_batch_join">tf.train.shuffle_batch_join(tensors_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Create batches by randomly shuffling tensors.</p>
<p>The <code>tensors_list</code> argument is a list of tuples of tensors, or a list of dictionaries of tensors. Each element in the list is treated similarly to the <code>tensors</code> argument of <code>tf.train.shuffle_batch()</code>.</p>
<p>This version enqueues a different list of tensors in different threads. It adds the following to the current <code>Graph</code>:</p>
<ul>
<li>A shuffling queue into which tensors from <code>tensors_list</code> are enqueued.</li>
<li>A <code>dequeue_many</code> operation to create batches from the queue.</li>
<li>A <code>QueueRunner</code> to <code>QUEUE_RUNNER</code> collection, to enqueue the tensors from <code>tensors_list</code>.</li>
</ul>
<p><code>len(tensors_list)</code> threads will be started, with thread <code>i</code> enqueuing the tensors from <code>tensors_list[i]</code>. <code>tensors_list[i1][j]</code> must match <code>tensors_list[i2][j]</code> in type and shape, except in the first dimension if <code>enqueue_many</code> is true.</p>
<p>If <code>enqueue_many</code> is <code>False</code>, each <code>tensors_list[i]</code> is assumed to represent a single example. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p>
<p>If <code>enqueue_many</code> is <code>True</code>, <code>tensors_list[i]</code> is assumed to represent a batch of examples, where the first dimension is indexed by example, and all members of <code>tensors_list[i]</code> should have the same size in the first dimension. If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>.</p>
<p>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p>
<p>The returned operation is a dequeue operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</p>
<p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded. In addition, all output tensors' static shapes, as accessed via the <code>get_shape</code> method will have a first <code>Dimension</code> value of <code>None</code>, and operations that depend on fixed batch_size would fail.</p>
<h5 id="args-99">Args:</h5>
<ul>
<li><b><code>tensors_list</code></b>: A list of tuples or dictionaries of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: An integer. The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>min_after_dequeue</code></b>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li>
<li><b><code>seed</code></b>: Seed for the random shuffling within the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors_list[i]</code>.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-86">Returns:</h5>
<p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>
<h5 id="raises-21">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors_list</code>.</li>
</ul>
<hr />
<h3 id="tf.train.maybe_shuffle_batch_jointensors_list-batch_size-capacity-min_after_dequeue-keep_input-seednone-enqueue_manyfalse-shapesnone-allow_smaller_final_batchfalse-shared_namenone-namenone"><a name="//apple_ref/cpp/Function/maybe_shuffle_batch_join" class="dashAnchor"></a><code id="maybe_shuffle_batch_join">tf.train.maybe_shuffle_batch_join(tensors_list, batch_size, capacity, min_after_dequeue, keep_input, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)</code></h3>
<p>Create batches by randomly shuffling conditionally-enqueued tensors.</p>
<p>See docstring in <code>shuffle_batch_join</code> for more details.</p>
<h5 id="args-100">Args:</h5>
<ul>
<li><b><code>tensors_list</code></b>: A list of tuples or dictionaries of tensors to enqueue.</li>
<li><b><code>batch_size</code></b>: An integer. The new batch size pulled from the queue.</li>
<li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li>
<li><b><code>min_after_dequeue</code></b>: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.</li>
<li><b><code>keep_input</code></b>: A <code>bool</code> scalar Tensor. If provided, this tensor controls whether the input is added to the queue or not. If it evaluates <code>True</code>, then <code>tensors_list</code> are added to the queue; otherwise they are dropped. This tensor essentially acts as a filtering mechanism.</li>
<li><b><code>seed</code></b>: Seed for the random shuffling within the queue.</li>
<li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensor_list_list</code> is a single example.</li>
<li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors_list[i]</code>.</li>
<li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li>
<li><b><code>shared_name</code></b>: (optional). If set, this queue will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: (Optional) A name for the operations.</li>
</ul>
<h5 id="returns-87">Returns:</h5>
<p>A list or dictionary of tensors with the same number and types as <code>tensors_list[i]</code>.</p>
<h5 id="raises-22">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors_list</code>.</li>
</ul>
