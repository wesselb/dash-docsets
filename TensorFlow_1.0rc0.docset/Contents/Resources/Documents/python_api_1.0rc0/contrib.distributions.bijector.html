<!-- This file is machine generated: DO NOT EDIT! -->
<h1 id="random-variable-transformations-contrib">Random variable transformations (contrib)</h1>
<p>[TOC]</p>
<p>Bijector Ops.</p>
<p>An API for invertible, differentiable transformations of random variables.</p>
<h2 id="background">Background</h2>
<p>Differentiable, bijective transformations of continuous random variables alter the calculations made in the cumulative/probability distribution functions and sample function. This module provides a standard interface for making these manipulations.</p>
<p>For more details and examples, see the <code>Bijector</code> docstring.</p>
<p>To apply a <code>Bijector</code>, use <code>distributions.TransformedDistribution</code>.</p>
<h2 id="bijectors">Bijectors</h2>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.affine"><a name="//apple_ref/cpp/Class/Affine" class="dashAnchor"></a><code id="Affine">class tf.contrib.distributions.bijector.Affine</code></h3>
<p>Bijector which computes <code>Y = g(X; shift, scale) = matmul(scale, X) + shift</code> where <code>scale = c * I + diag(D1) + tril(L) + V @ diag(D2) @ V.T</code>.</p>
<p>Write <code>A @ X</code> for <code>matmul(A, X)</code>. In TF parlance, the <code>scale</code> term is logically equivalent to:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">scale <span class="op">=</span> (
  scale_identity_multiplier <span class="op">*</span> tf.diag(tf.ones(d)) <span class="op">+</span>
  tf.diag(scale_diag) <span class="op">+</span>
  scale_tril <span class="op">+</span>
  scale_perturb_factor @ diag(scale_perturb_diag) @
    tf.transpose([scale_perturb_factor])
)</code></pre></div>
<p>The <code>scale</code> term is applied without necessarily materializing constituent matrices, i.e., the matmul is <a href="https://en.wikipedia.org/wiki/Matrix-free_methods">matrix-free</a> when possible.</p>
<p>Examples:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Y = X</span>
b <span class="op">=</span> Affine()

<span class="co"># Y = X + shift</span>
b <span class="op">=</span> Affine(shift<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>])

<span class="co"># Y = 2 * I @ X.T + shift</span>
b <span class="op">=</span> Affine(shift<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>],
           scale_identity_multiplier<span class="op">=</span><span class="dv">2</span>.)

<span class="co"># Y = tf.diag(d1) @ X.T + shift</span>
b <span class="op">=</span> Affine(shift<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>],
           scale_diag<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">1</span>])         <span class="co"># Implicitly 3x3.</span>

<span class="co"># Y = (I + v * v.T) @ X.T + shift</span>
b <span class="op">=</span> Affine(shift<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>],
           scale_perturb_factor<span class="op">=</span>[[<span class="dv">1</span>., <span class="dv">0</span>],
                                 [<span class="dv">0</span>, <span class="dv">1</span>],
                                 [<span class="dv">1</span>, <span class="dv">1</span>]])

<span class="co"># Y = (diag(d1) + v * diag(d2) * v.T) @ X.T + shift</span>
b <span class="op">=</span> Affine(shift<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>],
           scale_diag<span class="op">=</span>[<span class="dv">1</span>., <span class="dv">3</span>, <span class="dv">3</span>],          <span class="co"># Implicitly 3x3.</span>
           scale_perturb_diag<span class="op">=</span>[<span class="dv">2</span>., <span class="dv">1</span>],     <span class="co"># Implicitly 2x2.</span>
           scale_perturb_factor<span class="op">=</span>[[<span class="dv">1</span>., <span class="dv">0</span>],
                                 [<span class="dv">0</span>, <span class="dv">1</span>],
                                 [<span class="dv">1</span>, <span class="dv">1</span>]])</code></pre></div>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.__init__shiftnone-scale_identity_multipliernone-scale_diagnone-scale_trilnone-scale_perturb_factornone-scale_perturb_diagnone-event_ndims1-validate_argsfalse-nameaffine"><code id="Affine.__init__">tf.contrib.distributions.bijector.Affine.__init__(shift=None, scale_identity_multiplier=None, scale_diag=None, scale_tril=None, scale_perturb_factor=None, scale_perturb_diag=None, event_ndims=1, validate_args=False, name='affine')</code></h4>
<p>Instantiates the <code>Affine</code> bijector.</p>
<p>This <code>Bijector</code> is initialized with <code>shift</code> <code>Tensor</code> and <code>scale</code> arguments, giving the forward operation:</p>
<pre class="none"><code>Y = g(X) = scale @ X + shift</code></pre>
<p>where the <code>scale</code> term is logically equivalent to:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">scale <span class="op">=</span> (
  scale_identity_multiplier <span class="op">*</span> tf.diag(tf.ones(d)) <span class="op">+</span>
  tf.diag(scale_diag) <span class="op">+</span>
  scale_tril <span class="op">+</span>
  scale_perturb_factor @ diag(scale_perturb_diag) @
    tf.transpose([scale_perturb_factor])
)</code></pre></div>
<p>If none of <code>scale_identity_multiplier</code>, <code>scale_diag</code>, or <code>scale_tril</code> are specified then <code>scale += IdentityMatrix</code>. Otherwise specifying a <code>scale</code> argument has the semantics of <code>scale += Expand(arg)</code>, i.e., <code>scale_diag != None</code> means <code>scale += tf.diag(scale_diag)</code>.</p>
<h5 id="args">Args:</h5>
<ul>
<li><b><code>shift</code></b>: Numeric <code>Tensor</code>. If this is set to <code>None</code>, no shift is applied.</li>
<li><b><code>scale_identity_multiplier</code></b>: floating point rank 0 <code>Tensor</code> representing a scaling done to the identity matrix. When <code>scale_identity_multiplier = scale_diag=scale_tril = None</code> then <code>scale += IdentityMatrix</code>. Otherwise no scaled-identity-matrix is added to <code>scale</code>.</li>
<li><b><code>scale_diag</code></b>: Numeric <code>Tensor</code> representing the diagonal matrix. <code>scale_diag</code> has shape [N1, N2, ... k], which represents a k x k diagonal matrix. When <code>None</code> no diagonal term is added to <code>scale</code>.</li>
<li><b><code>scale_tril</code></b>: Numeric <code>Tensor</code> representing the diagonal matrix. <code>scale_diag</code> has shape [N1, N2, ... k, k], which represents a k x k lower triangular matrix. When <code>None</code> no <code>scale_tril</code> term is added to <code>scale</code>. The upper triangular elements above the diagonal are ignored.</li>
<li><b><code>scale_perturb_factor</code></b>: Numeric <code>Tensor</code> representing factor matrix with last two dimensions of shape <code>(k, r)</code>. When <code>None</code>, no rank-r update is added to <code>scale</code>.</li>
<li><b><code>scale_perturb_diag</code></b>: Numeric <code>Tensor</code> representing the diagonal matrix. <code>scale_perturb_diag</code> has shape [N1, N2, ... r], which represents an r x r Diagonal matrix. When <code>None</code> low rank updates will take the form <code>scale_perturb_factor * scale_perturb_factor.T</code>.</li>
<li><b><code>event_ndims</code></b>: Scalar <code>int32</code> <code>Tensor</code> indicating the number of dimensions associated with a particular draw from the distribution. Must be 0 or 1.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code> name given to ops managed by this object.</li>
</ul>
<h5 id="raises">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if <code>perturb_diag</code> is specified but not <code>perturb_factor</code>.</li>
<li><b><code>TypeError</code></b>: if <code>shift</code> has different <code>dtype</code> from <code>scale</code> arguments.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.dtype"><code id="Affine.dtype">tf.contrib.distributions.bijector.Affine.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.forwardx-nameforward-condition_kwargs"><code id="Affine.forward">tf.contrib.distributions.bijector.Affine.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-1">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-1">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Affine.forward_event_shape">tf.contrib.distributions.bijector.Affine.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-2">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-1">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Affine.forward_log_det_jacobian">tf.contrib.distributions.bijector.Affine.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-3">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-2">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-2">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.get_forward_event_shapeinput_shape"><code id="Affine.get_forward_event_shape">tf.contrib.distributions.bijector.Affine.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-4">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-3">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.get_inverse_event_shapeoutput_shape"><code id="Affine.get_inverse_event_shape">tf.contrib.distributions.bijector.Affine.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-5">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-4">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.graph_parents"><code id="Affine.graph_parents">tf.contrib.distributions.bijector.Affine.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.inversey-nameinverse-condition_kwargs"><code id="Affine.inverse">tf.contrib.distributions.bijector.Affine.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-6">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-5">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-3">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Affine.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Affine.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-7">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-6">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-4">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Affine.inverse_event_shape">tf.contrib.distributions.bijector.Affine.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-8">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-7">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Affine.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Affine.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-9">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-8">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-5">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.is_constant_jacobian"><code id="Affine.is_constant_jacobian">tf.contrib.distributions.bijector.Affine.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-9">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.name"><code id="Affine.name">tf.contrib.distributions.bijector.Affine.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.scale"><code id="Affine.scale">tf.contrib.distributions.bijector.Affine.scale</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.shaper"><code id="Affine.shaper">tf.contrib.distributions.bijector.Affine.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.shift"><code id="Affine.shift">tf.contrib.distributions.bijector.Affine.shift</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.affine.validate_args"><code id="Affine.validate_args">tf.contrib.distributions.bijector.Affine.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.affinelinearoperator"><a name="//apple_ref/cpp/Class/AffineLinearOperator" class="dashAnchor"></a><code id="AffineLinearOperator">class tf.contrib.distributions.bijector.AffineLinearOperator</code></h3>
<p>Bijector which computes <code>Y = g(X; shift, scale) = scale @ X.T + shift</code>.</p>
<p><code>shift</code> is a numeric <code>Tensor</code> and <code>scale</code> is a <code>LinearOperator</code>.</p>
<p>If <code>X</code> is a scalar then the forward transformation is: <code>scale * X + shift</code> where <code>*</code> denotes the scalar product.</p>
<p>Note: we don't always simply transpose <code>X</code> (but write it this way for brevity). Actually the input <code>X</code> undergoes the following transformation before being premultiplied by <code>scale</code>:</p>
<ol style="list-style-type: decimal">
<li>If there are no sample dims, we call <code>X = tf.expand_dims(X, 0)</code>, i.e., <code>new_sample_shape = [1]</code>. Otherwise do nothing.</li>
<li>The sample shape is flattened to have one dimension, i.e., <code>new_sample_shape = [n]</code> where <code>n = tf.reduce_prod(old_sample_shape)</code>.</li>
<li>The sample dim is cyclically rotated left by 1, i.e., <code>new_shape = [B1,...,Bb, k, n]</code> where <code>n</code> is as above, <code>k</code> is the event_shape, and <code>B1,...,Bb</code> are the batch shapes for each of <code>b</code> batch dimensions.</li>
</ol>
<p>(For more details see <code>shape.make_batch_of_event_sample_matrices</code>.)</p>
<p>The result of the above transformation is that <code>X</code> can be regarded as a batch of matrices where each column is a draw from the distribution. After premultiplying by <code>scale</code>, we take the inverse of this procedure. The input <code>Y</code> also undergoes the same transformation before/after premultiplying by <code>inv(scale)</code>.</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">linalg <span class="op">=</span> tf.contrib.linalg

x <span class="op">=</span> [<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>]

shift <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>., <span class="dv">0</span>., <span class="dv">1</span>]
diag <span class="op">=</span> [<span class="dv">1</span>., <span class="dv">2</span>, <span class="dv">3</span>]
scale <span class="op">=</span> linalg.LinearOperatorDiag(diag)
affine <span class="op">=</span> AffineLinearOperator(shift, scale)
<span class="co"># In this case, `forward` is equivalent to:</span>
<span class="co"># diag * scale + shift</span>
y <span class="op">=</span> affine.forward(x)  <span class="co"># [0., 4, 10]</span>

shift <span class="op">=</span> [<span class="dv">2</span>., <span class="dv">3</span>, <span class="dv">1</span>]
tril <span class="op">=</span> [[<span class="dv">1</span>., <span class="dv">0</span>, <span class="dv">0</span>],
        [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>],
        [<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>]]
scale <span class="op">=</span> linalg.LinearOperatorTriL(tril)
affine <span class="op">=</span> AffineLinearOperator(shift, scale)
<span class="co"># In this case, `forward` is equivalent to:</span>
<span class="co"># np.squeeze(np.matmul(tril, np.expand_dims(x, -1)), -1) + shift</span>
y <span class="op">=</span> affine.forward(x)  <span class="co"># [3., 7, 11]</span></code></pre></div>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.__init__shiftnone-scalenone-event_ndims1-validate_argsfalse-nameaffine_linear_operator"><code id="AffineLinearOperator.__init__">tf.contrib.distributions.bijector.AffineLinearOperator.__init__(shift=None, scale=None, event_ndims=1, validate_args=False, name='affine_linear_operator')</code></h4>
<p>Instantiates the <code>AffineLinearOperator</code> bijector.</p>
<h5 id="args-10">Args:</h5>
<ul>
<li><b><code>shift</code></b>: Numeric <code>Tensor</code>.</li>
<li><b><code>scale</code></b>: Subclass of <code>LinearOperator</code>. Represents the (batch) positive definite matrix <code>M</code> in <code>R^{k x k}</code>.</li>
<li><b><code>event_ndims</code></b>: Scalar <code>integer</code> <code>Tensor</code> indicating the number of dimensions associated with a particular draw from the distribution. Must be 0 or 1.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code> name given to ops managed by this object.</li>
</ul>
<h5 id="raises-6">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if <code>event_ndims</code> is not 0 or 1.</li>
<li><b><code>TypeError</code></b>: if <code>scale</code> is not a <code>LinearOperator</code>.</li>
<li><b><code>TypeError</code></b>: if <code>shift.dtype</code> does not match <code>scale.dtype</code>.</li>
<li><b><code>ValueError</code></b>: if not <code>scale.is_non_singular</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.dtype"><code id="AffineLinearOperator.dtype">tf.contrib.distributions.bijector.AffineLinearOperator.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.forwardx-nameforward-condition_kwargs"><code id="AffineLinearOperator.forward">tf.contrib.distributions.bijector.AffineLinearOperator.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-11">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-10">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-7">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.forward_event_shapeinput_shape-nameforward_event_shape"><code id="AffineLinearOperator.forward_event_shape">tf.contrib.distributions.bijector.AffineLinearOperator.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-12">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-11">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="AffineLinearOperator.forward_log_det_jacobian">tf.contrib.distributions.bijector.AffineLinearOperator.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-13">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-12">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-8">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.get_forward_event_shapeinput_shape"><code id="AffineLinearOperator.get_forward_event_shape">tf.contrib.distributions.bijector.AffineLinearOperator.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-14">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-13">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.get_inverse_event_shapeoutput_shape"><code id="AffineLinearOperator.get_inverse_event_shape">tf.contrib.distributions.bijector.AffineLinearOperator.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-15">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-14">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.graph_parents"><code id="AffineLinearOperator.graph_parents">tf.contrib.distributions.bijector.AffineLinearOperator.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.inversey-nameinverse-condition_kwargs"><code id="AffineLinearOperator.inverse">tf.contrib.distributions.bijector.AffineLinearOperator.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-16">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-15">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-9">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="AffineLinearOperator.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.AffineLinearOperator.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-17">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-16">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-10">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="AffineLinearOperator.inverse_event_shape">tf.contrib.distributions.bijector.AffineLinearOperator.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-18">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-17">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="AffineLinearOperator.inverse_log_det_jacobian">tf.contrib.distributions.bijector.AffineLinearOperator.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-19">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-18">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-11">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.is_constant_jacobian"><code id="AffineLinearOperator.is_constant_jacobian">tf.contrib.distributions.bijector.AffineLinearOperator.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-19">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.name"><code id="AffineLinearOperator.name">tf.contrib.distributions.bijector.AffineLinearOperator.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.scale"><code id="AffineLinearOperator.scale">tf.contrib.distributions.bijector.AffineLinearOperator.scale</code></h4>
<p>The <code>scale</code> <code>LinearOperator</code> in <code>Y = scale @ X.T + shift</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.shaper"><code id="AffineLinearOperator.shaper">tf.contrib.distributions.bijector.AffineLinearOperator.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.shift"><code id="AffineLinearOperator.shift">tf.contrib.distributions.bijector.AffineLinearOperator.shift</code></h4>
<p>The <code>shift</code> <code>Tensor</code> in <code>Y = scale @ X.T + shift</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.affinelinearoperator.validate_args"><code id="AffineLinearOperator.validate_args">tf.contrib.distributions.bijector.AffineLinearOperator.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.bijector"><a name="//apple_ref/cpp/Class/Bijector" class="dashAnchor"></a><code id="Bijector">class tf.contrib.distributions.bijector.Bijector</code></h3>
<p>Interface for transforming a <code>Distribution</code> sample.</p>
<p>A <code>Bijector</code> implements a <a href="https://en.wikipedia.org/wiki/Diffeomorphism">diffeomorphism</a>, i.e., a bijective, differentiable function. A <code>Bijector</code> is used by <code>TransformedDistribution</code> but can be generally used for transforming a <code>Distribution</code> generated <code>Tensor</code>. A <code>Bijector</code> is characterized by three operations:</p>
<ol style="list-style-type: decimal">
<li>Forward Evaluation</li>
</ol>
<p>Useful for turning one random outcome into another random outcome from a different distribution.</p>
<ol start="2" style="list-style-type: decimal">
<li>Inverse Evaluation</li>
</ol>
<p>Useful for &quot;reversing&quot; a transformation to compute one probability in terms of another.</p>
<ol start="3" style="list-style-type: decimal">
<li>(log o det o Jacobian o inverse)(x)</li>
</ol>
<p>&quot;The log of the determinant of the matrix of all first-order partial derivatives of the inverse function.&quot; Useful for inverting a transformation to compute one probability in terms of another. Geometrically, the det(Jacobian) is the volume of the transformation and is used to scale the probability.</p>
<p>By convention, transformations of random variables are named in terms of the forward transformation. The forward transformation creates samples, the inverse is useful for computing probabilities.</p>
<p>Example Use:</p>
<ul>
<li>Basic properties:</li>
</ul>
<p><code>python   x = ... # A tensor.   # Evaluate forward transformation.   fwd_x = my_bijector.forward(x)   x == my_bijector.inverse(fwd_x)   x != my_bijector.forward(fwd_x)  # Not equal because g(x) != g(g(x)).</code></p>
<ul>
<li>Computing a log-likelihood:</li>
</ul>
<p><code>python   def transformed_log_pdf(bijector, log_pdf, x):     return (bijector.inverse_log_det_jacobian(x) +             log_pdf(bijector.inverse(x)))</code></p>
<ul>
<li>Transforming a random outcome:</li>
</ul>
<p><code>python   def transformed_sample(bijector, x):     return bijector.forward(x)</code></p>
<p>Example transformations:</p>
<ul>
<li><p>&quot;Exponential&quot;</p>
<pre><code>Y = g(X) = exp(X)
X ~ Normal(0, 1)  # Univariate.</code></pre>
<p>Implies:</p>
<pre><code>  g^{-1}(Y) = log(Y)
  |Jacobian(g^{-1})(y)| = 1 / y
  Y ~ LogNormal(0, 1), i.e.,
  prob(Y=y) = |Jacobian(g^{-1})(y)| * prob(X=g^{-1}(y))
            = (1 / y) Normal(log(y); 0, 1)</code></pre>
<p>Here is an example of how one might implement the <code>Exp</code> bijector:</p>
<p><code>class Exp(Bijector):     def __init__(self, event_ndims=0, validate_args=False, name=&quot;exp&quot;):       super(Exp, self).__init__(batch_ndims=0, event_ndims=event_ndims,                                 validate_args=validate_args, name=name)     def _forward(self, x):       return math_ops.exp(x)     def _inverse_and_inverse_log_det_jacobian(self, y):       x = math_ops.log(y)       return x, -self._forward_log_det_jacobian(x)     def _forward_log_det_jacobian(self, x):       if self.shaper is None:         raise ValueError(&quot;Jacobian requires known event_ndims.&quot;)       _, _, event_dims = self.shaper.get_dims(x)       return math_ops.reduce_sum(x, reduction_indices=event_dims)</code></p></li>
<li><p>&quot;Affine&quot;</p>
<pre><code>Y = g(X) = sqrtSigma * X + mu
X ~ MultivariateNormal(0, I_d)</code></pre>
<p>Implies:</p>
<pre><code>  g^{-1}(Y) = inv(sqrtSigma) * (Y - mu)
  |Jacobian(g^{-1})(y)| = det(inv(sqrtSigma))
  Y ~ MultivariateNormal(mu, sqrtSigma) , i.e.,
  prob(Y=y) = |Jacobian(g^{-1})(y)| * prob(X=g^{-1}(y))
            = det(sqrtSigma)^(-d) *
              MultivariateNormal(inv(sqrtSigma) * (y - mu); 0, I_d)</code></pre></li>
</ul>
<p>Example of why a <code>Bijector</code> needs to understand sample, batch, event partitioning:</p>
<ul>
<li>Consider the <code>Exp</code> <code>Bijector</code> applied to a <code>Tensor</code> which has sample, batch, and event (S, B, E) shape semantics. Suppose the <code>Tensor</code>'s partitioned-shape is <code>(S=[4], B=[2], E=[3, 3])</code>.</li>
</ul>
<p>For <code>Exp</code>, the shape of the <code>Tensor</code> returned by <code>forward</code> and <code>inverse</code> is unchanged, i.e., <code>[4, 2, 3, 3]</code>. However the shape returned by <code>inverse_log_det_jacobian</code> is <code>[4, 2]</code> because the Jacobian is a reduction over the event dimensions.</p>
<p>Subclass Requirements:</p>
<ul>
<li>Typically subclasses implement <code>_forward</code> and one or both of:
<ul>
<li><code>_inverse</code>, <code>_inverse_log_det_jacobian</code>,</li>
<li><code>_inverse_and_inverse_log_det_jacobian</code>.</li>
</ul></li>
<li>If the <code>Bijector</code>'s use is limited to <code>TransformedDistribution</code> (or friends like <code>QuantizedDistribution</code>) then depending on your use, you may not need to implement all of <code>_forward</code> and <code>_inverse</code> functions. Examples:
<ol style="list-style-type: decimal">
<li>Sampling (e.g., <code>sample</code>) only requires <code>_forward</code>.</li>
<li>Probability functions (e.g., <code>prob</code>, <code>cdf</code>, <code>survival</code>) only require <code>_inverse</code> (and related).</li>
<li>Only calling probability functions on the output of <code>sample</code> means <code>_inverse</code> can be implemented as a cache lookup.</li>
</ol></li>
</ul>
<p>See <code>Example Use</code> [above] which shows how these functions are used to transform a distribution. (Note: <code>_forward</code> could theoretically be implemented as a cache lookup but this would require controlling the underlying sample generation mechanism.)</p>
<ul>
<li><p>If computation can be shared among <code>_inverse</code> and <code>_inverse_log_det_jacobian</code> it is preferable to implement <code>_inverse_and_inverse_log_det_jacobian</code>. This usually reduces graph-construction overhead because a <code>Distribution</code>'s implementation of <code>log_prob</code> will need to evaluate both the inverse Jacobian as well as the inverse function.</p></li>
<li><p>If an additional use case needs just <code>inverse</code> or just <code>inverse_log_det_jacobian</code> then he or she may also wish to implement these functions to avoid computing the <code>inverse_log_det_jacobian</code> or the <code>inverse</code>, respectively.</p></li>
<li><p>Subclasses should implement <code>_get_forward_event_shape</code>, <code>_forward_event_shape</code> (and <code>inverse</code> counterparts) if the transformation is shape-changing. By default the event-shape is assumed unchanged from input.</p></li>
</ul>
<p>Tips for implementing <code>_inverse</code> and <code>_inverse_log_det_jacobian</code>:</p>
<ul>
<li><p>As case 3 [above] indicates, under some circumstances the inverse function can be implemented as a cache lookup.</p></li>
<li><p>The inverse <code>log o det o Jacobian</code> can be implemented as the negative of the forward <code>log o det o Jacobian</code>. This is useful if the <code>inverse</code> is implemented as a cache or the inverse Jacobian is computationally more expensive (e.g., <code>CholeskyOuterProduct</code> <code>Bijector</code>). The following demonstrates the suggested implementation.</p></li>
</ul>
<p><code>python   def _inverse_and_log_det_jacobian(self, y):      x = # ... implement inverse, possibly via cache.      return x, -self._forward_log_det_jac(x)  # Note negation.</code></p>
<p>By overriding the <code>_inverse_and_log_det_jacobian</code> function we have access to the inverse in one call.</p>
<p>The correctness of this approach can be seen from the following claim.</p>
<ul>
<li><p>Claim:</p>
<p>Assume <code>Y=g(X)</code> is a bijection whose derivative exists and is nonzero for its domain, i.e., <code>d/dX g(X)!=0</code>. Then:</p>
<p><code>none   (log o det o jacobian o g^{-1})(Y) = -(log o det o jacobian o g)(X)</code></p></li>
<li><p>Proof:</p>
<p>From the bijective, nonzero differentiability of <code>g</code>, the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">inverse function theorem</a> implies <code>g^{-1}</code> is differentiable in the image of <code>g</code>. Applying the chain rule to <code>y = g(x) = g(g^{-1}(y))</code> yields <code>I = g'(g^{-1}(y))*g^{-1}'(y)</code>. The same theorem also implies <code>g{-1}'</code> is non-singular therefore: <code>inv[ g'(g^{-1}(y)) ] = g^{-1}'(y)</code>. The claim follows from <a href="https://en.wikipedia.org/wiki/Determinant#Multiplicativity_and_matrix_groups">properties of determinant</a>.</p></li>
<li><p>If possible, prefer a direct implementation of the inverse Jacobian. This should have superior numerical stability and will often share subgraphs with the <code>_inverse</code> implementation. - - -</p></li>
</ul>
<h4 id="tf.contrib.distributions.bijector.bijector.__init__batch_ndimsnone-event_ndimsnone-graph_parentsnone-is_constant_jacobianfalse-validate_argsfalse-dtypenone-namenone"><code id="Bijector.__init__">tf.contrib.distributions.bijector.Bijector.__init__(batch_ndims=None, event_ndims=None, graph_parents=None, is_constant_jacobian=False, validate_args=False, dtype=None, name=None)</code></h4>
<p>Constructs Bijector.</p>
<p>A <code>Bijector</code> transforms random variables into new random variables.</p>
<p>Examples:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create the Y = g(X) = X transform which operates on 4-Tensors of vectors.</span>
identity <span class="op">=</span> Identity(batch_ndims<span class="op">=</span><span class="dv">4</span>, event_ndims<span class="op">=</span><span class="dv">1</span>)

<span class="co"># Create the Y = g(X) = exp(X) transform which operates on matrices.</span>
exp <span class="op">=</span> Exp(batch_ndims<span class="op">=</span><span class="dv">0</span>, event_ndims<span class="op">=</span><span class="dv">2</span>)</code></pre></div>
<p>See <code>Bijector</code> subclass docstring for more details and specific examples.</p>
<h5 id="args-20">Args:</h5>
<ul>
<li><b><code>batch_ndims</code></b>: number of dimensions associated with batch coordinates.</li>
<li><b><code>event_ndims</code></b>: number of dimensions associated with event coordinates.</li>
<li><b><code>graph_parents</code></b>: Python list of graph prerequisites of this <code>Bijector</code>.</li>
<li><b><code>is_constant_jacobian</code></b>: <code>Boolean</code> indicating that the Jacobian is not a function of the input.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code>, default <code>False</code>. Whether to validate input with asserts. If <code>validate_args</code> is <code>False</code>, and the inputs are invalid, correct behavior is not guaranteed.</li>
<li><b><code>dtype</code></b>: <code>tf.dtype</code> supported by this <code>Bijector</code>. <code>None</code> means dtype is not enforced.</li>
<li><b><code>name</code></b>: The name to give Ops created by the initializer.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.dtype"><code id="Bijector.dtype">tf.contrib.distributions.bijector.Bijector.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.forwardx-nameforward-condition_kwargs"><code id="Bijector.forward">tf.contrib.distributions.bijector.Bijector.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-21">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-20">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-12">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Bijector.forward_event_shape">tf.contrib.distributions.bijector.Bijector.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-22">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-21">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Bijector.forward_log_det_jacobian">tf.contrib.distributions.bijector.Bijector.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-23">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-22">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-13">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.get_forward_event_shapeinput_shape"><code id="Bijector.get_forward_event_shape">tf.contrib.distributions.bijector.Bijector.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-24">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-23">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.get_inverse_event_shapeoutput_shape"><code id="Bijector.get_inverse_event_shape">tf.contrib.distributions.bijector.Bijector.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-25">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-24">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.graph_parents"><code id="Bijector.graph_parents">tf.contrib.distributions.bijector.Bijector.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.inversey-nameinverse-condition_kwargs"><code id="Bijector.inverse">tf.contrib.distributions.bijector.Bijector.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-26">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-25">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-14">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Bijector.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Bijector.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-27">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-26">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-15">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Bijector.inverse_event_shape">tf.contrib.distributions.bijector.Bijector.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-28">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-27">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Bijector.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Bijector.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-29">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-28">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-16">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.is_constant_jacobian"><code id="Bijector.is_constant_jacobian">tf.contrib.distributions.bijector.Bijector.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-29">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.name"><code id="Bijector.name">tf.contrib.distributions.bijector.Bijector.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.shaper"><code id="Bijector.shaper">tf.contrib.distributions.bijector.Bijector.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.bijector.validate_args"><code id="Bijector.validate_args">tf.contrib.distributions.bijector.Bijector.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.chain"><a name="//apple_ref/cpp/Class/Chain" class="dashAnchor"></a><code id="Chain">class tf.contrib.distributions.bijector.Chain</code></h3>
<p>Bijector which applies a sequence of bijectors.</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">chain <span class="op">=</span> Chain([Exp(), Softplus()], name<span class="op">=</span><span class="st">&quot;one_plus_exp&quot;</span>)</code></pre></div>
<p>Results in:</p>
<ul>
<li>Forward:</li>
</ul>
<p><code>python  exp = Exp()  softplus = Softplus()  Chain([exp, softplus]).forward(x)  = exp.forward(softplus.forward(x))  = tf.exp(tf.log(1. + tf.exp(x)))  = 1. + tf.exp(x)</code></p>
<ul>
<li>Inverse:</li>
</ul>
<p><code>python  exp = Exp()  softplus = Softplus()  Chain([exp, softplus]).inverse(y)  = softplus.inverse(exp.inverse(y))  = tf.log(tf.exp(tf.log(y)) - 1.)  = tf.log(y - 1.)</code> - - -</p>
<h4 id="tf.contrib.distributions.bijector.chain.__init__bijectors-validate_argsfalse-namenone"><code id="Chain.__init__">tf.contrib.distributions.bijector.Chain.__init__(bijectors=(), validate_args=False, name=None)</code></h4>
<p>Instantiates <code>Chain</code> bijector.</p>
<h5 id="args-30">Args:</h5>
<ul>
<li><b><code>bijectors</code></b>: Python list of bijector instances. An empty list makes this bijector equivalent to the <code>Identity</code> bijector.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code>, name given to ops managed by this object. Default: E.g., <code>Chain([Exp(), Softplus()]).name == &quot;chain_of_exp_of_softplus&quot;</code>.</li>
</ul>
<h5 id="raises-17">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if bijectors have different dtypes.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.bijectors"><code id="Chain.bijectors">tf.contrib.distributions.bijector.Chain.bijectors</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.dtype"><code id="Chain.dtype">tf.contrib.distributions.bijector.Chain.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.forwardx-nameforward-condition_kwargs"><code id="Chain.forward">tf.contrib.distributions.bijector.Chain.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-31">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-30">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-18">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Chain.forward_event_shape">tf.contrib.distributions.bijector.Chain.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-32">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-31">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Chain.forward_log_det_jacobian">tf.contrib.distributions.bijector.Chain.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-33">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-32">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-19">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.get_forward_event_shapeinput_shape"><code id="Chain.get_forward_event_shape">tf.contrib.distributions.bijector.Chain.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-34">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-33">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.get_inverse_event_shapeoutput_shape"><code id="Chain.get_inverse_event_shape">tf.contrib.distributions.bijector.Chain.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-35">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-34">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.graph_parents"><code id="Chain.graph_parents">tf.contrib.distributions.bijector.Chain.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.inversey-nameinverse-condition_kwargs"><code id="Chain.inverse">tf.contrib.distributions.bijector.Chain.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-36">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-35">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-20">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Chain.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Chain.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-37">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-36">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-21">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Chain.inverse_event_shape">tf.contrib.distributions.bijector.Chain.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-38">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-37">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Chain.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Chain.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-39">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-38">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-22">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.is_constant_jacobian"><code id="Chain.is_constant_jacobian">tf.contrib.distributions.bijector.Chain.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-39">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.name"><code id="Chain.name">tf.contrib.distributions.bijector.Chain.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.shaper"><code id="Chain.shaper">tf.contrib.distributions.bijector.Chain.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.chain.validate_args"><code id="Chain.validate_args">tf.contrib.distributions.bijector.Chain.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.choleskyouterproduct"><a name="//apple_ref/cpp/Class/CholeskyOuterProduct" class="dashAnchor"></a><code id="CholeskyOuterProduct">class tf.contrib.distributions.bijector.CholeskyOuterProduct</code></h3>
<p>Bijector which computes Y = g(X) = X X.T where X is a lower-triangular, positive-diagonal matrix.</p>
<p><code>event_ndims</code> must be 0 or 2, i.e., scalar or matrix.</p>
<p>Note: the upper-triangular part of X is ignored (whether or not its zero).</p>
<p>Examples:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">bijector.CholeskyOuterProduct(event_ndims<span class="op">=</span><span class="dv">2</span>).forward(x<span class="op">=</span>[[<span class="dv">1</span>., <span class="dv">0</span>], [<span class="dv">2</span>, <span class="dv">1</span>]])
<span class="co"># Result: [[1, 1], [1, 5]], i.e., x x.T</span>

bijector.SoftmaxCentered(event_ndims<span class="op">=</span><span class="dv">2</span>).inverse(y<span class="op">=</span>[[<span class="dv">1</span>., <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">5</span>]])
<span class="co"># Result: [[1, 0], [2, 1]], i.e., chol(y).</span></code></pre></div>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.__init__event_ndims2-validate_argsfalse-namecholesky_outer_product"><code id="CholeskyOuterProduct.__init__">tf.contrib.distributions.bijector.CholeskyOuterProduct.__init__(event_ndims=2, validate_args=False, name='cholesky_outer_product')</code></h4>
<p>Instantiates the <code>CholeskyOuterProduct</code> bijector.</p>
<h5 id="args-40">Args:</h5>
<ul>
<li><b><code>event_ndims</code></b>: <code>constant</code> <code>int32</code> scalar <code>Tensor</code> indicating the number of dimensions associated with a particular draw from the distribution. Must be 0 or 2.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code> name given to ops managed by this object.</li>
</ul>
<h5 id="raises-23">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if event_ndims is neither 0 or 2.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.dtype"><code id="CholeskyOuterProduct.dtype">tf.contrib.distributions.bijector.CholeskyOuterProduct.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.forwardx-nameforward-condition_kwargs"><code id="CholeskyOuterProduct.forward">tf.contrib.distributions.bijector.CholeskyOuterProduct.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-41">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-40">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-24">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.forward_event_shapeinput_shape-nameforward_event_shape"><code id="CholeskyOuterProduct.forward_event_shape">tf.contrib.distributions.bijector.CholeskyOuterProduct.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-42">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-41">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="CholeskyOuterProduct.forward_log_det_jacobian">tf.contrib.distributions.bijector.CholeskyOuterProduct.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-43">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-42">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-25">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.get_forward_event_shapeinput_shape"><code id="CholeskyOuterProduct.get_forward_event_shape">tf.contrib.distributions.bijector.CholeskyOuterProduct.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-44">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-43">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.get_inverse_event_shapeoutput_shape"><code id="CholeskyOuterProduct.get_inverse_event_shape">tf.contrib.distributions.bijector.CholeskyOuterProduct.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-45">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-44">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.graph_parents"><code id="CholeskyOuterProduct.graph_parents">tf.contrib.distributions.bijector.CholeskyOuterProduct.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.inversey-nameinverse-condition_kwargs"><code id="CholeskyOuterProduct.inverse">tf.contrib.distributions.bijector.CholeskyOuterProduct.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-46">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-45">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-26">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="CholeskyOuterProduct.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.CholeskyOuterProduct.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-47">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-46">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-27">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="CholeskyOuterProduct.inverse_event_shape">tf.contrib.distributions.bijector.CholeskyOuterProduct.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-48">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-47">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="CholeskyOuterProduct.inverse_log_det_jacobian">tf.contrib.distributions.bijector.CholeskyOuterProduct.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-49">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-48">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-28">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.is_constant_jacobian"><code id="CholeskyOuterProduct.is_constant_jacobian">tf.contrib.distributions.bijector.CholeskyOuterProduct.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-49">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.name"><code id="CholeskyOuterProduct.name">tf.contrib.distributions.bijector.CholeskyOuterProduct.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.shaper"><code id="CholeskyOuterProduct.shaper">tf.contrib.distributions.bijector.CholeskyOuterProduct.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.choleskyouterproduct.validate_args"><code id="CholeskyOuterProduct.validate_args">tf.contrib.distributions.bijector.CholeskyOuterProduct.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.exp"><a name="//apple_ref/cpp/Class/Exp" class="dashAnchor"></a><code id="Exp">class tf.contrib.distributions.bijector.Exp</code></h3>
<p>Bijector which computes Y = g(X) = exp(X).</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create the Y=g(X)=exp(X) transform which works only on Tensors with 1</span>
<span class="co"># batch ndim and 2 event ndims (i.e., vector of matrices).</span>
exp <span class="op">=</span> Exp(batch_ndims<span class="op">=</span><span class="dv">1</span>, event_ndims<span class="op">=</span><span class="dv">2</span>)
x <span class="op">=</span> [[[<span class="dv">1</span>., <span class="dv">2</span>],
       [<span class="dv">3</span>, <span class="dv">4</span>]],
      [[<span class="dv">5</span>, <span class="dv">6</span>],
       [<span class="dv">7</span>, <span class="dv">8</span>]]]
exp(x) <span class="op">==</span> exp.forward(x)
log(x) <span class="op">==</span> exp.inverse(x)</code></pre></div>
<p>Note: the exp(.) is applied element-wise but the Jacobian is a reduction over the event space. - - -</p>
<h4 id="tf.contrib.distributions.bijector.exp.__init__event_ndims0-validate_argsfalse-nameexp"><code id="Exp.__init__">tf.contrib.distributions.bijector.Exp.__init__(event_ndims=0, validate_args=False, name='exp')</code></h4>
<p>Instantiates the <code>Exp</code> bijector.</p>
<h5 id="args-50">Args:</h5>
<ul>
<li><b><code>event_ndims</code></b>: Scalar <code>int32</code> <code>Tensor</code> indicating the number of dimensions associated with a particular draw from the distribution.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code> name given to ops managed by this object.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.dtype"><code id="Exp.dtype">tf.contrib.distributions.bijector.Exp.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.forwardx-nameforward-condition_kwargs"><code id="Exp.forward">tf.contrib.distributions.bijector.Exp.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-51">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-50">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-29">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Exp.forward_event_shape">tf.contrib.distributions.bijector.Exp.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-52">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-51">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Exp.forward_log_det_jacobian">tf.contrib.distributions.bijector.Exp.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-53">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-52">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-30">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.get_forward_event_shapeinput_shape"><code id="Exp.get_forward_event_shape">tf.contrib.distributions.bijector.Exp.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-54">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-53">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.get_inverse_event_shapeoutput_shape"><code id="Exp.get_inverse_event_shape">tf.contrib.distributions.bijector.Exp.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-55">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-54">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.graph_parents"><code id="Exp.graph_parents">tf.contrib.distributions.bijector.Exp.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.inversey-nameinverse-condition_kwargs"><code id="Exp.inverse">tf.contrib.distributions.bijector.Exp.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-56">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-55">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-31">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Exp.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Exp.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-57">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-56">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-32">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Exp.inverse_event_shape">tf.contrib.distributions.bijector.Exp.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-58">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-57">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Exp.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Exp.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-59">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-58">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-33">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.is_constant_jacobian"><code id="Exp.is_constant_jacobian">tf.contrib.distributions.bijector.Exp.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-59">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.name"><code id="Exp.name">tf.contrib.distributions.bijector.Exp.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.power"><code id="Exp.power">tf.contrib.distributions.bijector.Exp.power</code></h4>
<p>The <code>c</code> in: <code>Y = g(X) = (1 + X * c)**(1 / c)</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.shaper"><code id="Exp.shaper">tf.contrib.distributions.bijector.Exp.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.exp.validate_args"><code id="Exp.validate_args">tf.contrib.distributions.bijector.Exp.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.identity"><a name="//apple_ref/cpp/Class/Identity" class="dashAnchor"></a><code id="Identity">class tf.contrib.distributions.bijector.Identity</code></h3>
<p>Bijector which computes Y = g(X) = X.</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create the Y=g(X)=X transform which is intended for Tensors with 1 batch</span>
<span class="co"># ndim and 1 event ndim (i.e., vector of vectors).</span>
identity <span class="op">=</span> Identity(batch_ndims<span class="op">=</span><span class="dv">1</span>, event_ndims<span class="op">=</span><span class="dv">1</span>)
x <span class="op">=</span> [[<span class="dv">1</span>., <span class="dv">2</span>],
     [<span class="dv">3</span>, <span class="dv">4</span>]]
x <span class="op">==</span> identity.forward(x) <span class="op">==</span> identity.inverse(x)</code></pre></div>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.__init__validate_argsfalse-nameidentity"><code id="Identity.__init__">tf.contrib.distributions.bijector.Identity.__init__(validate_args=False, name='identity')</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.dtype"><code id="Identity.dtype">tf.contrib.distributions.bijector.Identity.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.forwardx-nameforward-condition_kwargs"><code id="Identity.forward">tf.contrib.distributions.bijector.Identity.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-60">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-60">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-34">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Identity.forward_event_shape">tf.contrib.distributions.bijector.Identity.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-61">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-61">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Identity.forward_log_det_jacobian">tf.contrib.distributions.bijector.Identity.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-62">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-62">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-35">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.get_forward_event_shapeinput_shape"><code id="Identity.get_forward_event_shape">tf.contrib.distributions.bijector.Identity.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-63">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-63">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.get_inverse_event_shapeoutput_shape"><code id="Identity.get_inverse_event_shape">tf.contrib.distributions.bijector.Identity.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-64">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-64">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.graph_parents"><code id="Identity.graph_parents">tf.contrib.distributions.bijector.Identity.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.inversey-nameinverse-condition_kwargs"><code id="Identity.inverse">tf.contrib.distributions.bijector.Identity.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-65">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-65">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-36">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Identity.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Identity.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-66">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-66">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-37">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Identity.inverse_event_shape">tf.contrib.distributions.bijector.Identity.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-67">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-67">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Identity.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Identity.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-68">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-68">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-38">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.is_constant_jacobian"><code id="Identity.is_constant_jacobian">tf.contrib.distributions.bijector.Identity.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-69">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.name"><code id="Identity.name">tf.contrib.distributions.bijector.Identity.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.shaper"><code id="Identity.shaper">tf.contrib.distributions.bijector.Identity.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.identity.validate_args"><code id="Identity.validate_args">tf.contrib.distributions.bijector.Identity.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.inline"><a name="//apple_ref/cpp/Class/Inline" class="dashAnchor"></a><code id="Inline">class tf.contrib.distributions.bijector.Inline</code></h3>
<p>Bijector constructed from callables implementing forward, inverse, and inverse_log_det_jacobian.</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">exp <span class="op">=</span> Inline(
  forward_fn<span class="op">=</span>tf.exp,
  inverse_fn<span class="op">=</span>tf.log,
  inverse_log_det_jacobian_fn<span class="op">=</span>(
    <span class="kw">lambda</span> y: <span class="op">-</span>tf.reduce_sum(tf.log(y), reduction_indices<span class="op">=-</span><span class="dv">1</span>)),
  name<span class="op">=</span><span class="st">&quot;exp&quot;</span>)</code></pre></div>
<p>The above example is equivalent to the <code>Bijector</code> <code>Exp(event_ndims=1)</code>. - - -</p>
<h4 id="tf.contrib.distributions.bijector.inline.__init__forward_fnnone-inverse_fnnone-inverse_log_det_jacobian_fnnone-forward_log_det_jacobian_fnnone-get_forward_event_shape_fnnone-forward_event_shape_fnnone-get_inverse_event_shape_fnnone-inverse_event_shape_fnnone-is_constant_jacobianfalse-validate_argsfalse-nameinline"><code id="Inline.__init__">tf.contrib.distributions.bijector.Inline.__init__(forward_fn=None, inverse_fn=None, inverse_log_det_jacobian_fn=None, forward_log_det_jacobian_fn=None, get_forward_event_shape_fn=None, forward_event_shape_fn=None, get_inverse_event_shape_fn=None, inverse_event_shape_fn=None, is_constant_jacobian=False, validate_args=False, name='inline')</code></h4>
<p>Creates a <code>Bijector</code> from callables.</p>
<h5 id="args-69">Args:</h5>
<ul>
<li><b><code>forward_fn</code></b>: Python callable implementing the forward transformation.</li>
<li><b><code>inverse_fn</code></b>: Python callable implementing the inverse transformation.</li>
<li><b><code>inverse_log_det_jacobian_fn</code></b>: Python callable implementing the log o det o jacobian of the inverse transformation.</li>
<li><b><code>forward_log_det_jacobian_fn</code></b>: Python callable implementing the log o det o jacobian of the forward transformation.</li>
<li><b><code>get_forward_event_shape_fn</code></b>: Python callable implementing non-identical static event shape changes. Default: shape is assumed unchanged.</li>
<li><b><code>forward_event_shape_fn</code></b>: Python callable implementing non-identical event shape changes. Default: shape is assumed unchanged.</li>
<li><b><code>get_inverse_event_shape_fn</code></b>: Python callable implementing non-identical static event shape changes. Default: shape is assumed unchanged.</li>
<li><b><code>inverse_event_shape_fn</code></b>: Python callable implementing non-identical event shape changes. Default: shape is assumed unchanged.</li>
<li><b><code>is_constant_jacobian</code></b>: <code>Boolean</code> indicating that the Jacobian is constant for all input arguments.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code>, name given to ops managed by this object.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.dtype"><code id="Inline.dtype">tf.contrib.distributions.bijector.Inline.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.forwardx-nameforward-condition_kwargs"><code id="Inline.forward">tf.contrib.distributions.bijector.Inline.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-70">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-70">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-39">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Inline.forward_event_shape">tf.contrib.distributions.bijector.Inline.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-71">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-71">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Inline.forward_log_det_jacobian">tf.contrib.distributions.bijector.Inline.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-72">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-72">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-40">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.get_forward_event_shapeinput_shape"><code id="Inline.get_forward_event_shape">tf.contrib.distributions.bijector.Inline.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-73">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-73">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.get_inverse_event_shapeoutput_shape"><code id="Inline.get_inverse_event_shape">tf.contrib.distributions.bijector.Inline.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-74">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-74">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.graph_parents"><code id="Inline.graph_parents">tf.contrib.distributions.bijector.Inline.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.inversey-nameinverse-condition_kwargs"><code id="Inline.inverse">tf.contrib.distributions.bijector.Inline.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-75">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-75">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-41">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Inline.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Inline.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-76">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-76">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-42">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Inline.inverse_event_shape">tf.contrib.distributions.bijector.Inline.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-77">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-77">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Inline.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Inline.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-78">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-78">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-43">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.is_constant_jacobian"><code id="Inline.is_constant_jacobian">tf.contrib.distributions.bijector.Inline.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-79">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.name"><code id="Inline.name">tf.contrib.distributions.bijector.Inline.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.shaper"><code id="Inline.shaper">tf.contrib.distributions.bijector.Inline.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.inline.validate_args"><code id="Inline.validate_args">tf.contrib.distributions.bijector.Inline.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.invert"><a name="//apple_ref/cpp/Class/Invert" class="dashAnchor"></a><code id="Invert">class tf.contrib.distributions.bijector.Invert</code></h3>
<p>Bijector which inverts another Bijector.</p>
<p>Example Use: <a href="https://reference.wolfram.com/language/ref/ExpGammaDistribution.html">ExpGammaDistribution (see Background &amp; Context)</a> models <code>Y=log(X)</code> where <code>X ~ Gamma</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">exp_gamma_distribution <span class="op">=</span> TransformedDistribution(
  Gamma(alpha<span class="op">=</span><span class="dv">1</span>., beta<span class="op">=</span><span class="dv">2</span>.),
  bijector.Invert(bijector.Exp())</code></pre></div>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.__init__bijector-validate_argsfalse-namenone"><code id="Invert.__init__">tf.contrib.distributions.bijector.Invert.__init__(bijector, validate_args=False, name=None)</code></h4>
<p>Creates a <code>Bijector</code> which swaps the meaning of <code>inverse</code> and <code>forward</code>.</p>
<p>Note: An inverted bijector's <code>inverse_log_det_jacobian</code> is often more efficient if the base bijector implements <code>_forward_log_det_jacobian</code>. If <code>_forward_log_det_jacobian</code> is not implemented then the following code is used:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">y <span class="op">=</span> <span class="va">self</span>.inverse(x, <span class="op">**</span>condition_kwargs)
<span class="cf">return</span> <span class="op">-</span><span class="va">self</span>.inverse_log_det_jacobian(y, <span class="op">**</span>condition_kwargs)</code></pre></div>
<h5 id="args-79">Args:</h5>
<ul>
<li><b><code>bijector</code></b>: Bijector instance.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code>, name given to ops managed by this object.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.bijector"><code id="Invert.bijector">tf.contrib.distributions.bijector.Invert.bijector</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.dtype"><code id="Invert.dtype">tf.contrib.distributions.bijector.Invert.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.forwardx-nameforward-condition_kwargs"><code id="Invert.forward">tf.contrib.distributions.bijector.Invert.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-80">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-80">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-44">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Invert.forward_event_shape">tf.contrib.distributions.bijector.Invert.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-81">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-81">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Invert.forward_log_det_jacobian">tf.contrib.distributions.bijector.Invert.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-82">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-82">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-45">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.get_forward_event_shapeinput_shape"><code id="Invert.get_forward_event_shape">tf.contrib.distributions.bijector.Invert.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-83">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-83">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.get_inverse_event_shapeoutput_shape"><code id="Invert.get_inverse_event_shape">tf.contrib.distributions.bijector.Invert.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-84">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-84">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.graph_parents"><code id="Invert.graph_parents">tf.contrib.distributions.bijector.Invert.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.inversey-nameinverse-condition_kwargs"><code id="Invert.inverse">tf.contrib.distributions.bijector.Invert.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-85">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-85">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-46">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Invert.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Invert.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-86">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-86">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-47">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Invert.inverse_event_shape">tf.contrib.distributions.bijector.Invert.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-87">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-87">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Invert.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Invert.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-88">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-88">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-48">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.is_constant_jacobian"><code id="Invert.is_constant_jacobian">tf.contrib.distributions.bijector.Invert.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-89">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.name"><code id="Invert.name">tf.contrib.distributions.bijector.Invert.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.shaper"><code id="Invert.shaper">tf.contrib.distributions.bijector.Invert.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.invert.validate_args"><code id="Invert.validate_args">tf.contrib.distributions.bijector.Invert.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.sigmoidcentered"><a name="//apple_ref/cpp/Class/SigmoidCentered" class="dashAnchor"></a><code id="SigmoidCentered">class tf.contrib.distributions.bijector.SigmoidCentered</code></h3>
<p>Bijector which computes Y = g(X) = exp([X 0]) / (1 + exp(-X)).</p>
<p>Equivalent to: <code>bijector.SoftmaxCentered(event_ndims=0)</code>.</p>
<p>See <code>bijector.SoftmaxCentered</code> for more details. - - -</p>
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.__init__validate_argsfalse-namesigmoid_centered"><code id="SigmoidCentered.__init__">tf.contrib.distributions.bijector.SigmoidCentered.__init__(validate_args=False, name='sigmoid_centered')</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.dtype"><code id="SigmoidCentered.dtype">tf.contrib.distributions.bijector.SigmoidCentered.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.forwardx-nameforward-condition_kwargs"><code id="SigmoidCentered.forward">tf.contrib.distributions.bijector.SigmoidCentered.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-89">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-90">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-49">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.forward_event_shapeinput_shape-nameforward_event_shape"><code id="SigmoidCentered.forward_event_shape">tf.contrib.distributions.bijector.SigmoidCentered.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-90">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-91">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="SigmoidCentered.forward_log_det_jacobian">tf.contrib.distributions.bijector.SigmoidCentered.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-91">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-92">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-50">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.get_forward_event_shapeinput_shape"><code id="SigmoidCentered.get_forward_event_shape">tf.contrib.distributions.bijector.SigmoidCentered.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-92">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-93">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.get_inverse_event_shapeoutput_shape"><code id="SigmoidCentered.get_inverse_event_shape">tf.contrib.distributions.bijector.SigmoidCentered.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-93">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-94">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.graph_parents"><code id="SigmoidCentered.graph_parents">tf.contrib.distributions.bijector.SigmoidCentered.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.inversey-nameinverse-condition_kwargs"><code id="SigmoidCentered.inverse">tf.contrib.distributions.bijector.SigmoidCentered.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-94">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-95">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-51">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="SigmoidCentered.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.SigmoidCentered.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-95">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-96">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-52">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="SigmoidCentered.inverse_event_shape">tf.contrib.distributions.bijector.SigmoidCentered.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-96">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-97">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="SigmoidCentered.inverse_log_det_jacobian">tf.contrib.distributions.bijector.SigmoidCentered.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-97">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-98">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-53">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.is_constant_jacobian"><code id="SigmoidCentered.is_constant_jacobian">tf.contrib.distributions.bijector.SigmoidCentered.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-99">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.name"><code id="SigmoidCentered.name">tf.contrib.distributions.bijector.SigmoidCentered.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.shaper"><code id="SigmoidCentered.shaper">tf.contrib.distributions.bijector.SigmoidCentered.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.sigmoidcentered.validate_args"><code id="SigmoidCentered.validate_args">tf.contrib.distributions.bijector.SigmoidCentered.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.softmaxcentered"><a name="//apple_ref/cpp/Class/SoftmaxCentered" class="dashAnchor"></a><code id="SoftmaxCentered">class tf.contrib.distributions.bijector.SoftmaxCentered</code></h3>
<p>Bijector which computes <code>Y = g(X) = exp([X 0]) / sum(exp([X 0]))</code>.</p>
<p>To implement <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> as a bijection, the forward transformation appends a value to the input and the inverse removes this coordinate. The appended coordinate represents a pivot, e.g., <code>softmax(x) = exp(x-c) / sum(exp(x-c))</code> where <code>c</code> is the implicit last coordinate.</p>
<p>Because we append a coordinate, this bijector only supports <code>event_ndim in [0, 1]</code>, i.e., scalars and vectors.</p>
<p>Example Use:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">bijector.SoftmaxCentered(event_ndims<span class="op">=</span><span class="dv">1</span>).forward(tf.log([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]))
<span class="co"># Result: [0.2, 0.3, 0.4, 0.1]</span>
<span class="co"># Extra result: 0.1</span>

bijector.SoftmaxCentered(event_ndims<span class="op">=</span><span class="dv">1</span>).inverse([<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>])
<span class="co"># Result: tf.log([2, 3, 4])</span>
<span class="co"># Extra coordinate removed.</span></code></pre></div>
<p>At first blush it may seem like the <a href="https://en.wikipedia.org/wiki/Invariance_of_domain">Invariance of domain</a> theorem implies this implementation is not a bijection. However, the appended dimension makes the (forward) image non-open and the theorem does not directly apply. - - -</p>
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.__init__event_ndims0-validate_argsfalse-namesoftmax_centered"><code id="SoftmaxCentered.__init__">tf.contrib.distributions.bijector.SoftmaxCentered.__init__(event_ndims=0, validate_args=False, name='softmax_centered')</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.dtype"><code id="SoftmaxCentered.dtype">tf.contrib.distributions.bijector.SoftmaxCentered.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.forwardx-nameforward-condition_kwargs"><code id="SoftmaxCentered.forward">tf.contrib.distributions.bijector.SoftmaxCentered.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-98">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-100">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-54">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.forward_event_shapeinput_shape-nameforward_event_shape"><code id="SoftmaxCentered.forward_event_shape">tf.contrib.distributions.bijector.SoftmaxCentered.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-99">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-101">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="SoftmaxCentered.forward_log_det_jacobian">tf.contrib.distributions.bijector.SoftmaxCentered.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-100">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-102">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-55">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.get_forward_event_shapeinput_shape"><code id="SoftmaxCentered.get_forward_event_shape">tf.contrib.distributions.bijector.SoftmaxCentered.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-101">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-103">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.get_inverse_event_shapeoutput_shape"><code id="SoftmaxCentered.get_inverse_event_shape">tf.contrib.distributions.bijector.SoftmaxCentered.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-102">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-104">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.graph_parents"><code id="SoftmaxCentered.graph_parents">tf.contrib.distributions.bijector.SoftmaxCentered.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.inversey-nameinverse-condition_kwargs"><code id="SoftmaxCentered.inverse">tf.contrib.distributions.bijector.SoftmaxCentered.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-103">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-105">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-56">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="SoftmaxCentered.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.SoftmaxCentered.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-104">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-106">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-57">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="SoftmaxCentered.inverse_event_shape">tf.contrib.distributions.bijector.SoftmaxCentered.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-105">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-107">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="SoftmaxCentered.inverse_log_det_jacobian">tf.contrib.distributions.bijector.SoftmaxCentered.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-106">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-108">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-58">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.is_constant_jacobian"><code id="SoftmaxCentered.is_constant_jacobian">tf.contrib.distributions.bijector.SoftmaxCentered.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-109">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.name"><code id="SoftmaxCentered.name">tf.contrib.distributions.bijector.SoftmaxCentered.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.shaper"><code id="SoftmaxCentered.shaper">tf.contrib.distributions.bijector.SoftmaxCentered.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softmaxcentered.validate_args"><code id="SoftmaxCentered.validate_args">tf.contrib.distributions.bijector.SoftmaxCentered.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.softplus"><a name="//apple_ref/cpp/Class/Softplus" class="dashAnchor"></a><code id="Softplus">class tf.contrib.distributions.bijector.Softplus</code></h3>
<p>Bijector which computes <code>Y = g(X) = Log[1 + exp(X)]</code>.</p>
<p>The softplus <code>Bijector</code> has the following two useful properties:</p>
<ul>
<li>The domain is the positive real numbers</li>
<li><code>softplus(x) approx x</code>, for large <code>x</code>, so it does not overflow as easily as the <code>Exp</code> <code>Bijector</code>.</li>
</ul>
<p>Example Use:</p>
<p><code>python   # Create the Y=g(X)=softplus(X) transform which works only on Tensors with 1   # batch ndim and 2 event ndims (i.e., vector of matrices).   softplus = Softplus(batch_ndims=1, event_ndims=2)   x = [[[1., 2],          [3, 4]],         [[5, 6],          [7, 8]]]   log(1 + exp(x)) == softplus.forward(x)   log(exp(x) - 1) == softplus.inverse(x)</code></p>
<p>Note: log(.) and exp(.) are applied element-wise but the Jacobian is a reduction over the event space. - - -</p>
<h4 id="tf.contrib.distributions.bijector.softplus.__init__event_ndims0-validate_argsfalse-namesoftplus"><code id="Softplus.__init__">tf.contrib.distributions.bijector.Softplus.__init__(event_ndims=0, validate_args=False, name='softplus')</code></h4>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.dtype"><code id="Softplus.dtype">tf.contrib.distributions.bijector.Softplus.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.forwardx-nameforward-condition_kwargs"><code id="Softplus.forward">tf.contrib.distributions.bijector.Softplus.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-107">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-110">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-59">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.forward_event_shapeinput_shape-nameforward_event_shape"><code id="Softplus.forward_event_shape">tf.contrib.distributions.bijector.Softplus.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-108">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-111">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="Softplus.forward_log_det_jacobian">tf.contrib.distributions.bijector.Softplus.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-109">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-112">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-60">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.get_forward_event_shapeinput_shape"><code id="Softplus.get_forward_event_shape">tf.contrib.distributions.bijector.Softplus.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-110">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-113">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.get_inverse_event_shapeoutput_shape"><code id="Softplus.get_inverse_event_shape">tf.contrib.distributions.bijector.Softplus.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-111">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-114">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.graph_parents"><code id="Softplus.graph_parents">tf.contrib.distributions.bijector.Softplus.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.inversey-nameinverse-condition_kwargs"><code id="Softplus.inverse">tf.contrib.distributions.bijector.Softplus.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-112">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-115">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-61">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="Softplus.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.Softplus.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-113">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-116">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-62">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="Softplus.inverse_event_shape">tf.contrib.distributions.bijector.Softplus.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-114">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-117">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="Softplus.inverse_log_det_jacobian">tf.contrib.distributions.bijector.Softplus.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-115">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-118">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-63">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.is_constant_jacobian"><code id="Softplus.is_constant_jacobian">tf.contrib.distributions.bijector.Softplus.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-119">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.name"><code id="Softplus.name">tf.contrib.distributions.bijector.Softplus.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.shaper"><code id="Softplus.shaper">tf.contrib.distributions.bijector.Softplus.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.softplus.validate_args"><code id="Softplus.validate_args">tf.contrib.distributions.bijector.Softplus.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
<h2 id="other-functions-and-classes">Other Functions and Classes</h2>
<hr />
<h3 id="class-tf.contrib.distributions.bijector.powertransform"><a name="//apple_ref/cpp/Class/PowerTransform" class="dashAnchor"></a><code id="PowerTransform">class tf.contrib.distributions.bijector.PowerTransform</code></h3>
<p>Bijector which computes <code>Y = g(X) = (1 + X * c)**(1 / c), X &gt;= -1 / c</code>.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Power_transform">power transform</a> maps inputs from <code>[0, inf]</code> to <code>[-1/c, inf]</code>; this is equivalent to the <code>inverse</code> of this bijector.</p>
<p>This bijector is equivalent to the <code>Exp</code> bijector when <code>c=0</code>. - - -</p>
<h4 id="tf.contrib.distributions.bijector.powertransform.__init__power0.0-event_ndims0-validate_argsfalse-namepower_transform"><code id="PowerTransform.__init__">tf.contrib.distributions.bijector.PowerTransform.__init__(power=0.0, event_ndims=0, validate_args=False, name='power_transform')</code></h4>
<p>Instantiates the <code>PowerTransform</code> bijector.</p>
<h5 id="args-116">Args:</h5>
<ul>
<li><b><code>power</code></b>: Python <code>float</code> scalar indicating the transform power, i.e., <code>Y = g(X) = (1 + X * c)**(1 / c)</code> where <code>c</code> is the <code>power</code>.</li>
<li><b><code>event_ndims</code></b>: Python scalar indicating the number of dimensions associated with a particular draw from the distribution.</li>
<li><b><code>validate_args</code></b>: <code>Boolean</code> indicating whether arguments should be checked for correctness.</li>
<li><b><code>name</code></b>: <code>String</code> name given to ops managed by this object.</li>
</ul>
<h5 id="raises-64">Raises:</h5>
<ul>
<li><b><code>ValueError</code></b>: if <code>power &lt; 0</code> or is not known statically.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.dtype"><code id="PowerTransform.dtype">tf.contrib.distributions.bijector.PowerTransform.dtype</code></h4>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.forwardx-nameforward-condition_kwargs"><code id="PowerTransform.forward">tf.contrib.distributions.bijector.PowerTransform.forward(x, name='forward', **condition_kwargs)</code></h4>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h5 id="args-117">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-120">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-65">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.forward_event_shapeinput_shape-nameforward_event_shape"><code id="PowerTransform.forward_event_shape">tf.contrib.distributions.bijector.PowerTransform.forward_event_shape(input_shape, name='forward_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-118">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-121">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.forward_log_det_jacobianx-nameforward_log_det_jacobian-condition_kwargs"><code id="PowerTransform.forward_log_det_jacobian">tf.contrib.distributions.bijector.PowerTransform.forward_log_det_jacobian(x, name='forward_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the forward_log_det_jacobian.</p>
<h5 id="args-119">Args:</h5>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-122">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-66">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.get_forward_event_shapeinput_shape"><code id="PowerTransform.get_forward_event_shape">tf.contrib.distributions.bijector.PowerTransform.get_forward_event_shape(input_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape</code>. May be only partially defined.</p>
<h5 id="args-120">Args:</h5>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h5 id="returns-123">Returns:</h5>
<ul>
<li><b><code>forward_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.get_inverse_event_shapeoutput_shape"><code id="PowerTransform.get_inverse_event_shape">tf.contrib.distributions.bijector.PowerTransform.get_inverse_event_shape(output_shape)</code></h4>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape</code>. May be only partially defined.</p>
<h5 id="args-121">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h5 id="returns-124">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.graph_parents"><code id="PowerTransform.graph_parents">tf.contrib.distributions.bijector.PowerTransform.graph_parents</code></h4>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.inversey-nameinverse-condition_kwargs"><code id="PowerTransform.inverse">tf.contrib.distributions.bijector.PowerTransform.inverse(y, name='inverse', **condition_kwargs)</code></h4>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h5 id="args-122">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-125">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-67">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.inverse_and_inverse_log_det_jacobiany-nameinverse_and_inverse_log_det_jacobian-condition_kwargs"><code id="PowerTransform.inverse_and_inverse_log_det_jacobian">tf.contrib.distributions.bijector.PowerTransform.inverse_and_inverse_log_det_jacobian(y, name='inverse_and_inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns both the inverse evaluation and inverse_log_det_jacobian.</p>
<p>Enables possibly more efficient calculation when both inverse and corresponding Jacobian are needed.</p>
<p>See <code>inverse()</code>, <code>inverse_log_det_jacobian()</code> for more details.</p>
<h5 id="args-123">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-126">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-68">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_and_inverse_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.inverse_event_shapeoutput_shape-nameinverse_event_shape"><code id="PowerTransform.inverse_event_shape">tf.contrib.distributions.bijector.PowerTransform.inverse_event_shape(output_shape, name='inverse_event_shape')</code></h4>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h5 id="args-124">Args:</h5>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h5 id="returns-127">Returns:</h5>
<ul>
<li><b><code>inverse_event_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.inverse_log_det_jacobiany-nameinverse_log_det_jacobian-condition_kwargs"><code id="PowerTransform.inverse_log_det_jacobian">tf.contrib.distributions.bijector.PowerTransform.inverse_log_det_jacobian(y, name='inverse_log_det_jacobian', **condition_kwargs)</code></h4>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h5 id="args-125">Args:</h5>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
<li><b><code>**condition_kwargs</code></b>: Named arguments forwarded to subclass implementation.</li>
</ul>
<h5 id="returns-128">Returns:</h5>
<p><code>Tensor</code>.</p>
<h5 id="raises-69">Raises:</h5>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_inverse_log_det_jacobian</code> nor <code>_inverse_and_inverse_log_det_jacobian</code> are implemented.</li>
</ul>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.is_constant_jacobian"><code id="PowerTransform.is_constant_jacobian">tf.contrib.distributions.bijector.PowerTransform.is_constant_jacobian</code></h4>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h5 id="returns-129">Returns:</h5>
<p><code>Boolean</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.name"><code id="PowerTransform.name">tf.contrib.distributions.bijector.PowerTransform.name</code></h4>
<p>Returns the string name of this <code>Bijector</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.power"><code id="PowerTransform.power">tf.contrib.distributions.bijector.PowerTransform.power</code></h4>
<p>The <code>c</code> in: <code>Y = g(X) = (1 + X * c)**(1 / c)</code>.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.shaper"><code id="PowerTransform.shaper">tf.contrib.distributions.bijector.PowerTransform.shaper</code></h4>
<p>Returns shape object used to manage shape constraints.</p>
<hr />
<h4 id="tf.contrib.distributions.bijector.powertransform.validate_args"><code id="PowerTransform.validate_args">tf.contrib.distributions.bijector.PowerTransform.validate_args</code></h4>
<p>Returns True if Tensor arguments will be validated.</p>
