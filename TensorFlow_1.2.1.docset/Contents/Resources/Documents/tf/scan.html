<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.scan" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.scan" class="dashAnchor"></a><h1 id="tf.scan">tf.scan</h1>
<h3 id="tf.scan-1"><code>tf.scan</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">scan(
    fn,
    elems,
    initializer<span class="op">=</span><span class="va">None</span>,
    parallel_iterations<span class="op">=</span><span class="dv">10</span>,
    back_prop<span class="op">=</span><span class="va">True</span>,
    swap_memory<span class="op">=</span><span class="va">False</span>,
    infer_shape<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/functional_ops.py"><code>tensorflow/python/ops/functional_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/functional_ops.md#Higher_Order_Operators">Higher Order Functions &gt; Higher Order Operators</a></p>
<p>scan on the list of tensors unpacked from <code>elems</code> on dimension 0.</p>
<p>The simplest version of <code>scan</code> repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn. If <code>initializer</code> is None, <code>elems</code> must contain at least one element, and its first element is used as the initializer.</p>
<p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>[len(values)] + fn(initializer, values[0]).shape</code>.</p>
<p>This method also allows multi-arity <code>elems</code> and accumulator. If <code>elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The second argument of <code>fn</code> must match the structure of <code>elems</code>.</p>
<p>If no <code>initializer</code> is provided, the output structure and dtypes of <code>fn</code> are assumed to be the same as its input; and in this case, the first argument of <code>fn</code> must match the structure of <code>elems</code>.</p>
<p>If an <code>initializer</code> is provided, then the output of <code>fn</code> must have the same structure as <code>initializer</code>; and the first argument of <code>fn</code> must match this structure.</p>
<p>For example, if <code>elems</code> is <code>(t1, [t2, t3])</code> and <code>initializer</code> is <code>[i1, i2]</code> then an appropriate signature for <code>fn</code> in <code>python2</code> is: <code>fn = lambda (acc_p1, acc_p2), (t1 [t2, t3]):</code> and <code>fn</code> must return a list, <code>[acc_n1, acc_n2]</code>. An alternative correct signature for <code>fn</code>, and the one that works in <code>python3</code>, is: <code>fn = lambda a, t:</code>, where <code>a</code> and <code>t</code> correspond to the input tuples.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>fn</code></b>: The callable to be performed. It accepts two arguments. The first will have the same structure as <code>initializer</code> if one is provided, otherwise it will have the same structure as <code>elems</code>. The second will have the same (possibly nested) structure as <code>elems</code>. Its output must have the same structure as <code>initializer</code> if one is provided, otherwise it must have the same structure as <code>elems</code>.</li>
<li><b><code>elems</code></b>: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be the first argument to <code>fn</code>.</li>
<li><b><code>initializer</code></b>: (optional) A tensor or (possibly nested) sequence of tensors, initial value for the accumulator, and the expected output type of <code>fn</code>.</li>
<li><b><code>parallel_iterations</code></b>: (optional) The number of iterations allowed to run in parallel.</li>
<li><b><code>back_prop</code></b>: (optional) True enables support for back propagation.</li>
<li><b><code>swap_memory</code></b>: (optional) True enables GPU-CPU memory swapping.</li>
<li><b><code>infer_shape</code></b>: (optional) False disables tests for consistent output shapes.</li>
<li><b><code>name</code></b>: (optional) Name prefix for the returned tensors.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code>fn</code> to tensors unpacked from <code>elems</code> along the first dimension, and the previous accumulator value(s), from first to last.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>fn</code> is not callable or the structure of the output of <code>fn</code> and <code>initializer</code> do not match.</li>
<li><b><code>ValueError</code></b>: if the lengths of the output of <code>fn</code> and <code>initializer</code> do not match.</li>
</ul>
<p>Examples: <code>python   elems = np.array([1, 2, 3, 4, 5, 6])   sum = scan(lambda a, x: a + x, elems)   # sum == [1, 3, 6, 10, 15, 21]</code></p>
<p><code>python   elems = np.array([1, 2, 3, 4, 5, 6])   initializer = np.array(0)   sum_one = scan(       lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)   # sum_one == [1, 2, 3, 4, 5, 6]</code></p>
<p><code>python   elems = np.array([1, 0, 0, 0, 0, 0])   initializer = (np.array(0), np.array(1))   fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)   # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])</code></p>
