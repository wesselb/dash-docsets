<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.batch_normalization" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.batch_normalization" class="dashAnchor"></a><h1 id="tf.nn.batch_normalization">tf.nn.batch_normalization</h1>
<h3 id="tf.nn.batch_normalization-1"><code>tf.nn.batch_normalization</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">batch_normalization(
    x,
    mean,
    variance,
    offset,
    scale,
    variance_epsilon,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/nn_impl.py"><code>tensorflow/python/ops/nn_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../../api_guides/python/nn.md#Normalization">Neural Network &gt; Normalization</a></p>
<p>Batch normalization.</p>
<p>As described in http://arxiv.org/abs/1502.03167. Normalizes a tensor by <code>mean</code> and <code>variance</code>, and applies (optionally) a <code>scale</code> \(\) to it, as well as an <code>offset</code> \(\):</p>
<p>\(+\)</p>
<p><code>mean</code>, <code>variance</code>, <code>offset</code> and <code>scale</code> are all expected to be of one of two shapes:</p>
<ul>
<li>In all generality, they can have the same number of dimensions as the input <code>x</code>, with identical sizes as <code>x</code> for the dimensions that are not normalized over (the 'depth' dimension(s)), and dimension 1 for the others which are being normalized over. <code>mean</code> and <code>variance</code> in this case would typically be the outputs of <code>tf.nn.moments(..., keep_dims=True)</code> during training, or running averages thereof during inference.</li>
<li>In the common case where the 'depth' dimension is the last dimension in the input tensor <code>x</code>, they may be one dimensional tensors of the same size as the 'depth' dimension. This is the case for example for the common <code>[batch, depth]</code> layout of fully-connected layers, and <code>[batch, height, width, depth]</code> for convolutions. <code>mean</code> and <code>variance</code> in this case would typically be the outputs of <code>tf.nn.moments(..., keep_dims=False)</code> during training, or running averages thereof during inference.</li>
</ul>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>x</code></b>: Input <code>Tensor</code> of arbitrary dimensionality.</li>
<li><b><code>mean</code></b>: A mean <code>Tensor</code>.</li>
<li><b><code>variance</code></b>: A variance <code>Tensor</code>.</li>
<li><b><code>offset</code></b>: An offset <code>Tensor</code>, often denoted \(\) in equations, or None. If present, will be added to the normalized tensor.</li>
<li><b><code>scale</code></b>: A scale <code>Tensor</code>, often denoted \(\) in equations, or <code>None</code>. If present, the scale is applied to the normalized tensor.</li>
<li><b><code>variance_epsilon</code></b>: A small float number to avoid dividing by 0.</li>
<li><b><code>name</code></b>: A name for this operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>the normalized, scaled, offset tensor.</p>
