<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.embedding_lookup_sparse" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.embedding_lookup_sparse" class="dashAnchor"></a><h1 id="tf.nn.embedding_lookup_sparse">tf.nn.embedding_lookup_sparse</h1>
<h3 id="tf.nn.embedding_lookup_sparse-1"><code>tf.nn.embedding_lookup_sparse</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">embedding_lookup_sparse(
    params,
    sp_ids,
    sp_weights,
    partition_strategy<span class="op">=</span><span class="st">&#39;mod&#39;</span>,
    name<span class="op">=</span><span class="va">None</span>,
    combiner<span class="op">=</span><span class="va">None</span>,
    max_norm<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/embedding_ops.py"><code>tensorflow/python/ops/embedding_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../../api_guides/python/nn.md#Embeddings">Neural Network &gt; Embeddings</a></p>
<p>Computes embeddings for the given ids and weights.</p>
<p>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</p>
<p>It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>params</code></b>: A single tensor representing the complete embedding tensor, or a list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a <code>PartitionedVariable</code>, created by partitioning along dimension 0. Each element must be appropriately sized for the given <code>partition_strategy</code>.</li>
<li><b><code>sp_ids</code></b>: N x M SparseTensor of int64 ids (typically from FeatureValueToId), where N is typically batch size and M is arbitrary.</li>
<li><b><code>sp_weights</code></b>: either a SparseTensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, sp_weights must have exactly the same shape and indices as sp_ids.</li>
<li><b><code>partition_strategy</code></b>: A string specifying the partitioning strategy, relevant if <code>len(params) &gt; 1</code>. Currently <code>&quot;div&quot;</code> and <code>&quot;mod&quot;</code> are supported. Default is <code>&quot;mod&quot;</code>. See <code>tf.nn.embedding_lookup</code> for more details.</li>
<li><b><code>name</code></b>: Optional name for the op.</li>
<li><b><code>combiner</code></b>: A string specifying the reduction op. Currently &quot;mean&quot;, &quot;sqrtn&quot; and &quot;sum&quot; are supported. &quot;sum&quot; computes the weighted sum of the embedding results for each row. &quot;mean&quot; is the weighted sum divided by the total weight. &quot;sqrtn&quot; is the weighted sum divided by the square root of the sum of the squares of the weights.</li>
<li><b><code>max_norm</code></b>: If not None, each embedding is normalized to have l2 norm equal to max_norm before combining.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by sp_ids, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified.</p>
<p>In other words, if</p>
<pre><code>shape(combined params) = [p0, p1, ..., pm]</code></pre>
<p>and</p>
<pre><code>shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn]</code></pre>
<p>then</p>
<pre><code>shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].</code></pre>
<p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p>
<pre><code>[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id 0, weight 1.0
[2, 3]: id 1, weight 3.0</code></pre>
<p>with <code>combiner</code>=&quot;mean&quot;, then the output will be a 3x20 matrix where</p>
<pre><code>output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
output[1, :] = params[0, :] * 1.0
output[2, :] = params[1, :] * 3.0</code></pre>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If sp_ids is not a SparseTensor, or if sp_weights is neither None nor SparseTensor.</li>
<li><b><code>ValueError</code></b>: If combiner is not one of {&quot;mean&quot;, &quot;sqrtn&quot;, &quot;sum&quot;}.</li>
</ul>
