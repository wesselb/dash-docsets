<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.crelu" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.crelu" class="dashAnchor"></a><h1 id="tf.nn.crelu">tf.nn.crelu</h1>
<h3 id="tf.nn.crelu-1"><code>tf.nn.crelu</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">crelu(
    features,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/nn_ops.py"><code>tensorflow/python/ops/nn_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../../api_guides/python/nn.md#Activation_Functions">Neural Network &gt; Activation Functions</a></p>
<p>Computes Concatenated ReLU.</p>
<p>Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the <em>negative</em> part of the activation. Note that as a result this non-linearity doubles the depth of the activations. Source: <a href="https://arxiv.org/abs/1603.05201">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units. W. Shang, et al.</a></p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>features</code></b>: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, or <code>int8</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> with the same type as <code>features</code>.</p>
