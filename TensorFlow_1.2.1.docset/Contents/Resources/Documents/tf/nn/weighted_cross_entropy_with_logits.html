<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.weighted_cross_entropy_with_logits" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.weighted_cross_entropy_with_logits" class="dashAnchor"></a><h1 id="tf.nn.weighted_cross_entropy_with_logits">tf.nn.weighted_cross_entropy_with_logits</h1>
<h3 id="tf.nn.weighted_cross_entropy_with_logits-1"><code>tf.nn.weighted_cross_entropy_with_logits</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">weighted_cross_entropy_with_logits(
    targets,
    logits,
    pos_weight,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/nn_impl.py"><code>tensorflow/python/ops/nn_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../../api_guides/python/nn.md#Classification">Neural Network &gt; Classification</a></p>
<p>Computes a weighted cross entropy.</p>
<p>This is like <code>sigmoid_cross_entropy_with_logits()</code> except that <code>pos_weight</code>, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p>
<p>The usual cross-entropy cost is defined as:</p>
<pre><code>targets * -log(sigmoid(logits)) +
    (1 - targets) * -log(1 - sigmoid(logits))</code></pre>
<p>The argument <code>pos_weight</code> is used as a multiplier for the positive targets:</p>
<pre><code>targets * -log(sigmoid(logits)) * pos_weight +
    (1 - targets) * -log(1 - sigmoid(logits))</code></pre>
<p>For brevity, let <code>x = logits</code>, <code>z = targets</code>, <code>q = pos_weight</code>. The loss is:</p>
<pre><code>  qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))
= (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))</code></pre>
<p>Setting <code>l = (1 + (q - 1) * z)</code>, to ensure stability and avoid overflow, the implementation uses</p>
<pre><code>(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))</code></pre>
<p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>targets</code></b>: A <code>Tensor</code> of the same type and shape as <code>logits</code>.</li>
<li><b><code>logits</code></b>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li>
<li><b><code>pos_weight</code></b>: A coefficient to use on the positive examples.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise weighted logistic losses.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>logits</code> and <code>targets</code> do not have the same shape.</li>
</ul>
