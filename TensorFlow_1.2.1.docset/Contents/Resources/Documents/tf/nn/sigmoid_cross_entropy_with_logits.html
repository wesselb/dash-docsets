<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.nn.sigmoid_cross_entropy_with_logits" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.nn.sigmoid_cross_entropy_with_logits" class="dashAnchor"></a><h1 id="tf.nn.sigmoid_cross_entropy_with_logits">tf.nn.sigmoid_cross_entropy_with_logits</h1>
<h3 id="tf.nn.sigmoid_cross_entropy_with_logits-1"><code>tf.nn.sigmoid_cross_entropy_with_logits</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">sigmoid_cross_entropy_with_logits(
    _sentinel<span class="op">=</span><span class="va">None</span>,
    labels<span class="op">=</span><span class="va">None</span>,
    logits<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/nn_impl.py"><code>tensorflow/python/ops/nn_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../../api_guides/python/nn.md#Classification">Neural Network &gt; Classification</a></p>
<p>Computes sigmoid cross entropy given <code>logits</code>.</p>
<p>Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.</p>
<p>For brevity, let <code>x = logits</code>, <code>z = labels</code>. The logistic loss is</p>
<pre><code>  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))</code></pre>
<p>For x &lt; 0, to avoid overflow in exp(-x), we reformulate the above</p>
<pre><code>  x - x * z + log(1 + exp(-x))
= log(exp(x)) - x * z + log(1 + exp(-x))
= - x * z + log(1 + exp(x))</code></pre>
<p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation</p>
<pre><code>max(x, 0) - x * z + log(1 + exp(-abs(x)))</code></pre>
<p><code>logits</code> and <code>labels</code> must have the same type and shape.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>_sentinel</code></b>: Used to prevent positional parameters. Internal, do not use.</li>
<li><b><code>labels</code></b>: A <code>Tensor</code> of the same type and shape as <code>logits</code>.</li>
<li><b><code>logits</code></b>: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise logistic losses.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If <code>logits</code> and <code>labels</code> do not have the same shape.</li>
</ul>
