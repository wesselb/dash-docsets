<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.legacy_seq2seq.embedding_attention_decoder" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.contrib.legacy_seq2seq.embedding_attention_decoder" class="dashAnchor"></a><h1 id="tf.contrib.legacy_seq2seq.embedding_attention_decoder">tf.contrib.legacy_seq2seq.embedding_attention_decoder</h1>
<h3 id="tf.contrib.legacy_seq2seq.embedding_attention_decoder-1"><code>tf.contrib.legacy_seq2seq.embedding_attention_decoder</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">embedding_attention_decoder(
    decoder_inputs,
    initial_state,
    attention_states,
    cell,
    num_symbols,
    embedding_size,
    num_heads<span class="op">=</span><span class="dv">1</span>,
    output_size<span class="op">=</span><span class="va">None</span>,
    output_projection<span class="op">=</span><span class="va">None</span>,
    feed_previous<span class="op">=</span><span class="va">False</span>,
    update_embedding_for_previous<span class="op">=</span><span class="va">True</span>,
    dtype<span class="op">=</span><span class="va">None</span>,
    scope<span class="op">=</span><span class="va">None</span>,
    initial_state_attention<span class="op">=</span><span class="va">False</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py"><code>tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py</code></a>.</p>
<p>RNN decoder with embedding and attention and a pure-decoding option.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>decoder_inputs</code></b>: A list of 1D batch-sized int32 Tensors (decoder inputs).</li>
<li><b><code>initial_state</code></b>: 2D Tensor [batch_size x cell.state_size].</li>
<li><b><code>attention_states</code></b>: 3D Tensor [batch_size x attn_length x attn_size].</li>
<li><b><code>cell</code></b>: tf.nn.rnn_cell.RNNCell defining the cell function.</li>
<li><b><code>num_symbols</code></b>: Integer, how many symbols come into the embedding.</li>
<li><b><code>embedding_size</code></b>: Integer, the length of the embedding vector for each symbol.</li>
<li><b><code>num_heads</code></b>: Number of attention heads that read from attention_states.</li>
<li><b><code>output_size</code></b>: Size of the output vectors; if None, use output_size.</li>
<li><b><code>output_projection</code></b>: None or a pair (W, B) of output projection weights and biases; W has shape [output_size x num_symbols] and B has shape [num_symbols]; if provided and feed_previous=True, each fed previous output will first be multiplied by W and added B.</li>
<li><b><code>feed_previous</code></b>: Boolean; if True, only the first of decoder_inputs will be used (the &quot;GO&quot; symbol), and all other decoder inputs will be generated by: next = embedding_lookup(embedding, argmax(previous_output)), In effect, this implements a greedy decoder. It can also be used during training to emulate http://arxiv.org/abs/1506.03099. If False, decoder_inputs are used as given (the standard decoder case).</li>
<li><b><code>update_embedding_for_previous</code></b>: Boolean; if False and feed_previous=True, only the embedding for the first symbol of decoder_inputs (the &quot;GO&quot; symbol) will be updated by back propagation. Embeddings for the symbols generated from the decoder itself remain unchanged. This parameter has no effect if feed_previous=False.</li>
<li><b><code>dtype</code></b>: The dtype to use for the RNN initial states (default: tf.float32).</li>
<li><b><code>scope</code></b>: VariableScope for the created subgraph; defaults to &quot;embedding_attention_decoder&quot;.</li>
<li><b><code>initial_state_attention</code></b>: If False (default), initial attentions are zero. If True, initialize the attentions from the initial state and attention states -- useful when we wish to resume decoding from a previously stored decoder state and attention states.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tuple of the form (outputs, state), where: outputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x output_size] containing the generated outputs. state: The state of each decoder cell at the final time-step. It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: When output_projection has the wrong shape.</li>
</ul>
