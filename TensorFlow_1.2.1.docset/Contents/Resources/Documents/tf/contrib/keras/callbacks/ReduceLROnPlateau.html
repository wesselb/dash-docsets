<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.keras.callbacks.ReduceLROnPlateau" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="in_cooldown"/> <meta itemprop="property" content="on_batch_begin"/> <meta itemprop="property" content="on_batch_end"/> <meta itemprop="property" content="on_epoch_begin"/> <meta itemprop="property" content="on_epoch_end"/> <meta itemprop="property" content="on_train_begin"/> <meta itemprop="property" content="on_train_end"/> <meta itemprop="property" content="set_model"/> <meta itemprop="property" content="set_params"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.keras.callbacks.ReduceLROnPlateau" class="dashAnchor"></a><h1 id="tf.contrib.keras.callbacks.reducelronplateau">tf.contrib.keras.callbacks.ReduceLROnPlateau</h1>
<h3 id="class-tf.contrib.keras.callbacks.reducelronplateau"><code>class tf.contrib.keras.callbacks.ReduceLROnPlateau</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/keras/python/keras/callbacks.py"><code>tensorflow/contrib/keras/python/keras/callbacks.py</code></a>.</p>
<p>Reduce learning rate when a metric has stopped improving.</p>
<p>Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.</p>
<p>Example: <code>python         reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,                                       patience=5, min_lr=0.001)         model.fit(X_train, Y_train, callbacks=[reduce_lr])</code></p>
<h4 id="arguments">Arguments:</h4>
<pre><code>monitor: quantity to be monitored.
factor: factor by which the learning rate will
    be reduced. new_lr = lr * factor
patience: number of epochs with no improvement
    after which learning rate will be reduced.
verbose: int. 0: quiet, 1: update messages.
mode: one of {auto, min, max}. In `min` mode,
    lr will be reduced when the quantity
    monitored has stopped decreasing; in `max`
    mode it will be reduced when the quantity
    monitored has stopped increasing; in `auto`
    mode, the direction is automatically inferred
    from the name of the monitored quantity.
epsilon: threshold for measuring the new optimum,
    to only focus on significant changes.
cooldown: number of epochs to wait before resuming
    normal operation after lr has been reduced.
min_lr: lower bound on the learning rate.</code></pre>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>,
    factor<span class="op">=</span><span class="fl">0.1</span>,
    patience<span class="op">=</span><span class="dv">10</span>,
    verbose<span class="op">=</span><span class="dv">0</span>,
    mode<span class="op">=</span><span class="st">&#39;auto&#39;</span>,
    epsilon<span class="op">=</span><span class="fl">0.0001</span>,
    cooldown<span class="op">=</span><span class="dv">0</span>,
    min_lr<span class="op">=</span><span class="dv">0</span>
)</code></pre></div>
<h3 id="in_cooldown">
<code>in_cooldown</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">in_cooldown()</code></pre></div>
<h3 id="on_batch_begin">
<code>on_batch_begin</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_batch_begin(
    batch,
    logs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="on_batch_end">
<code>on_batch_end</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_batch_end(
    batch,
    logs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="on_epoch_begin">
<code>on_epoch_begin</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_epoch_begin(
    epoch,
    logs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="on_epoch_end">
<code>on_epoch_end</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_epoch_end(
    epoch,
    logs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="on_train_begin">
<code>on_train_begin</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_train_begin(logs<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<h3 id="on_train_end">
<code>on_train_end</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">on_train_end(logs<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<h3 id="set_model">
<code>set_model</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_model(model)</code></pre></div>
<h3 id="set_params">
<code>set_params</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_params(params)</code></pre></div>
