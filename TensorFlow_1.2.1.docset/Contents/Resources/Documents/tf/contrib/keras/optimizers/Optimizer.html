<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.keras.optimizers.Optimizer" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="from_config"/> <meta itemprop="property" content="get_config"/> <meta itemprop="property" content="get_gradients"/> <meta itemprop="property" content="get_updates"/> <meta itemprop="property" content="get_weights"/> <meta itemprop="property" content="set_weights"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.keras.optimizers.Optimizer" class="dashAnchor"></a><h1 id="tf.contrib.keras.optimizers.optimizer">tf.contrib.keras.optimizers.Optimizer</h1>
<h3 id="class-tf.contrib.keras.optimizers.optimizer"><code>class tf.contrib.keras.optimizers.Optimizer</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/keras/python/keras/optimizers.py"><code>tensorflow/contrib/keras/python/keras/optimizers.py</code></a>.</p>
<p>Abstract optimizer base class.</p>
<p>Note: this is the parent class of all optimizers, not an actual optimizer that can be used for training models.</p>
<p>All Keras optimizers support the following keyword arguments:</p>
<pre><code>clipnorm: float &gt;= 0. Gradients will be clipped
    when their L2 norm exceeds this value.
clipvalue: float &gt;= 0. Gradients will be clipped
    when their absolute value exceeds this value.</code></pre>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(<span class="op">**</span>kwargs)</code></pre></div>
<h3 id="from_config">
<code>from_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">from_config(
    cls,
    config
)</code></pre></div>
<h3 id="get_config">
<code>get_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_config()</code></pre></div>
<h3 id="get_gradients">
<code>get_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_gradients(
    loss,
    params
)</code></pre></div>
<h3 id="get_updates">
<code>get_updates</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates(
    params,
    constraints,
    loss
)</code></pre></div>
<h3 id="get_weights">
<code>get_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_weights()</code></pre></div>
<p>Returns the current value of the weights of the optimizer.</p>
<h4 id="returns">Returns:</h4>
<pre><code>A list of numpy arrays.</code></pre>
<h3 id="set_weights">
<code>set_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_weights(weights)</code></pre></div>
<p>Sets the weights of the optimizer, from Numpy arrays.</p>
<p>Should only be called after computing the gradients (otherwise the optimizer has no weights).</p>
<h4 id="arguments">Arguments:</h4>
<pre><code>weights: a list of Numpy arrays. The number
    of arrays and their shape must match
    number of the dimensions of the weights
    of the optimizer (i.e. it should match the
    output of `get_weights`).</code></pre>
<h4 id="raises">Raises:</h4>
<pre><code>ValueError: in case of incompatible weight shapes.</code></pre>
