<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.keras.optimizers.Adamax" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="from_config"/> <meta itemprop="property" content="get_config"/> <meta itemprop="property" content="get_gradients"/> <meta itemprop="property" content="get_updates"/> <meta itemprop="property" content="get_weights"/> <meta itemprop="property" content="set_weights"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.keras.optimizers.Adamax" class="dashAnchor"></a><h1 id="tf.contrib.keras.optimizers.adamax">tf.contrib.keras.optimizers.Adamax</h1>
<h3 id="class-tf.contrib.keras.optimizers.adamax"><code>class tf.contrib.keras.optimizers.Adamax</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/keras/python/keras/optimizers.py"><code>tensorflow/contrib/keras/python/keras/optimizers.py</code></a>.</p>
<p>Adamax optimizer from Adam paper's Section 7.</p>
<p>It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper.</p>
<h4 id="arguments">Arguments:</h4>
<pre><code>lr: float &gt;= 0. Learning rate.
beta_1/beta_2: floats, 0 &lt; beta &lt; 1. Generally close to 1.
epsilon: float &gt;= 0. Fuzz factor.
decay: float &gt;= 0. Learning rate decay over each update.</code></pre>
<p>References: - <a href="http://arxiv.org/abs/1412.6980v8">Adam - A Method for Stochastic Optimization</a></p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    lr<span class="op">=</span><span class="fl">0.002</span>,
    beta_1<span class="op">=</span><span class="fl">0.9</span>,
    beta_2<span class="op">=</span><span class="fl">0.999</span>,
    epsilon<span class="op">=</span><span class="fl">1e-08</span>,
    decay<span class="op">=</span><span class="fl">0.0</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="from_config">
<code>from_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">from_config(
    cls,
    config
)</code></pre></div>
<h3 id="get_config">
<code>get_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_config()</code></pre></div>
<h3 id="get_gradients">
<code>get_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_gradients(
    loss,
    params
)</code></pre></div>
<h3 id="get_updates">
<code>get_updates</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates(
    params,
    constraints,
    loss
)</code></pre></div>
<h3 id="get_weights">
<code>get_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_weights()</code></pre></div>
<p>Returns the current value of the weights of the optimizer.</p>
<h4 id="returns">Returns:</h4>
<pre><code>A list of numpy arrays.</code></pre>
<h3 id="set_weights">
<code>set_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_weights(weights)</code></pre></div>
<p>Sets the weights of the optimizer, from Numpy arrays.</p>
<p>Should only be called after computing the gradients (otherwise the optimizer has no weights).</p>
<h4 id="arguments-1">Arguments:</h4>
<pre><code>weights: a list of Numpy arrays. The number
    of arrays and their shape must match
    number of the dimensions of the weights
    of the optimizer (i.e. it should match the
    output of `get_weights`).</code></pre>
<h4 id="raises">Raises:</h4>
<pre><code>ValueError: in case of incompatible weight shapes.</code></pre>
