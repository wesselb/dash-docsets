<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.keras.layers.LSTM" /> <meta itemprop="property" content="constraints"/> <meta itemprop="property" content="graph"/> <meta itemprop="property" content="input"/> <meta itemprop="property" content="input_mask"/> <meta itemprop="property" content="input_shape"/> <meta itemprop="property" content="losses"/> <meta itemprop="property" content="non_trainable_variables"/> <meta itemprop="property" content="non_trainable_weights"/> <meta itemprop="property" content="output"/> <meta itemprop="property" content="output_mask"/> <meta itemprop="property" content="output_shape"/> <meta itemprop="property" content="scope_name"/> <meta itemprop="property" content="trainable_variables"/> <meta itemprop="property" content="trainable_weights"/> <meta itemprop="property" content="updates"/> <meta itemprop="property" content="variables"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__deepcopy__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_loss"/> <meta itemprop="property" content="add_update"/> <meta itemprop="property" content="add_variable"/> <meta itemprop="property" content="add_weight"/> <meta itemprop="property" content="apply"/> <meta itemprop="property" content="build"/> <meta itemprop="property" content="call"/> <meta itemprop="property" content="compute_mask"/> <meta itemprop="property" content="count_params"/> <meta itemprop="property" content="from_config"/> <meta itemprop="property" content="get_config"/> <meta itemprop="property" content="get_constants"/> <meta itemprop="property" content="get_initial_states"/> <meta itemprop="property" content="get_input_at"/> <meta itemprop="property" content="get_input_mask_at"/> <meta itemprop="property" content="get_input_shape_at"/> <meta itemprop="property" content="get_losses_for"/> <meta itemprop="property" content="get_output_at"/> <meta itemprop="property" content="get_output_mask_at"/> <meta itemprop="property" content="get_output_shape_at"/> <meta itemprop="property" content="get_updates_for"/> <meta itemprop="property" content="get_weights"/> <meta itemprop="property" content="preprocess_input"/> <meta itemprop="property" content="reset_states"/> <meta itemprop="property" content="set_weights"/> <meta itemprop="property" content="step"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.keras.layers.LSTM" class="dashAnchor"></a><h1 id="tf.contrib.keras.layers.lstm">tf.contrib.keras.layers.LSTM</h1>
<h3 id="class-tf.contrib.keras.layers.lstm"><code>class tf.contrib.keras.layers.LSTM</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/keras/python/keras/layers/recurrent.py"><code>tensorflow/contrib/keras/python/keras/layers/recurrent.py</code></a>.</p>
<p>Long-Short Term Memory unit - Hochreiter 1997.</p>
<p>For a step-by-step description of the algorithm, see <a href="http://deeplearning.net/tutorial/lstm.html">this tutorial</a>.</p>
<h4 id="arguments">Arguments:</h4>
<pre><code>units: Positive integer, dimensionality of the output space.
activation: Activation function to use.
    If you pass None, no activation is applied
    (ie. &quot;linear&quot; activation: `a(x) = x`).
recurrent_activation: Activation function to use
    for the recurrent step.
use_bias: Boolean, whether the layer uses a bias vector.
kernel_initializer: Initializer for the `kernel` weights matrix,
    used for the linear transformation of the inputs..
recurrent_initializer: Initializer for the `recurrent_kernel`
    weights matrix,
    used for the linear transformation of the recurrent state..
bias_initializer: Initializer for the bias vector.
unit_forget_bias: Boolean.
    If True, add 1 to the bias of the forget gate at initialization.
    Setting it to true will also force `bias_initializer=&quot;zeros&quot;`.
    This is recommended in [Jozefowicz et
      al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
kernel_regularizer: Regularizer function applied to
    the `kernel` weights matrix.
recurrent_regularizer: Regularizer function applied to
    the `recurrent_kernel` weights matrix.
bias_regularizer: Regularizer function applied to the bias vector.
activity_regularizer: Regularizer function applied to
    the output of the layer (its &quot;activation&quot;)..
kernel_constraint: Constraint function applied to
    the `kernel` weights matrix.
recurrent_constraint: Constraint function applied to
    the `recurrent_kernel` weights matrix.
bias_constraint: Constraint function applied to the bias vector.
dropout: Float between 0 and 1.
    Fraction of the units to drop for
    the linear transformation of the inputs.
recurrent_dropout: Float between 0 and 1.
    Fraction of the units to drop for
    the linear transformation of the recurrent state.</code></pre>
<p>References: - <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">Long short-term memory</a> (original 1997 paper) - <a href="http://www.cs.toronto.edu/~graves/preprint.pdf">Supervised sequence labeling with recurrent neural networks</a> - <a href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p>
<h2 id="properties">Properties</h2>
<h3 id="constraints">
<code>constraints</code>
</h3>
<h3 id="graph">
<code>graph</code>
</h3>
<h3 id="input">
<code>input</code>
</h3>
<p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer.</p>
<h4 id="returns">Returns:</h4>
<pre><code>Input tensor or list of input tensors.</code></pre>
<h4 id="raises">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="input_mask">
<code>input_mask</code>
</h3>
<p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer.</p>
<h4 id="returns-1">Returns:</h4>
<pre><code>Input mask tensor (potentially None) or list of input
mask tensors.</code></pre>
<h4 id="raises-1">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="input_shape">
<code>input_shape</code>
</h3>
<p>Retrieves the input shape(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer.</p>
<h4 id="returns-2">Returns:</h4>
<pre><code>Input shape, as `TensorShape`
(or list of `TensorShape`, one tuple per input tensor).</code></pre>
<h4 id="raises-2">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="losses">
<code>losses</code>
</h3>
<h3 id="non_trainable_variables">
<code>non_trainable_variables</code>
</h3>
<h3 id="non_trainable_weights">
<code>non_trainable_weights</code>
</h3>
<h3 id="output">
<code>output</code>
</h3>
<p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer.</p>
<h4 id="returns-3">Returns:</h4>
<pre><code>Output tensor or list of output tensors.</code></pre>
<h4 id="raises-3">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="output_mask">
<code>output_mask</code>
</h3>
<p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer.</p>
<h4 id="returns-4">Returns:</h4>
<pre><code>Output mask tensor (potentially None) or list of output
mask tensors.</code></pre>
<h4 id="raises-4">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="output_shape">
<code>output_shape</code>
</h3>
<p>Retrieves the output shape(s) of a layer.</p>
<p>Only applicable if the layer has one inbound node, or if all inbound nodes have the same output shape.</p>
<h4 id="returns-5">Returns:</h4>
<pre><code>Output shape, as `TensorShape`
(or list of `TensorShape`, one tuple per output tensor).</code></pre>
<h4 id="raises-5">Raises:</h4>
<pre><code>AttributeError: if the layer is connected to
more than one incoming layers.</code></pre>
<h3 id="scope_name">
<code>scope_name</code>
</h3>
<h3 id="trainable_variables">
<code>trainable_variables</code>
</h3>
<h3 id="trainable_weights">
<code>trainable_weights</code>
</h3>
<h3 id="updates">
<code>updates</code>
</h3>
<h3 id="variables">
<code>variables</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-6">Returns:</h4>
<p>A list of variables.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-7">Returns:</h4>
<p>A list of variables.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    units,
    activation<span class="op">=</span><span class="st">&#39;tanh&#39;</span>,
    recurrent_activation<span class="op">=</span><span class="st">&#39;hard_sigmoid&#39;</span>,
    use_bias<span class="op">=</span><span class="va">True</span>,
    kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>,
    recurrent_initializer<span class="op">=</span><span class="st">&#39;orthogonal&#39;</span>,
    bias_initializer<span class="op">=</span><span class="st">&#39;zeros&#39;</span>,
    unit_forget_bias<span class="op">=</span><span class="va">True</span>,
    kernel_regularizer<span class="op">=</span><span class="va">None</span>,
    recurrent_regularizer<span class="op">=</span><span class="va">None</span>,
    bias_regularizer<span class="op">=</span><span class="va">None</span>,
    activity_regularizer<span class="op">=</span><span class="va">None</span>,
    kernel_constraint<span class="op">=</span><span class="va">None</span>,
    recurrent_constraint<span class="op">=</span><span class="va">None</span>,
    bias_constraint<span class="op">=</span><span class="va">None</span>,
    dropout<span class="op">=</span><span class="fl">0.0</span>,
    recurrent_dropout<span class="op">=</span><span class="fl">0.0</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    inputs,
    initial_state<span class="op">=</span><span class="va">None</span>,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="__deepcopy__">
<code><strong>deepcopy</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__deepcopy__(memo)</code></pre></div>
<h3 id="add_loss">
<code>add_loss</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_loss(
    losses,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a specific set of inputs.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>losses</code></b>: Loss tensor, or list/tuple of tensors.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the loss(es) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses are created. If <code>None</code> is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses).</li>
</ul>
<h3 id="add_update">
<code>add_update</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_update(
    updates,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a specific set of inputs.</p>
<h4 id="arguments-2">Arguments:</h4>
<ul>
<li><b><code>updates</code></b>: Update op, or list/tuple of update ops.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the update(s) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates are created. If <code>None</code> is passed, the updates are assumed to be unconditional, and will apply across all dataflows of the layer.</li>
</ul>
<h3 id="add_variable">
<code>add_variable</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_variable(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<h4 id="arguments-3">Arguments:</h4>
<ul>
<li><b><code>name</code></b>: variable name.</li>
<li><b><code>shape</code></b>: variable shape.</li>
<li><b><code>dtype</code></b>: The type of the variable. Defaults to <code>self.dtype</code>.</li>
<li><b><code>initializer</code></b>: initializer instance (callable).</li>
<li><b><code>regularizer</code></b>: regularizer instance (callable).</li>
<li><b><code>trainable</code></b>: whether the variable should be part of the layer's &quot;trainable_variables&quot; (e.g. variables, biases) or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p>The created variable.</p>
<h3 id="add_weight">
<code>add_weight</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_weight(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>,
    constraint<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Adds a weight variable to the layer.</p>
<h4 id="arguments-4">Arguments:</h4>
<pre><code>name: String, the name for the weight variable.
shape: The shape tuple of the weight.
dtype: The dtype of the weight.
initializer: An Initializer instance (callable).
regularizer: An optional Regularizer instance.
trainable: A boolean, whether the weight should
    be trained via backprop or not (assuming
    that the layer itself is also trainable).
constraint: An optional Constraint instance.</code></pre>
<h4 id="returns-9">Returns:</h4>
<pre><code>The created weight variable.</code></pre>
<h3 id="apply">
<code>apply</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">apply</span>(
    inputs,
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Apply the layer on a input.</p>
<p>This simply wraps <code>self.__call__</code>.</p>
<h4 id="arguments-5">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor(s). *args: additional positional arguments to be passed to <code>self.call</code>. **kwargs: additional keyword arguments to be passed to <code>self.call</code>.</li>
</ul>
<h4 id="returns-10">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="build">
<code>build</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">build(input_shape)</code></pre></div>
<h3 id="call">
<code>call</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">call(
    inputs,
    mask<span class="op">=</span><span class="va">None</span>,
    initial_state<span class="op">=</span><span class="va">None</span>,
    training<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="compute_mask">
<code>compute_mask</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">compute_mask(
    inputs,
    mask
)</code></pre></div>
<h3 id="count_params">
<code>count_params</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">count_params()</code></pre></div>
<p>Count the total number of scalars composing the weights.</p>
<h4 id="returns-11">Returns:</h4>
<pre><code>An integer count.</code></pre>
<h4 id="raises-6">Raises:</h4>
<pre><code>RuntimeError: if the layer isn&#39;t yet built
    (in which case its weights aren&#39;t yet defined).</code></pre>
<h3 id="from_config">
<code>from_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">from_config(
    cls,
    config
)</code></pre></div>
<p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Container), nor weights (handled by <code>set_weights</code>).</p>
<h4 id="arguments-6">Arguments:</h4>
<pre><code>config: A Python dictionary, typically the
    output of get_config.</code></pre>
<h4 id="returns-12">Returns:</h4>
<pre><code>A layer instance.</code></pre>
<h3 id="get_config">
<code>get_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_config()</code></pre></div>
<h3 id="get_constants">
<code>get_constants</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_constants(
    inputs,
    training<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="get_initial_states">
<code>get_initial_states</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_initial_states(inputs)</code></pre></div>
<h3 id="get_input_at">
<code>get_input_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_input_at(node_index)</code></pre></div>
<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<h4 id="arguments-7">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-13">Returns:</h4>
<pre><code>A tensor (or list of tensors if the layer has multiple inputs).</code></pre>
<h3 id="get_input_mask_at">
<code>get_input_mask_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_input_mask_at(node_index)</code></pre></div>
<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<h4 id="arguments-8">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-14">Returns:</h4>
<pre><code>A mask tensor
(or list of tensors if the layer has multiple inputs).</code></pre>
<h3 id="get_input_shape_at">
<code>get_input_shape_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_input_shape_at(node_index)</code></pre></div>
<p>Retrieves the input shape(s) of a layer at a given node.</p>
<h4 id="arguments-9">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-15">Returns:</h4>
<pre><code>A shape tuple
(or list of shape tuples if the layer has multiple inputs).</code></pre>
<h3 id="get_losses_for">
<code>get_losses_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_losses_for(inputs)</code></pre></div>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<h4 id="arguments-10">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses were created. If you pass <code>inputs=None</code>, unconditional losses are returned, such as weight regularization losses.</li>
</ul>
<h4 id="returns-16">Returns:</h4>
<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_output_at">
<code>get_output_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_output_at(node_index)</code></pre></div>
<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<h4 id="arguments-11">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-17">Returns:</h4>
<pre><code>A tensor (or list of tensors if the layer has multiple outputs).</code></pre>
<h3 id="get_output_mask_at">
<code>get_output_mask_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_output_mask_at(node_index)</code></pre></div>
<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<h4 id="arguments-12">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-18">Returns:</h4>
<pre><code>A mask tensor
(or list of tensors if the layer has multiple outputs).</code></pre>
<h3 id="get_output_shape_at">
<code>get_output_shape_at</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_output_shape_at(node_index)</code></pre></div>
<p>Retrieves the output shape(s) of a layer at a given node.</p>
<h4 id="arguments-13">Arguments:</h4>
<pre><code>node_index: Integer, index of the node
    from which to retrieve the attribute.
    E.g. `node_index=0` will correspond to the
    first time the layer was called.</code></pre>
<h4 id="returns-19">Returns:</h4>
<pre><code>A shape tuple
(or list of shape tuples if the layer has multiple outputs).</code></pre>
<h3 id="get_updates_for">
<code>get_updates_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates_for(inputs)</code></pre></div>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<h4 id="arguments-14">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates were created. If you pass <code>inputs=None</code>, unconditional updates are returned.</li>
</ul>
<h4 id="returns-20">Returns:</h4>
<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_weights">
<code>get_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_weights()</code></pre></div>
<p>Returns the current weights of the layer.</p>
<h4 id="returns-21">Returns:</h4>
<pre><code>Weights values as a list of numpy arrays.</code></pre>
<h3 id="preprocess_input">
<code>preprocess_input</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">preprocess_input(
    inputs,
    training<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="reset_states">
<code>reset_states</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">reset_states(states_value<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<h3 id="set_weights">
<code>set_weights</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_weights(weights)</code></pre></div>
<p>Sets the weights of the layer, from Numpy arrays.</p>
<h4 id="arguments-15">Arguments:</h4>
<pre><code>weights: a list of Numpy arrays. The number
    of arrays and their shape must match
    number of the dimensions of the weights
    of the layer (i.e. it should match the
    output of `get_weights`).</code></pre>
<h4 id="raises-7">Raises:</h4>
<pre><code>ValueError: If the provided weights list does not match the
    layer&#39;s specifications.</code></pre>
<h3 id="step">
<code>step</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">step(
    inputs,
    states
)</code></pre></div>
