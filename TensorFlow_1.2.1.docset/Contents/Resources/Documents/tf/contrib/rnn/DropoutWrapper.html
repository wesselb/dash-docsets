<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.rnn.DropoutWrapper" /> <meta itemprop="property" content="graph"/> <meta itemprop="property" content="losses"/> <meta itemprop="property" content="non_trainable_variables"/> <meta itemprop="property" content="non_trainable_weights"/> <meta itemprop="property" content="output_size"/> <meta itemprop="property" content="scope_name"/> <meta itemprop="property" content="state_size"/> <meta itemprop="property" content="trainable_variables"/> <meta itemprop="property" content="trainable_weights"/> <meta itemprop="property" content="updates"/> <meta itemprop="property" content="variables"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__deepcopy__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_loss"/> <meta itemprop="property" content="add_update"/> <meta itemprop="property" content="add_variable"/> <meta itemprop="property" content="apply"/> <meta itemprop="property" content="build"/> <meta itemprop="property" content="call"/> <meta itemprop="property" content="get_losses_for"/> <meta itemprop="property" content="get_updates_for"/> <meta itemprop="property" content="zero_state"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.rnn.DropoutWrapper" class="dashAnchor"></a><h1 id="tf.contrib.rnn.dropoutwrapper">tf.contrib.rnn.DropoutWrapper</h1>
<h3 id="class-tf.contrib.rnn.dropoutwrapper"><code>class tf.contrib.rnn.DropoutWrapper</code></h3>
<a name="//apple_ref/cpp/Class/tf.nn.rnn_cell.DropoutWrapper" class="dashAnchor"></a><h3 id="class-tf.nn.rnn_cell.dropoutwrapper"><code>class tf.nn.rnn_cell.DropoutWrapper</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/rnn_cell_impl.py"><code>tensorflow/python/ops/rnn_cell_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../../../api_guides/python/contrib.rnn.md#Core_RNN_Cell_wrappers_RNNCells_that_wrap_other_RNNCells_">RNN and Cells (contrib) &gt; Core RNN Cell wrappers (RNNCells that wrap other RNNCells)</a></p>
<p>Operator adding dropout to inputs and outputs of the given cell.</p>
<h2 id="properties">Properties</h2>
<h3 id="graph">
<code>graph</code>
</h3>
<h3 id="losses">
<code>losses</code>
</h3>
<h3 id="non_trainable_variables">
<code>non_trainable_variables</code>
</h3>
<h3 id="non_trainable_weights">
<code>non_trainable_weights</code>
</h3>
<h3 id="output_size">
<code>output_size</code>
</h3>
<h3 id="scope_name">
<code>scope_name</code>
</h3>
<h3 id="state_size">
<code>state_size</code>
</h3>
<h3 id="trainable_variables">
<code>trainable_variables</code>
</h3>
<h3 id="trainable_weights">
<code>trainable_weights</code>
</h3>
<h3 id="updates">
<code>updates</code>
</h3>
<h3 id="variables">
<code>variables</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns">Returns:</h4>
<p>A list of variables.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-1">Returns:</h4>
<p>A list of variables.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    cell,
    input_keep_prob<span class="op">=</span><span class="fl">1.0</span>,
    output_keep_prob<span class="op">=</span><span class="fl">1.0</span>,
    state_keep_prob<span class="op">=</span><span class="fl">1.0</span>,
    variational_recurrent<span class="op">=</span><span class="va">False</span>,
    input_size<span class="op">=</span><span class="va">None</span>,
    dtype<span class="op">=</span><span class="va">None</span>,
    seed<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Create a cell with added input, state, and/or output dropout.</p>
<p>If <code>variational_recurrent</code> is set to <code>True</code> (<strong>NOT</strong> the default behavior), then the the same dropout mask is applied at every step, as described in:</p>
<p>Y. Gal, Z Ghahramani. &quot;A Theoretically Grounded Application of Dropout in Recurrent Neural Networks&quot;. https://arxiv.org/abs/1512.05287</p>
<p>Otherwise a different dropout mask is applied at every time step.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>cell</code></b>: an RNNCell, a projection to output_size is added to it.</li>
<li><b><code>input_keep_prob</code></b>: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added.</li>
<li><b><code>output_keep_prob</code></b>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added.</li>
<li><b><code>state_keep_prob</code></b>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the <em>output</em> states of the cell.</li>
<li><b><code>variational_recurrent</code></b>: Python bool. If <code>True</code>, then the same dropout pattern is applied across all time steps per run call. If this parameter is set, <code>input_size</code> <strong>must</strong> be provided.</li>
<li><b><code>input_size</code></b>: (optional) (possibly nested tuple of) <code>TensorShape</code> objects containing the depth(s) of the input tensors expected to be passed in to the <code>DropoutWrapper</code>. Required and used <strong>iff</strong> <code>variational_recurrent = True</code> and <code>input_keep_prob &lt; 1</code>.</li>
<li><b><code>dtype</code></b>: (optional) The <code>dtype</code> of the input, state, and output tensors. Required and used <strong>iff</strong> <code>variational_recurrent = True</code>.</li>
<li><b><code>seed</code></b>: (optional) integer, the randomness seed.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if cell is not an RNNCell.</li>
<li><b><code>ValueError</code></b>: if any of the keep_probs are not between 0 and 1.</li>
</ul>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    inputs,
    state,
    scope<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Run the cell with the declared dropouts.</p>
<h3 id="__deepcopy__">
<code><strong>deepcopy</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__deepcopy__(memo)</code></pre></div>
<h3 id="add_loss">
<code>add_loss</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_loss(
    losses,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a specific set of inputs.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>losses</code></b>: Loss tensor, or list/tuple of tensors.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the loss(es) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses are created. If <code>None</code> is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses).</li>
</ul>
<h3 id="add_update">
<code>add_update</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_update(
    updates,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a specific set of inputs.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>updates</code></b>: Update op, or list/tuple of update ops.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the update(s) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates are created. If <code>None</code> is passed, the updates are assumed to be unconditional, and will apply across all dataflows of the layer.</li>
</ul>
<h3 id="add_variable">
<code>add_variable</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_variable(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<h4 id="arguments-2">Arguments:</h4>
<ul>
<li><b><code>name</code></b>: variable name.</li>
<li><b><code>shape</code></b>: variable shape.</li>
<li><b><code>dtype</code></b>: The type of the variable. Defaults to <code>self.dtype</code>.</li>
<li><b><code>initializer</code></b>: initializer instance (callable).</li>
<li><b><code>regularizer</code></b>: regularizer instance (callable).</li>
<li><b><code>trainable</code></b>: whether the variable should be part of the layer's &quot;trainable_variables&quot; (e.g. variables, biases) or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>The created variable.</p>
<h3 id="apply">
<code>apply</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">apply</span>(
    inputs,
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Apply the layer on a input.</p>
<p>This simply wraps <code>self.__call__</code>.</p>
<h4 id="arguments-3">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor(s). *args: additional positional arguments to be passed to <code>self.call</code>. **kwargs: additional keyword arguments to be passed to <code>self.call</code>.</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="build">
<code>build</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">build(_)</code></pre></div>
<h3 id="call">
<code>call</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">call(
    inputs,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>The logic of the layer lives here.</p>
<h4 id="arguments-4">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: input tensor(s). **kwargs: additional keyword arguments.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="get_losses_for">
<code>get_losses_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_losses_for(inputs)</code></pre></div>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<h4 id="arguments-5">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses were created. If you pass <code>inputs=None</code>, unconditional losses are returned, such as weight regularization losses.</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_updates_for">
<code>get_updates_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates_for(inputs)</code></pre></div>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<h4 id="arguments-6">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates were created. If you pass <code>inputs=None</code>, unconditional updates are returned.</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<h3 id="zero_state">
<code>zero_state</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">zero_state(
    batch_size,
    dtype
)</code></pre></div>
