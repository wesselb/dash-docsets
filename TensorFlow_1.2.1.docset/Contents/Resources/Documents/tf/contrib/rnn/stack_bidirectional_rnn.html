<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.rnn.stack_bidirectional_rnn" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.contrib.rnn.stack_bidirectional_rnn" class="dashAnchor"></a><h1 id="tf.contrib.rnn.stack_bidirectional_rnn">tf.contrib.rnn.stack_bidirectional_rnn</h1>
<h3 id="tf.contrib.rnn.stack_bidirectional_rnn-1"><code>tf.contrib.rnn.stack_bidirectional_rnn</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">stack_bidirectional_rnn(
    cells_fw,
    cells_bw,
    inputs,
    initial_states_fw<span class="op">=</span><span class="va">None</span>,
    initial_states_bw<span class="op">=</span><span class="va">None</span>,
    dtype<span class="op">=</span><span class="va">None</span>,
    sequence_length<span class="op">=</span><span class="va">None</span>,
    scope<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/rnn/python/ops/rnn.py"><code>tensorflow/contrib/rnn/python/ops/rnn.py</code></a>.</p>
<p>Creates a bidirectional recurrent neural network.</p>
<p>Stacks several bidirectional rnn layers. The combined forward and backward layer outputs are used as input of the next layer. tf.bidirectional_rnn does not allow to share forward and backward information between layers. The input_size of the first forward and backward cells must match. The initial state for both directions is zero and no intermediate states are returned.</p>
<p>As described in https://arxiv.org/abs/1303.5778</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>cells_fw</code></b>: List of instances of RNNCell, one per layer, to be used for forward direction.</li>
<li><b><code>cells_bw</code></b>: List of instances of RNNCell, one per layer, to be used for backward direction.</li>
<li><b><code>inputs</code></b>: A length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements.</li>
<li><b><code>initial_states_fw</code></b>: (optional) A list of the initial states (one per layer) for the forward RNN. Each tensor must has an appropriate type and shape <code>[batch_size, cell_fw.state_size]</code>.</li>
<li><b><code>initial_states_bw</code></b>: (optional) Same as for <code>initial_states_fw</code>, but using the corresponding properties of <code>cells_bw</code>.</li>
<li><b><code>dtype</code></b>: (optional) The data type for the initial state. Required if either of the initial states are not provided.</li>
<li><b><code>sequence_length</code></b>: (optional) An int32/int64 vector, size <code>[batch_size]</code>, containing the actual lengths for each of the sequences.</li>
<li><b><code>scope</code></b>: VariableScope for the created subgraph; defaults to None.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tuple (outputs, output_state_fw, output_state_bw) where: outputs is a length <code>T</code> list of outputs (one for each input), which are depth-concatenated forward and backward outputs. output_states_fw is the final states, one tensor per layer, of the forward rnn. output_states_bw is the final states, one tensor per layer, of the backward rnn.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>cell_fw</code> or <code>cell_bw</code> is not an instance of <code>RNNCell</code>.</li>
<li><b><code>ValueError</code></b>: If inputs is None, not a list or an empty list.</li>
</ul>
