<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.rnn.RNNCell" /> <meta itemprop="property" content="graph"/> <meta itemprop="property" content="losses"/> <meta itemprop="property" content="non_trainable_variables"/> <meta itemprop="property" content="non_trainable_weights"/> <meta itemprop="property" content="output_size"/> <meta itemprop="property" content="scope_name"/> <meta itemprop="property" content="state_size"/> <meta itemprop="property" content="trainable_variables"/> <meta itemprop="property" content="trainable_weights"/> <meta itemprop="property" content="updates"/> <meta itemprop="property" content="variables"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__deepcopy__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_loss"/> <meta itemprop="property" content="add_update"/> <meta itemprop="property" content="add_variable"/> <meta itemprop="property" content="apply"/> <meta itemprop="property" content="build"/> <meta itemprop="property" content="call"/> <meta itemprop="property" content="get_losses_for"/> <meta itemprop="property" content="get_updates_for"/> <meta itemprop="property" content="zero_state"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.rnn.RNNCell" class="dashAnchor"></a><h1 id="tf.contrib.rnn.rnncell">tf.contrib.rnn.RNNCell</h1>
<h3 id="class-tf.contrib.rnn.rnncell"><code>class tf.contrib.rnn.RNNCell</code></h3>
<a name="//apple_ref/cpp/Class/tf.nn.rnn_cell.RNNCell" class="dashAnchor"></a><h3 id="class-tf.nn.rnn_cell.rnncell"><code>class tf.nn.rnn_cell.RNNCell</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/rnn_cell_impl.py"><code>tensorflow/python/ops/rnn_cell_impl.py</code></a>.</p>
<p>See the guides: <a href="../../../../../api_guides/python/contrib.rnn.md#Base_interface_for_all_RNN_Cells">RNN and Cells (contrib) &gt; Base interface for all RNN Cells</a>, <a href="../../../../../api_guides/python/contrib.seq2seq.html">Seq2seq Library (contrib)</a></p>
<p>Abstract object representing an RNN cell.</p>
<p>Every <code>RNNCell</code> must have the properties below and implement <code>call</code> with the signature <code>(output, next_state) = call(input, state)</code>. The optional third input argument, <code>scope</code>, is allowed for backwards compatibility purposes; but should be left off for new subclasses.</p>
<p>This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units.</p>
<p>An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs. This operation results in an output matrix with <code>self.output_size</code> columns. If <code>self.state_size</code> is an integer, this operation also results in a new state matrix with <code>self.state_size</code> columns. If <code>self.state_size</code> is a (possibly nested tuple of) TensorShape object(s), then it should return a matching structure of Tensors having shape <code>[batch_size].concatenate(s)</code> for each <code>s</code> in <code>self.batch_size</code>.</p>
<h2 id="properties">Properties</h2>
<h3 id="graph">
<code>graph</code>
</h3>
<h3 id="losses">
<code>losses</code>
</h3>
<h3 id="non_trainable_variables">
<code>non_trainable_variables</code>
</h3>
<h3 id="non_trainable_weights">
<code>non_trainable_weights</code>
</h3>
<h3 id="output_size">
<code>output_size</code>
</h3>
<p>Integer or TensorShape: size of outputs produced by this cell.</p>
<h3 id="scope_name">
<code>scope_name</code>
</h3>
<h3 id="state_size">
<code>state_size</code>
</h3>
<p>size(s) of state(s) used by this cell.</p>
<p>It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.</p>
<h3 id="trainable_variables">
<code>trainable_variables</code>
</h3>
<h3 id="trainable_weights">
<code>trainable_weights</code>
</h3>
<h3 id="updates">
<code>updates</code>
</h3>
<h3 id="variables">
<code>variables</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns">Returns:</h4>
<p>A list of variables.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-1">Returns:</h4>
<p>A list of variables.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    trainable<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>,
    dtype<span class="op">=</span>tf.float32,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    inputs,
    state,
    scope<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Run this RNN cell on inputs, starting from the given state.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>inputs</code></b>: <code>2-D</code> tensor with shape <code>[batch_size x input_size]</code>.</li>
<li><b><code>state</code></b>: if <code>self.state_size</code> is an integer, this should be a <code>2-D Tensor</code> with shape <code>[batch_size x self.state_size]</code>. Otherwise, if <code>self.state_size</code> is a tuple of integers, this should be a tuple with shapes <code>[batch_size x s] for s in self.state_size</code>.</li>
<li><b><code>scope</code></b>: VariableScope for the created subgraph; defaults to class name.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>A pair containing:</p>
<ul>
<li>Output: A <code>2-D</code> tensor with shape <code>[batch_size x self.output_size]</code>.</li>
<li>New state: Either a single <code>2-D</code> tensor, or a tuple of tensors matching the arity and shapes of <code>state</code>.</li>
</ul>
<h3 id="__deepcopy__">
<code><strong>deepcopy</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__deepcopy__(memo)</code></pre></div>
<h3 id="add_loss">
<code>add_loss</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_loss(
    losses,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a specific set of inputs.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>losses</code></b>: Loss tensor, or list/tuple of tensors.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the loss(es) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses are created. If <code>None</code> is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses).</li>
</ul>
<h3 id="add_update">
<code>add_update</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_update(
    updates,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a specific set of inputs.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>updates</code></b>: Update op, or list/tuple of update ops.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the update(s) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates are created. If <code>None</code> is passed, the updates are assumed to be unconditional, and will apply across all dataflows of the layer.</li>
</ul>
<h3 id="add_variable">
<code>add_variable</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_variable(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<h4 id="arguments-2">Arguments:</h4>
<ul>
<li><b><code>name</code></b>: variable name.</li>
<li><b><code>shape</code></b>: variable shape.</li>
<li><b><code>dtype</code></b>: The type of the variable. Defaults to <code>self.dtype</code>.</li>
<li><b><code>initializer</code></b>: initializer instance (callable).</li>
<li><b><code>regularizer</code></b>: regularizer instance (callable).</li>
<li><b><code>trainable</code></b>: whether the variable should be part of the layer's &quot;trainable_variables&quot; (e.g. variables, biases) or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>The created variable.</p>
<h3 id="apply">
<code>apply</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">apply</span>(
    inputs,
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Apply the layer on a input.</p>
<p>This simply wraps <code>self.__call__</code>.</p>
<h4 id="arguments-3">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor(s). *args: additional positional arguments to be passed to <code>self.call</code>. **kwargs: additional keyword arguments to be passed to <code>self.call</code>.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="build">
<code>build</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">build(_)</code></pre></div>
<h3 id="call">
<code>call</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">call(
    inputs,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>The logic of the layer lives here.</p>
<h4 id="arguments-4">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: input tensor(s). **kwargs: additional keyword arguments.</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="get_losses_for">
<code>get_losses_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_losses_for(inputs)</code></pre></div>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<h4 id="arguments-5">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses were created. If you pass <code>inputs=None</code>, unconditional losses are returned, such as weight regularization losses.</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_updates_for">
<code>get_updates_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates_for(inputs)</code></pre></div>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<h4 id="arguments-6">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates were created. If you pass <code>inputs=None</code>, unconditional updates are returned.</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<h3 id="zero_state">
<code>zero_state</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">zero_state(
    batch_size,
    dtype
)</code></pre></div>
<p>Return zero-filled state tensor(s).</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>batch_size</code></b>: int, float, or unit Tensor representing the batch size.</li>
<li><b><code>dtype</code></b>: the data type to use for the state.</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
