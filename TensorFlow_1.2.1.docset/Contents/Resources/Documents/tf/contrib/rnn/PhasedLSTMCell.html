<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.rnn.PhasedLSTMCell" /> <meta itemprop="property" content="graph"/> <meta itemprop="property" content="losses"/> <meta itemprop="property" content="non_trainable_variables"/> <meta itemprop="property" content="non_trainable_weights"/> <meta itemprop="property" content="output_size"/> <meta itemprop="property" content="scope_name"/> <meta itemprop="property" content="state_size"/> <meta itemprop="property" content="trainable_variables"/> <meta itemprop="property" content="trainable_weights"/> <meta itemprop="property" content="updates"/> <meta itemprop="property" content="variables"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__deepcopy__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_loss"/> <meta itemprop="property" content="add_update"/> <meta itemprop="property" content="add_variable"/> <meta itemprop="property" content="apply"/> <meta itemprop="property" content="build"/> <meta itemprop="property" content="call"/> <meta itemprop="property" content="get_losses_for"/> <meta itemprop="property" content="get_updates_for"/> <meta itemprop="property" content="zero_state"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.rnn.PhasedLSTMCell" class="dashAnchor"></a><h1 id="tf.contrib.rnn.phasedlstmcell">tf.contrib.rnn.PhasedLSTMCell</h1>
<h3 id="class-tf.contrib.rnn.phasedlstmcell"><code>class tf.contrib.rnn.PhasedLSTMCell</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/rnn/python/ops/rnn_cell.py"><code>tensorflow/contrib/rnn/python/ops/rnn_cell.py</code></a>.</p>
<p>Phased LSTM recurrent network cell.</p>
<p>https://arxiv.org/pdf/1610.09513v1.pdf</p>
<h2 id="properties">Properties</h2>
<h3 id="graph">
<code>graph</code>
</h3>
<h3 id="losses">
<code>losses</code>
</h3>
<h3 id="non_trainable_variables">
<code>non_trainable_variables</code>
</h3>
<h3 id="non_trainable_weights">
<code>non_trainable_weights</code>
</h3>
<h3 id="output_size">
<code>output_size</code>
</h3>
<h3 id="scope_name">
<code>scope_name</code>
</h3>
<h3 id="state_size">
<code>state_size</code>
</h3>
<h3 id="trainable_variables">
<code>trainable_variables</code>
</h3>
<h3 id="trainable_weights">
<code>trainable_weights</code>
</h3>
<h3 id="updates">
<code>updates</code>
</h3>
<h3 id="variables">
<code>variables</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns">Returns:</h4>
<p>A list of variables.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-1">Returns:</h4>
<p>A list of variables.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    num_units,
    use_peepholes<span class="op">=</span><span class="va">False</span>,
    leak<span class="op">=</span><span class="fl">0.001</span>,
    ratio_on<span class="op">=</span><span class="fl">0.1</span>,
    trainable_ratio_on<span class="op">=</span><span class="va">True</span>,
    period_init_min<span class="op">=</span><span class="fl">1.0</span>,
    period_init_max<span class="op">=</span><span class="fl">1000.0</span>,
    reuse<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Initialize the Phased LSTM cell.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>num_units</code></b>: int, The number of units in the Phased LSTM cell.</li>
<li><b><code>use_peepholes</code></b>: bool, set True to enable peephole connections.</li>
<li><b><code>leak</code></b>: float or scalar float Tensor with value in [0, 1]. Leak applied during training.</li>
<li><b><code>ratio_on</code></b>: float or scalar float Tensor with value in [0, 1]. Ratio of the period during which the gates are open.</li>
<li><b><code>trainable_ratio_on</code></b>: bool, weather ratio_on is trainable.</li>
<li><b><code>period_init_min</code></b>: float or scalar float Tensor. With value &gt; 0. Minimum value of the initalized period. The period values are initialized by drawing from the distribution: e^U(log(period_init_min), log(period_init_max)) Where U(.,.) is the uniform distribution.</li>
<li><b><code>period_init_max</code></b>: float or scalar float Tensor. With value &gt; period_init_min. Maximum value of the initalized period.</li>
<li><b><code>reuse</code></b>: (optional) Python boolean describing whether to reuse variables in an existing scope. If not <code>True</code>, and the existing scope already has the given variables, an error is raised.</li>
</ul>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    inputs,
    state,
    scope<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Run this RNN cell on inputs, starting from the given state.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>inputs</code></b>: <code>2-D</code> tensor with shape <code>[batch_size x input_size]</code>.</li>
<li><b><code>state</code></b>: if <code>self.state_size</code> is an integer, this should be a <code>2-D Tensor</code> with shape <code>[batch_size x self.state_size]</code>. Otherwise, if <code>self.state_size</code> is a tuple of integers, this should be a tuple with shapes <code>[batch_size x s] for s in self.state_size</code>.</li>
<li><b><code>scope</code></b>: VariableScope for the created subgraph; defaults to class name.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>A pair containing:</p>
<ul>
<li>Output: A <code>2-D</code> tensor with shape <code>[batch_size x self.output_size]</code>.</li>
<li>New state: Either a single <code>2-D</code> tensor, or a tuple of tensors matching the arity and shapes of <code>state</code>.</li>
</ul>
<h3 id="__deepcopy__">
<code><strong>deepcopy</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__deepcopy__(memo)</code></pre></div>
<h3 id="add_loss">
<code>add_loss</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_loss(
    losses,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a specific set of inputs.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>losses</code></b>: Loss tensor, or list/tuple of tensors.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the loss(es) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses are created. If <code>None</code> is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses).</li>
</ul>
<h3 id="add_update">
<code>add_update</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_update(
    updates,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a specific set of inputs.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>updates</code></b>: Update op, or list/tuple of update ops.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the update(s) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates are created. If <code>None</code> is passed, the updates are assumed to be unconditional, and will apply across all dataflows of the layer.</li>
</ul>
<h3 id="add_variable">
<code>add_variable</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_variable(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<h4 id="arguments-2">Arguments:</h4>
<ul>
<li><b><code>name</code></b>: variable name.</li>
<li><b><code>shape</code></b>: variable shape.</li>
<li><b><code>dtype</code></b>: The type of the variable. Defaults to <code>self.dtype</code>.</li>
<li><b><code>initializer</code></b>: initializer instance (callable).</li>
<li><b><code>regularizer</code></b>: regularizer instance (callable).</li>
<li><b><code>trainable</code></b>: whether the variable should be part of the layer's &quot;trainable_variables&quot; (e.g. variables, biases) or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>The created variable.</p>
<h3 id="apply">
<code>apply</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">apply</span>(
    inputs,
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Apply the layer on a input.</p>
<p>This simply wraps <code>self.__call__</code>.</p>
<h4 id="arguments-3">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor(s). *args: additional positional arguments to be passed to <code>self.call</code>. **kwargs: additional keyword arguments to be passed to <code>self.call</code>.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="build">
<code>build</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">build(_)</code></pre></div>
<h3 id="call">
<code>call</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">call(
    inputs,
    state
)</code></pre></div>
<p>Phased LSTM Cell.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>inputs</code></b>: A tuple of 2 Tensor. The first Tensor has shape [batch, 1], and type float32 or float64. It stores the time. The second Tensor has shape [batch, features_size], and type float32. It stores the features.</li>
<li><b><code>state</code></b>: rnn_cell_impl.LSTMStateTuple, state from previous timestep.</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>A tuple containing: - A Tensor of float32, and shape [batch_size, num_units], representing the output of the cell. - A rnn_cell_impl.LSTMStateTuple, containing 2 Tensors of float32, shape [batch_size, num_units], representing the new state and the output.</p>
<h3 id="get_losses_for">
<code>get_losses_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_losses_for(inputs)</code></pre></div>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<h4 id="arguments-4">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses were created. If you pass <code>inputs=None</code>, unconditional losses are returned, such as weight regularization losses.</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_updates_for">
<code>get_updates_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates_for(inputs)</code></pre></div>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<h4 id="arguments-5">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates were created. If you pass <code>inputs=None</code>, unconditional updates are returned.</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<h3 id="zero_state">
<code>zero_state</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">zero_state(
    batch_size,
    dtype
)</code></pre></div>
<p>Return zero-filled state tensor(s).</p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>batch_size</code></b>: int, float, or unit Tensor representing the batch size.</li>
<li><b><code>dtype</code></b>: the data type to use for the state.</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code>2-D</code> tensors with the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
