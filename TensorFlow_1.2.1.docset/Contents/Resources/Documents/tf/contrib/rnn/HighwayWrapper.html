<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.rnn.HighwayWrapper" /> <meta itemprop="property" content="graph"/> <meta itemprop="property" content="losses"/> <meta itemprop="property" content="non_trainable_variables"/> <meta itemprop="property" content="non_trainable_weights"/> <meta itemprop="property" content="output_size"/> <meta itemprop="property" content="scope_name"/> <meta itemprop="property" content="state_size"/> <meta itemprop="property" content="trainable_variables"/> <meta itemprop="property" content="trainable_weights"/> <meta itemprop="property" content="updates"/> <meta itemprop="property" content="variables"/> <meta itemprop="property" content="weights"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__deepcopy__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="add_loss"/> <meta itemprop="property" content="add_update"/> <meta itemprop="property" content="add_variable"/> <meta itemprop="property" content="apply"/> <meta itemprop="property" content="build"/> <meta itemprop="property" content="call"/> <meta itemprop="property" content="get_losses_for"/> <meta itemprop="property" content="get_updates_for"/> <meta itemprop="property" content="zero_state"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.rnn.HighwayWrapper" class="dashAnchor"></a><h1 id="tf.contrib.rnn.highwaywrapper">tf.contrib.rnn.HighwayWrapper</h1>
<h3 id="class-tf.contrib.rnn.highwaywrapper"><code>class tf.contrib.rnn.HighwayWrapper</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/rnn/python/ops/rnn_cell.py"><code>tensorflow/contrib/rnn/python/ops/rnn_cell.py</code></a>.</p>
<p>RNNCell wrapper that adds highway connection on cell input and output.</p>
<p>Based on: R. K. Srivastava, K. Greff, and J. Schmidhuber, &quot;Highway networks&quot;, arXiv preprint arXiv:1505.00387, 2015. https://arxiv.org/abs/1505.00387</p>
<h2 id="properties">Properties</h2>
<h3 id="graph">
<code>graph</code>
</h3>
<h3 id="losses">
<code>losses</code>
</h3>
<h3 id="non_trainable_variables">
<code>non_trainable_variables</code>
</h3>
<h3 id="non_trainable_weights">
<code>non_trainable_weights</code>
</h3>
<h3 id="output_size">
<code>output_size</code>
</h3>
<h3 id="scope_name">
<code>scope_name</code>
</h3>
<h3 id="state_size">
<code>state_size</code>
</h3>
<h3 id="trainable_variables">
<code>trainable_variables</code>
</h3>
<h3 id="trainable_weights">
<code>trainable_weights</code>
</h3>
<h3 id="updates">
<code>updates</code>
</h3>
<h3 id="variables">
<code>variables</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns">Returns:</h4>
<p>A list of variables.</p>
<h3 id="weights">
<code>weights</code>
</h3>
<p>Returns the list of all layer variables/weights.</p>
<h4 id="returns-1">Returns:</h4>
<p>A list of variables.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    cell,
    couple_carry_transform_gates<span class="op">=</span><span class="va">True</span>,
    carry_bias_init<span class="op">=</span><span class="fl">1.0</span>
)</code></pre></div>
<p>Constructs a <code>HighwayWrapper</code> for <code>cell</code>.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>cell</code></b>: An instance of <code>RNNCell</code>.</li>
<li><b><code>couple_carry_transform_gates</code></b>: boolean, should the Carry and Transform gate be coupled.</li>
<li><b><code>carry_bias_init</code></b>: float, carry gates bias initialization.</li>
</ul>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    inputs,
    state,
    scope<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Run the cell and add its inputs to its outputs.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>inputs</code></b>: cell inputs.</li>
<li><b><code>state</code></b>: cell state.</li>
<li><b><code>scope</code></b>: optional cell scope.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>Tuple of cell outputs and new state.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If cell inputs and outputs have different structure (type).</li>
<li><b><code>ValueError</code></b>: If cell inputs and outputs have different structure (value).</li>
</ul>
<h3 id="__deepcopy__">
<code><strong>deepcopy</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">__deepcopy__(memo)</code></pre></div>
<h3 id="add_loss">
<code>add_loss</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_loss(
    losses,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a specific set of inputs.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>losses</code></b>: Loss tensor, or list/tuple of tensors.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the loss(es) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses are created. If <code>None</code> is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses).</li>
</ul>
<h3 id="add_update">
<code>add_update</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_update(
    updates,
    inputs<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing a same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a specific set of inputs.</p>
<h4 id="arguments-1">Arguments:</h4>
<ul>
<li><b><code>updates</code></b>: Update op, or list/tuple of update ops.</li>
<li><b><code>inputs</code></b>: Optional input tensor(s) that the update(s) depend on. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates are created. If <code>None</code> is passed, the updates are assumed to be unconditional, and will apply across all dataflows of the layer.</li>
</ul>
<h3 id="add_variable">
<code>add_variable</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">add_variable(
    name,
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>
)</code></pre></div>
<p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<h4 id="arguments-2">Arguments:</h4>
<ul>
<li><b><code>name</code></b>: variable name.</li>
<li><b><code>shape</code></b>: variable shape.</li>
<li><b><code>dtype</code></b>: The type of the variable. Defaults to <code>self.dtype</code>.</li>
<li><b><code>initializer</code></b>: initializer instance (callable).</li>
<li><b><code>regularizer</code></b>: regularizer instance (callable).</li>
<li><b><code>trainable</code></b>: whether the variable should be part of the layer's &quot;trainable_variables&quot; (e.g. variables, biases) or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>The created variable.</p>
<h3 id="apply">
<code>apply</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">apply</span>(
    inputs,
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>Apply the layer on a input.</p>
<p>This simply wraps <code>self.__call__</code>.</p>
<h4 id="arguments-3">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor(s). *args: additional positional arguments to be passed to <code>self.call</code>. **kwargs: additional keyword arguments to be passed to <code>self.call</code>.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="build">
<code>build</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">build(_)</code></pre></div>
<h3 id="call">
<code>call</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">call(
    inputs,
    <span class="op">**</span>kwargs
)</code></pre></div>
<p>The logic of the layer lives here.</p>
<h4 id="arguments-4">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: input tensor(s). **kwargs: additional keyword arguments.</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p>Output tensor(s).</p>
<h3 id="get_losses_for">
<code>get_losses_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_losses_for(inputs)</code></pre></div>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<h4 id="arguments-5">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the losses were created. If you pass <code>inputs=None</code>, unconditional losses are returned, such as weight regularization losses.</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<p>List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<h3 id="get_updates_for">
<code>get_updates_for</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_updates_for(inputs)</code></pre></div>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<h4 id="arguments-6">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Input tensor or list/tuple of input tensors. Must match the <code>inputs</code> argument passed to the <code>__call__</code> method at the time the updates were created. If you pass <code>inputs=None</code>, unconditional updates are returned.</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<p>List of update ops of the layer that depend on <code>inputs</code>.</p>
<h3 id="zero_state">
<code>zero_state</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">zero_state(
    batch_size,
    dtype
)</code></pre></div>
