<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.training.batch_sequences_with_states" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.contrib.training.batch_sequences_with_states" class="dashAnchor"></a><h1 id="tf.contrib.training.batch_sequences_with_states">tf.contrib.training.batch_sequences_with_states</h1>
<h3 id="tf.contrib.training.batch_sequences_with_states-1"><code>tf.contrib.training.batch_sequences_with_states</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">batch_sequences_with_states(
    input_key,
    input_sequences,
    input_context,
    input_length,
    initial_states,
    num_unroll,
    batch_size,
    num_threads<span class="op">=</span><span class="dv">3</span>,
    capacity<span class="op">=</span><span class="dv">1000</span>,
    allow_small_batch<span class="op">=</span><span class="va">True</span>,
    pad<span class="op">=</span><span class="va">True</span>,
    make_keys_unique<span class="op">=</span><span class="va">False</span>,
    make_keys_unique_seed<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py"><code>tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py</code></a>.</p>
<p>See the guide: <a href="../../../../../api_guides/python/contrib.training.md#Splitting_sequence_inputs_into_minibatches_with_state_saving">Training (contrib) &gt; Splitting sequence inputs into minibatches with state saving</a></p>
<p>Creates batches of segments of sequential input.</p>
<p>This method creates a <code>SequenceQueueingStateSaver</code> (SQSS) and adds it to the queuerunners. It returns a <code>NextQueuedSequenceBatch</code>.</p>
<p>It accepts one example at a time identified by a unique <code>input_key</code>. <code>input_sequence</code> is a dict with values that are tensors with time as first dimension. This time dimension must be the same across those tensors of an example. It can vary across examples. Although it always has to be a multiple of <code>num_unroll</code>. Hence, padding may be necessary and it is turned on by default by <code>pad=True</code>.</p>
<p><code>input_length</code> is a Tensor scalar or an int recording the time dimension prior to padding. It should be between 0 and the time dimension. One reason we want to keep track of it is so that we can take it into consideration when computing the loss. If <code>pad=True</code> then <code>input_length</code> can be <code>None</code> and will be inferred.</p>
<p>This methods segments <code>input_sequence</code> into segments of length <code>num_unroll</code>. It batches input sequences from <code>batch_size</code> many examples. These mini-batches are available through the <code>sequence</code> property of the output. Moreover, for each entry in the batch we can access its original <code>input_key</code> in <code>key</code> and its input length in <code>total_length</code>. <code>length</code> records within this segment how many non-padded time steps there are.</p>
<p>Static features of an example that do not vary across time can be part of the <code>input_context</code>, a dict with Tensor values. This method copies the context for each segment and makes it available in the <code>context</code> of the output.</p>
<p>This method can maintain and update a state for each example. It accepts some initial_states as a dict with Tensor values. The first mini-batch an example is contained has initial_states as entry of the <code>state</code>. If save_state is called then the next segment will have the updated entry of the <code>state</code>. See <code>NextQueuedSequenceBatch</code> for a complete list of properties and methods.</p>
<p>Example usage:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">batch_size <span class="op">=</span> <span class="dv">32</span>
num_unroll <span class="op">=</span> <span class="dv">20</span>
num_enqueue_threads <span class="op">=</span> <span class="dv">3</span>
lstm_size <span class="op">=</span> <span class="dv">8</span>
cell <span class="op">=</span> tf.contrib.rnn.BasicLSTMCell(num_units<span class="op">=</span>lstm_size)

key, sequences, context <span class="op">=</span> my_parser(raw_data)
initial_state_values <span class="op">=</span> tf.zeros((state_size,), dtype<span class="op">=</span>tf.float32)
initial_states <span class="op">=</span> {<span class="st">&quot;lstm_state&quot;</span>: initial_state_values}
batch <span class="op">=</span> tf.batch_sequences_with_states(
    input_key<span class="op">=</span>key,
    input_sequences<span class="op">=</span>sequences,
    input_context<span class="op">=</span>context,
    input_length<span class="op">=</span>tf.shape(sequences[<span class="st">&quot;input&quot;</span>])[<span class="dv">0</span>],
    initial_states<span class="op">=</span>initial_states,
    num_unroll<span class="op">=</span>num_unroll,
    batch_size<span class="op">=</span>batch_size,
    num_threads<span class="op">=</span>num_enqueue_threads,
    capacity<span class="op">=</span>batch_size <span class="op">*</span> num_enqueue_threads <span class="op">*</span> <span class="dv">2</span>)

inputs <span class="op">=</span> batch.sequences[<span class="st">&quot;input&quot;</span>]
context_label <span class="op">=</span> batch.context[<span class="st">&quot;label&quot;</span>]

inputs_by_time <span class="op">=</span> tf.split(value<span class="op">=</span>inputs, num_or_size_splits<span class="op">=</span>num_unroll, axis<span class="op">=</span><span class="dv">1</span>)
<span class="cf">assert</span> <span class="bu">len</span>(inputs_by_time) <span class="op">==</span> num_unroll

lstm_output, _ <span class="op">=</span> tf.contrib.rnn.static_state_saving_rnn(
  cell,
  inputs_by_time,
  state_saver<span class="op">=</span>batch,
  state_name<span class="op">=</span><span class="st">&quot;lstm_state&quot;</span>)

<span class="co"># Start a prefetcher in the background</span>
sess <span class="op">=</span> tf.Session()

tf.train.start_queue_runners(sess<span class="op">=</span>session)

<span class="cf">while</span> <span class="va">True</span>:
  <span class="co"># Step through batches, perform training or inference...</span>
  session.run([lstm_output])</code></pre></div>
<h4 id="args">Args:</h4>
<ul>
<li><p><b><code>input_key</code></b>: A string scalar <code>Tensor</code>, the <strong>unique</strong> key for the given input example. This is used to keep track of the split minibatch elements of this input. Batched keys of the current iteration are made accessible via the <code>key</code> property. The shape of <code>input_key</code> (scalar) must be fully specified. Consider setting <code>make_keys_unique</code> to True when iterating over the same input multiple times.</p>
<strong>Note</strong>: if <code>make_keys_unique=False</code> then <code>input_key</code>s must be unique.</li>
<li><p><b><code>input_sequences</code></b>: A dict mapping string names to <code>Tensor</code> values. The values must all have matching first dimension, called <code>value_length</code>. They may vary from input to input. The remainder of the shape (other than the first dimension) must be fully specified. The <code>SequenceQueueingStateSaver</code> will split these tensors along this first dimension into minibatch elements of dimension <code>num_unrolled</code>. Batched and segmented sequences of the current iteration are made accessible via the <code>sequences</code> property.</p>
<strong>Note</strong>: if <code>pad=False</code>, then <code>value_length</code> must always be a multiple of <code>num_unroll</code>.</li>
<li><p><b><code>input_context</code></b>: A dict mapping string names to <code>Tensor</code> values. The values are treated as &quot;global&quot; across all time splits of the given input example, and will be copied across for all minibatch elements accordingly. Batched and copied context of the current iteration are made accessible via the <code>context</code> property.</p>
<strong>Note</strong>: All input_context values must have fully defined shapes.</li>
<li><b><code>input_length</code></b>: None or an int32 scalar <code>Tensor</code>, the length of the sequence prior to padding. If <code>input_length=None</code> and <code>pad=True</code> then the length will be inferred and will be equal to <code>value_length</code>. If <code>pad=False</code> then <code>input_length</code> cannot be <code>None</code>: <code>input_length</code> must be specified. Its shape of <code>input_length</code> (scalar) must be fully specified. Its value may be at most <code>value_length</code> for any given input (see above for the definition of <code>value_length</code>). Batched and total lengths of the current iteration are made accessible via the <code>length</code> and <code>total_length</code> properties.</li>
<li><p><b><code>initial_states</code></b>: A dict mapping string state names to multi-dimensional values (e.g. constants or tensors). This input defines the set of states that will be kept track of during computing iterations, and which can be accessed via the <code>state</code> and <code>save_state</code> methods.</p>
<strong>Note</strong>: All initial_state values must have fully defined shapes.</li>
<li><b><code>num_unroll</code></b>: Python integer, how many time steps to unroll at a time. The input sequences of length k are then split into k / num_unroll many segments.</li>
<li><b><code>batch_size</code></b>: int or int32 scalar <code>Tensor</code>, how large minibatches should be when accessing the <code>state()</code> method and <code>context</code>, <code>sequences</code>, etc, properties.</li>
<li><b><code>num_threads</code></b>: The int number of threads enqueuing input examples into a queue.</li>
<li><b><code>capacity</code></b>: The max capacity of the queue in number of examples. Needs to be at least <code>batch_size</code>. Defaults to 1000. When iterating over the same input example multiple times reusing their keys the <code>capacity</code> must be smaller than the number of examples.</li>
<li><b><code>allow_small_batch</code></b>: If true, the queue will return smaller batches when there aren't enough input examples to fill a whole batch and the end of the input has been reached.</li>
<li><b><code>pad</code></b>: If <code>True</code>, <code>input_sequences</code> will be padded to multiple of <code>num_unroll</code>. In that case <code>input_length</code> may be <code>None</code> and is assumed to be the length of first dimension of values in <code>input_sequences</code> (i.e. <code>value_length</code>).</li>
<li><b><code>make_keys_unique</code></b>: Whether to append a random integer to the <code>input_key</code> in an effort to make it unique. The seed can be set via <code>make_keys_unique_seed</code>.</li>
<li><b><code>make_keys_unique_seed</code></b>: If <code>make_keys_unique=True</code> this fixes the seed with which a random postfix is generated.</li>
<li><p><b><code>name</code></b>: An op name string (optional).</p></li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A NextQueuedSequenceBatch with segmented and batched inputs and their states.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if any of the inputs is not an expected type.</li>
<li><b><code>ValueError</code></b>: if any of the input values is inconsistent, e.g. if not enough shape information is available from inputs to build the state saver.</li>
</ul>
