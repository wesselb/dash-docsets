<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.opt.VariableClippingOptimizer" /> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="apply_gradients"/> <meta itemprop="property" content="compute_gradients"/> <meta itemprop="property" content="get_name"/> <meta itemprop="property" content="get_slot"/> <meta itemprop="property" content="get_slot_names"/> <meta itemprop="property" content="minimize"/> <meta itemprop="property" content="GATE_GRAPH"/> <meta itemprop="property" content="GATE_NONE"/> <meta itemprop="property" content="GATE_OP"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.opt.VariableClippingOptimizer" class="dashAnchor"></a><h1 id="tf.contrib.opt.variableclippingoptimizer">tf.contrib.opt.VariableClippingOptimizer</h1>
<h3 id="class-tf.contrib.opt.variableclippingoptimizer"><code>class tf.contrib.opt.VariableClippingOptimizer</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/opt/python/training/variable_clipping_optimizer.py"><code>tensorflow/contrib/opt/python/training/variable_clipping_optimizer.py</code></a>.</p>
<p>Wrapper optimizer that clips the norm of specified variables after update.</p>
<p>This optimizer delegates all aspects of gradient calculation and application to an underlying optimizer. After applying gradients, this optimizer then clips the variable to have a maximum L2 norm along specified dimensions. NB: this is quite different from clipping the norm of the gradients.</p>
<p>Multiple instances of <code>VariableClippingOptimizer</code> may be chained to specify different max norms for different subsets of variables.</p>
<p>This is more efficient at serving-time than using normalization during embedding lookup, at the expense of more expensive training and fewer guarantees about the norms.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    opt,
    vars_to_clip_dims,
    max_norm,
    use_locking<span class="op">=</span><span class="va">False</span>,
    colocate_clip_ops_with_vars<span class="op">=</span><span class="va">False</span>,
    name<span class="op">=</span><span class="st">&#39;VariableClipping&#39;</span>
)</code></pre></div>
<p>Construct a new clip-norm optimizer.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>opt</code></b>: The actual optimizer that will be used to compute and apply the gradients. Must be one of the Optimizer classes.</li>
<li><b><code>vars_to_clip_dims</code></b>: A dict with keys as Variables and values as lists of dimensions along which to compute the L2-norm. See <code>tf.clip_by_norm</code> for more details.</li>
<li><b><code>max_norm</code></b>: The L2-norm to clip to, for all variables specified.</li>
<li><b><code>use_locking</code></b>: If <code>True</code> use locks for clip update operations.</li>
<li><b><code>colocate_clip_ops_with_vars</code></b>: If <code>True</code>, try colocating the clip norm ops with the corresponding variable.</li>
<li><b><code>name</code></b>: Optional name prefix for the operations created when applying gradients. Defaults to &quot;VariableClipping&quot;.</li>
</ul>
<h3 id="apply_gradients">
<code>apply_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">apply_gradients(
    grads_and_vars,
    global_step<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="compute_gradients">
<code>compute_gradients</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">compute_gradients(
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="get_name">
<code>get_name</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_name()</code></pre></div>
<h3 id="get_slot">
<code>get_slot</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_slot(
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="get_slot_names">
<code>get_slot_names</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_slot_names(
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h3 id="minimize">
<code>minimize</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">minimize(
    loss,
    global_step<span class="op">=</span><span class="va">None</span>,
    var_list<span class="op">=</span><span class="va">None</span>,
    gate_gradients<span class="op">=</span>GATE_OP,
    aggregation_method<span class="op">=</span><span class="va">None</span>,
    colocate_gradients_with_ops<span class="op">=</span><span class="va">False</span>,
    name<span class="op">=</span><span class="va">None</span>,
    grad_loss<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Add operations to minimize <code>loss</code> by updating <code>var_list</code>.</p>
<p>This method simply combines calls <code>compute_gradients()</code> and <code>apply_gradients()</code>. If you want to process the gradient before applying them call <code>compute_gradients()</code> and <code>apply_gradients()</code> explicitly instead of using this function.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>loss</code></b>: A <code>Tensor</code> containing the value to minimize.</li>
<li><b><code>global_step</code></b>: Optional <code>Variable</code> to increment by one after the variables have been updated.</li>
<li><b><code>var_list</code></b>: Optional list or tuple of <code>Variable</code> objects to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li>
<li><b><code>gate_gradients</code></b>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li>
<li><b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li>
<li><b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.</li>
<li><b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>An Operation that updates the variables in <code>var_list</code>. If <code>global_step</code> was not <code>None</code>, that operation also increments <code>global_step</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If some of the variables are not <code>Variable</code> objects.</li>
</ul>
<h2 id="class-members">Class Members</h2>
<h3 id="GATE_GRAPH">
<code>GATE_GRAPH</code>
</h3>
<h3 id="GATE_NONE">
<code>GATE_NONE</code>
</h3>
<h3 id="GATE_OP">
<code>GATE_OP</code>
</h3>
