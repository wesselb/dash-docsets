<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.seq2seq.BahdanauAttention" /> <meta itemprop="property" content="alignments_size"/> <meta itemprop="property" content="batch_size"/> <meta itemprop="property" content="keys"/> <meta itemprop="property" content="memory_layer"/> <meta itemprop="property" content="query_layer"/> <meta itemprop="property" content="values"/> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="initial_alignments"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.seq2seq.BahdanauAttention" class="dashAnchor"></a><h1 id="tf.contrib.seq2seq.bahdanauattention">tf.contrib.seq2seq.BahdanauAttention</h1>
<h3 id="class-tf.contrib.seq2seq.bahdanauattention"><code>class tf.contrib.seq2seq.BahdanauAttention</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"><code>tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py</code></a>.</p>
<p>See the guide: <a href="../../../../../api_guides/python/contrib.seq2seq.md#Attention">Seq2seq Library (contrib) &gt; Attention</a></p>
<p>Implements Bhadanau-style (additive) attention.</p>
<p>This attention has two forms. The first is Bhandanau attention, as described in:</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot; ICLR 2015. https://arxiv.org/abs/1409.0473</p>
<p>The second is the normalized form. This form is inspired by the weight normalization article:</p>
<p>Tim Salimans, Diederik P. Kingma. &quot;Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.&quot; https://arxiv.org/abs/1602.07868</p>
<p>To enable the second form, construct the object with parameter <code>normalize=True</code>.</p>
<h2 id="properties">Properties</h2>
<h3 id="alignments_size">
<code>alignments_size</code>
</h3>
<h3 id="batch_size">
<code>batch_size</code>
</h3>
<h3 id="keys">
<code>keys</code>
</h3>
<h3 id="memory_layer">
<code>memory_layer</code>
</h3>
<h3 id="query_layer">
<code>query_layer</code>
</h3>
<h3 id="values">
<code>values</code>
</h3>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    num_units,
    memory,
    memory_sequence_length<span class="op">=</span><span class="va">None</span>,
    normalize<span class="op">=</span><span class="va">False</span>,
    probability_fn<span class="op">=</span><span class="va">None</span>,
    score_mask_value<span class="op">=</span><span class="bu">float</span>(<span class="st">&#39;-inf&#39;</span>),
    name<span class="op">=</span><span class="st">&#39;BahdanauAttention&#39;</span>
)</code></pre></div>
<p>Construct the Attention mechanism.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>num_units</code></b>: The depth of the query mechanism.</li>
<li><b><code>memory</code></b>: The memory to query; usually the output of an RNN encoder. This tensor should be shaped <code>[batch_size, max_time, ...]</code>. memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths.</li>
<li><b><code>normalize</code></b>: Python boolean. Whether to normalize the energy term.</li>
<li><b><code>probability_fn</code></b>: (optional) A <code>callable</code>. Converts the score to probabilities. The default is <a href="../../../tf/nn/softmax.html"><code>tf.nn.softmax</code></a>. Other options include <a href="../../../tf/contrib/seq2seq/hardmax.html"><code>tf.contrib.seq2seq.hardmax</code></a> and <a href="../../../tf/contrib/sparsemax/sparsemax.html"><code>tf.contrib.sparsemax.sparsemax</code></a>. Its signature should be: <code>probabilities = probability_fn(score)</code>.</li>
<li><b><code>score_mask_value</code></b>: (optional): The mask value for score before passing into <code>probability_fn</code>. The default is -inf. Only used if <code>memory_sequence_length</code> is not None.</li>
<li><b><code>name</code></b>: Name to use when creating ops.</li>
</ul>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    query,
    previous_alignments
)</code></pre></div>
<p>Score the query based on the keys and values.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>query</code></b>: Tensor of dtype matching <code>self.values</code> and shape <code>[batch_size, query_depth]</code>.</li>
<li><b><code>previous_alignments</code></b>: Tensor of dtype matching <code>self.values</code> and shape <code>[batch_size, alignments_size]</code> (<code>alignments_size</code> is memory's <code>max_time</code>).</li>
</ul>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>alignments</code></b>: Tensor of dtype matching <code>self.values</code> and shape <code>[batch_size, alignments_size]</code> (<code>alignments_size</code> is memory's <code>max_time</code>).</li>
</ul>
<h3 id="initial_alignments">
<code>initial_alignments</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">initial_alignments(
    batch_size,
    dtype
)</code></pre></div>
<p>Creates the initial alignment values for the <code>AttentionWrapper</code> class.</p>
<p>This is important for AttentionMechanisms that use the previous alignment to calculate the alignment at the next time step (e.g. monotonic attention).</p>
<p>The default behavior is to return a tensor of all zeros.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>batch_size</code></b>: <code>int32</code> scalar, the batch_size.</li>
<li><b><code>dtype</code></b>: The <code>dtype</code>.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>A <code>dtype</code> tensor shaped <code>[batch_size, alignments_size]</code> (<code>alignments_size</code> is the values' <code>max_time</code>).</p>
