<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.distributions.bijectors.Softplus" /> <meta itemprop="property" content="dtype"/> <meta itemprop="property" content="event_ndims"/> <meta itemprop="property" content="graph_parents"/> <meta itemprop="property" content="hinge_softness"/> <meta itemprop="property" content="is_constant_jacobian"/> <meta itemprop="property" content="name"/> <meta itemprop="property" content="validate_args"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="forward"/> <meta itemprop="property" content="forward_event_shape"/> <meta itemprop="property" content="forward_event_shape_tensor"/> <meta itemprop="property" content="forward_log_det_jacobian"/> <meta itemprop="property" content="inverse"/> <meta itemprop="property" content="inverse_event_shape"/> <meta itemprop="property" content="inverse_event_shape_tensor"/> <meta itemprop="property" content="inverse_log_det_jacobian"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.contrib.distributions.bijectors.Softplus" class="dashAnchor"></a><h1 id="tf.contrib.distributions.bijectors.softplus">tf.contrib.distributions.bijectors.Softplus</h1>
<h3 id="class-tf.contrib.distributions.bijectors.softplus"><code>class tf.contrib.distributions.bijectors.Softplus</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/distributions/python/ops/bijectors/softplus_impl.py"><code>tensorflow/contrib/distributions/python/ops/bijectors/softplus_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../../../../api_guides/python/contrib.distributions.bijectors.md#Bijectors">Random variable transformations (contrib) &gt; Bijectors</a></p>
<p>Bijector which computes <code>Y = g(X) = Log[1 + exp(X)]</code>.</p>
<p>The softplus <code>Bijector</code> has the following two useful properties:</p>
<ul>
<li>The domain is the positive real numbers</li>
<li><code>softplus(x) approx x</code>, for large <code>x</code>, so it does not overflow as easily as the <code>Exp</code> <code>Bijector</code>.</li>
</ul>
<p>The optional nonzero <code>hinge_softness</code> parameter changes the transition at zero. With <code>hinge_softness = c</code>, the bijector is:</p>
<p><code>f_c(x) := c * g(x / c) = c * Log[1 + exp(x / c)].</code></p>
<p>For large <code>x &gt;&gt; 1</code>, <code>c * Log[1 + exp(x / c)] approx c * Log[exp(x / c)] = x</code>, so the behavior for large <code>x</code> is the same as the standard softplus.</p>
<p>As <code>c &gt; 0</code> approaches 0 from the right, <code>f_c(x)</code> becomes less and less soft, approaching <code>max(0, x)</code>.</p>
<ul>
<li><code>c = 1</code> is the default.</li>
<li><code>c &gt; 0</code> but small means <code>f(x) approx ReLu(x) = max(0, x)</code>.</li>
<li><code>c &lt; 0</code> flips sign and reflects around the <code>y-axis</code>: <code>f_{-c}(x) = -f_c(-x)</code>.</li>
<li><code>c = 0</code> results in a non-bijective transformation and triggers an exception.</li>
</ul>
<p>Example Use:</p>
<p><code>python   # Create the Y=g(X)=softplus(X) transform which works only on Tensors with 1   # batch ndim and 2 event ndims (i.e., vector of matrices).   softplus = Softplus(event_ndims=2)   x = [[[1., 2],         [3, 4]],        [[5, 6],         [7, 8]]]   log(1 + exp(x)) == softplus.forward(x)   log(exp(x) - 1) == softplus.inverse(x)</code></p>
<p>Note: log(.) and exp(.) are applied element-wise but the Jacobian is a reduction over the event space.</p>
<h2 id="properties">Properties</h2>
<h3 id="dtype">
<code>dtype</code>
</h3>
<p>dtype of <code>Tensor</code>s transformable by this distribution.</p>
<h3 id="event_ndims">
<code>event_ndims</code>
</h3>
<p>Returns then number of event dimensions this bijector operates on.</p>
<h3 id="graph_parents">
<code>graph_parents</code>
</h3>
<p>Returns this <code>Bijector</code>'s graph_parents as a Python list.</p>
<h3 id="hinge_softness">
<code>hinge_softness</code>
</h3>
<h3 id="is_constant_jacobian">
<code>is_constant_jacobian</code>
</h3>
<p>Returns true iff the Jacobian is not a function of x.</p>
<p>Note: Jacobian is either constant for both forward and inverse or neither.</p>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>is_constant_jacobian</code></b>: Python <code>bool</code>.</li>
</ul>
<h3 id="name">
<code>name</code>
</h3>
<p>Returns the string name of this <code>Bijector</code>.</p>
<h3 id="validate_args">
<code>validate_args</code>
</h3>
<p>Returns True if Tensor arguments will be validated.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    <span class="op">*</span>args,
    <span class="op">**</span>kwargs
)</code></pre></div>
<h5 id="kwargs"><code>kwargs</code>:</h5>
<ul>
<li><code>hinge_softness</code>: Nonzero floating point <code>Tensor</code>. Controls the softness of what would otherwise be a kink at the origin. Default is 1.0</li>
</ul>
<h3 id="forward">
<code>forward</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">forward(
    x,
    name<span class="op">=</span><span class="st">&#39;forward&#39;</span>
)</code></pre></div>
<p>Returns the forward <code>Bijector</code> evaluation, i.e., X = g(Y).</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p><code>Tensor</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>x.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_forward</code> is not implemented.</li>
</ul>
<h3 id="forward_event_shape">
<code>forward_event_shape</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">forward_event_shape(input_shape)</code></pre></div>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>forward_event_shape_tensor</code>. May be only partially defined.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>input_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>forward</code> function.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<ul>
<li><b><code>forward_event_shape_tensor</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>forward</code>. Possibly unknown.</li>
</ul>
<h3 id="forward_event_shape_tensor">
<code>forward_event_shape_tensor</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">forward_event_shape_tensor(
    input_shape,
    name<span class="op">=</span><span class="st">&#39;forward_event_shape_tensor&#39;</span>
)</code></pre></div>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>input_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>forward</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<ul>
<li><b><code>forward_event_shape_tensor</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>forward</code>.</li>
</ul>
<h3 id="forward_log_det_jacobian">
<code>forward_log_det_jacobian</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">forward_log_det_jacobian(
    x,
    name<span class="op">=</span><span class="st">&#39;forward_log_det_jacobian&#39;</span>
)</code></pre></div>
<p>Returns both the forward_log_det_jacobian.</p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>x</code></b>: <code>Tensor</code>. The input to the &quot;forward&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
</ul>
<h4 id="returns-4">Returns:</h4>
<p><code>Tensor</code>.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if neither <code>_forward_log_det_jacobian</code> nor {<code>_inverse</code>, <code>_inverse_log_det_jacobian</code>} are implemented.</li>
</ul>
<h3 id="inverse">
<code>inverse</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">inverse(
    y,
    name<span class="op">=</span><span class="st">&#39;inverse&#39;</span>
)</code></pre></div>
<p>Returns the inverse <code>Bijector</code> evaluation, i.e., X = g^{-1}(Y).</p>
<h4 id="args-4">Args:</h4>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
</ul>
<h4 id="returns-5">Returns:</h4>
<p><code>Tensor</code>.</p>
<h4 id="raises-2">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_inverse</code> is not implemented.</li>
</ul>
<h3 id="inverse_event_shape">
<code>inverse_event_shape</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">inverse_event_shape(output_shape)</code></pre></div>
<p>Shape of a single sample from a single batch as a <code>TensorShape</code>.</p>
<p>Same meaning as <code>inverse_event_shape_tensor</code>. May be only partially defined.</p>
<h4 id="args-5">Args:</h4>
<ul>
<li><b><code>output_shape</code></b>: <code>TensorShape</code> indicating event-portion shape passed into <code>inverse</code> function.</li>
</ul>
<h4 id="returns-6">Returns:</h4>
<ul>
<li><b><code>inverse_event_shape_tensor</code></b>: <code>TensorShape</code> indicating event-portion shape after applying <code>inverse</code>. Possibly unknown.</li>
</ul>
<h3 id="inverse_event_shape_tensor">
<code>inverse_event_shape_tensor</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">inverse_event_shape_tensor(
    output_shape,
    name<span class="op">=</span><span class="st">&#39;inverse_event_shape_tensor&#39;</span>
)</code></pre></div>
<p>Shape of a single sample from a single batch as an <code>int32</code> 1D <code>Tensor</code>.</p>
<h4 id="args-6">Args:</h4>
<ul>
<li><b><code>output_shape</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape passed into <code>inverse</code> function.</li>
<li><b><code>name</code></b>: name to give to the op</li>
</ul>
<h4 id="returns-7">Returns:</h4>
<ul>
<li><b><code>inverse_event_shape_tensor</code></b>: <code>Tensor</code>, <code>int32</code> vector indicating event-portion shape after applying <code>inverse</code>.</li>
</ul>
<h3 id="inverse_log_det_jacobian">
<code>inverse_log_det_jacobian</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">inverse_log_det_jacobian(
    y,
    name<span class="op">=</span><span class="st">&#39;inverse_log_det_jacobian&#39;</span>
)</code></pre></div>
<p>Returns the (log o det o Jacobian o inverse)(y).</p>
<p>Mathematically, returns: <code>log(det(dX/dY))(Y)</code>. (Recall that: <code>X=g^{-1}(Y)</code>.)</p>
<p>Note that <code>forward_log_det_jacobian</code> is the negative of this function.</p>
<h4 id="args-7">Args:</h4>
<ul>
<li><b><code>y</code></b>: <code>Tensor</code>. The input to the &quot;inverse&quot; Jacobian evaluation.</li>
<li><b><code>name</code></b>: The name to give this op.</li>
</ul>
<h4 id="returns-8">Returns:</h4>
<p><code>Tensor</code>.</p>
<h4 id="raises-3">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>self.dtype</code> is specified and <code>y.dtype</code> is not <code>self.dtype</code>.</li>
<li><b><code>NotImplementedError</code></b>: if <code>_inverse_log_det_jacobian</code> is not implemented.</li>
</ul>
