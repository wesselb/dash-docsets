<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.contrib.layers.feature_column" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.contrib.layers.feature_column" class="dashAnchor"></a><h1 id="module-tf.contrib.layers.feature_column">Module: tf.contrib.layers.feature_column</h1>
<h3 id="module-tf.contrib.layers.feature_column-1">Module <code>tf.contrib.layers.feature_column</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/feature_column.py"><code>tensorflow/contrib/layers/python/layers/feature_column.py</code></a>.</p>
<p>This API defines FeatureColumn abstraction.</p>
<p>FeatureColumns provide a high level abstraction for ingesting and representing features in tf.learn Estimator models.</p>
<p>FeatureColumns are the primary way of encoding features for pre-canned tf.learn Estimators.</p>
<p>When using FeatureColumns with tf.learn models, the type of feature column you should choose depends on (1) the feature type and (2) the model type.</p>
<ol style="list-style-type: decimal">
<li>Feature type:</li>
</ol>
<ul>
<li>Continuous features can be represented by <code>real_valued_column</code>.</li>
<li>Categorical features can be represented by any <code>sparse_column_with_*</code> column (<code>sparse_column_with_keys</code>, <code>sparse_column_with_vocabulary_file</code>, <code>sparse_column_with_hash_bucket</code>, <code>sparse_column_with_integerized_feature</code>).</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Model type:</li>
</ol>
<ul>
<li>Deep neural network models (<code>DNNClassifier</code>, <code>DNNRegressor</code>).</li>
</ul>
<p>Continuous features can be directly fed into deep neural network models.</p>
<pre><code> age_column = real_valued_column(&quot;age&quot;)</code></pre>
<p>To feed sparse features into DNN models, wrap the column with <code>embedding_column</code> or <code>one_hot_column</code>. <code>one_hot_column</code> will create a dense boolean tensor with an entry for each possible value, and thus the computation cost is linear in the number of possible values versus the number of values that occur in the sparse tensor. Thus using a &quot;one_hot_column&quot; is only recommended for features with only a few possible values. For features with many possible values or for very sparse features, <code>embedding_column</code> is recommended.</p>
<pre><code> embedded_dept_column = embedding_column(
   sparse_column_with_keys(&quot;department&quot;, [&quot;math&quot;, &quot;philosphy&quot;, ...]),
   dimension=10)</code></pre>
<ul>
<li>Wide (aka linear) models (<code>LinearClassifier</code>, <code>LinearRegressor</code>).</li>
</ul>
<p>Sparse features can be fed directly into linear models. When doing so an embedding_lookups are used to efficiently perform the sparse matrix multiplication.</p>
<pre><code> dept_column = sparse_column_with_keys(&quot;department&quot;,
   [&quot;math&quot;, &quot;philosophy&quot;, &quot;english&quot;])</code></pre>
<p>It is recommended that continuous features be bucketized before being fed into linear models.</p>
<pre><code> bucketized_age_column = bucketized_column(
  source_column=age_column,
  boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])</code></pre>
<p>Sparse features can be crossed (also known as conjuncted or combined) in order to form non-linearities, and then fed into linear models.</p>
<pre><code>cross_dept_age_column = crossed_column(
  columns=[department_column, bucketized_age_column],
  hash_bucket_size=1000)</code></pre>
<p>Example of building tf.learn model using FeatureColumns:</p>
<p># Define features and transformations deep_feature_columns = [age_column, embedded_dept_column] wide_feature_columns = [dept_column, bucketized_age_column, cross_dept_age_column]</p>
<p># Build deep model estimator = DNNClassifier( feature_columns=deep_feature_columns, hidden_units=[500, 250, 50]) estimator.train(...)</p>
<p># Or build a wide model estimator = LinearClassifier( feature_columns=wide_feature_columns) estimator.train(...)</p>
<p># Or build a wide and deep model! estimator = DNNLinearCombinedClassifier( linear_feature_columns=wide_feature_columns, dnn_feature_columns=deep_feature_columns, dnn_hidden_units=[500, 250, 50]) estimator.train(...)</p>
<p>FeatureColumns can also be transformed into a generic input layer for custom models using <code>input_from_feature_columns</code> within <code>feature_column_ops.py</code>.</p>
<p>Example of building non-tf.learn model using FeatureColumns:</p>
<p># Building model via layers</p>
<p>deep_feature_columns = [age_column, embedded_dept_column] columns_to_tensor = parse_feature_columns_from_examples( serialized=my_data, feature_columns=deep_feature_columns) first_layer = input_from_feature_columns( columns_to_tensors=columns_to_tensor, feature_columns=deep_feature_columns) second_layer = fully_connected(first_layer, ...)</p>
<p>See feature_column_ops_test for more examples.</p>
