<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.uniform_unit_scaling_initializer" /> <meta itemprop="property" content="__call__"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="from_config"/> <meta itemprop="property" content="get_config"/></p>
</div>
<a name="//apple_ref/cpp/Function/tf.uniform_unit_scaling_initializer" class="dashAnchor"></a><h1 id="tf.uniform_unit_scaling_initializer">tf.uniform_unit_scaling_initializer</h1>
<h3 id="class-tf.uniform_unit_scaling_initializer"><code>class tf.uniform_unit_scaling_initializer</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/init_ops.py"><code>tensorflow/python/ops/init_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/state_ops.md#Sharing_Variables">Variables &gt; Sharing Variables</a></p>
<p>Initializer that generates tensors without scaling variance.</p>
<p>When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. If the input is <code>x</code> and the operation <code>x * W</code>, and we want to initialize <code>W</code> uniformly at random, we need to pick <code>W</code> from</p>
<pre><code>[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]</code></pre>
<p>to keep the scale intact, where <code>dim = W.shape[0]</code> (the size of the input). A similar calculation for convolutional networks gives an analogous result with <code>dim</code> equal to the product of the first 3 dimensions. When nonlinearities are present, we need to multiply this by a constant <code>factor</code>. See <a href="https://arxiv.org/abs/1412.6558">Sussillo et al., 2014</a> (<a href="http://arxiv.org/pdf/1412.6558.pdf">pdf</a>) for deeper motivation, experiments and the calculation of constants. In section 2.3 there, the constants were numerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>factor</code></b>: Float. A multiplicative factor by which the values will be scaled.</li>
<li><b><code>seed</code></b>: A Python integer. Used to create random seeds. See <a href="../tf/set_random_seed.html"><code>tf.set_random_seed</code></a> for behavior.</li>
<li><b><code>dtype</code></b>: The data type. Only floating point types are supported.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    factor<span class="op">=</span><span class="fl">1.0</span>,
    seed<span class="op">=</span><span class="va">None</span>,
    dtype<span class="op">=</span>tf.float32
)</code></pre></div>
<h3 id="__call__">
<code><strong>call</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__call__</span>(
    shape,
    dtype<span class="op">=</span><span class="va">None</span>,
    partition_info<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<h3 id="from_config">
<code>from_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">from_config(
    cls,
    config
)</code></pre></div>
<p>Instantiates an initializer from a configuration dictionary.</p>
<p>Example:</p>
<pre><code>initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)</code></pre>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>config</code></b>: A Python dictionary. It will typically be the output of <code>get_config</code>.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>An Initializer instance.</p>
<h3 id="get_config">
<code>get_config</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_config()</code></pre></div>
