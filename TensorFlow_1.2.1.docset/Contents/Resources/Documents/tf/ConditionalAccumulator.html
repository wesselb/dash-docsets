<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.ConditionalAccumulator" /> <meta itemprop="property" content="accumulator_ref"/> <meta itemprop="property" content="dtype"/> <meta itemprop="property" content="name"/> <meta itemprop="property" content="__init__"/> <meta itemprop="property" content="apply_grad"/> <meta itemprop="property" content="num_accumulated"/> <meta itemprop="property" content="set_global_step"/> <meta itemprop="property" content="take_grad"/></p>
</div>
<a name="//apple_ref/cpp/Class/tf.ConditionalAccumulator" class="dashAnchor"></a><h1 id="tf.conditionalaccumulator">tf.ConditionalAccumulator</h1>
<h3 id="class-tf.conditionalaccumulator"><code>class tf.ConditionalAccumulator</code></h3>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/data_flow_ops.py"><code>tensorflow/python/ops/data_flow_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/io_ops.md#Conditional_Accumulators">Inputs and Readers &gt; Conditional Accumulators</a></p>
<p>A conditional accumulator for aggregating gradients.</p>
<p>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</p>
<p>Extraction of the average gradient is blocked until the required number of gradients has been accumulated.</p>
<h2 id="properties">Properties</h2>
<h3 id="accumulator_ref">
<code>accumulator_ref</code>
</h3>
<p>The underlying accumulator reference.</p>
<h3 id="dtype">
<code>dtype</code>
</h3>
<p>The datatype of the gradients accumulated by this accumulator.</p>
<h3 id="name">
<code>name</code>
</h3>
<p>The name of the underlying accumulator.</p>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="fu">__init__</span>(
    dtype,
    shape<span class="op">=</span><span class="va">None</span>,
    shared_name<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="st">&#39;conditional_accumulator&#39;</span>
)</code></pre></div>
<p>Creates a new ConditionalAccumulator.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>dtype</code></b>: Datatype of the accumulated gradients.</li>
<li><b><code>shape</code></b>: Shape of the accumulated gradients.</li>
<li><b><code>shared_name</code></b>: Optional. If non-empty, this accumulator will be shared under the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the accumulator.</li>
</ul>
<h3 id="apply_grad">
<code>apply_grad</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">apply_grad(
    grad,
    local_step<span class="op">=</span><span class="dv">0</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Attempts to apply a gradient to the accumulator.</p>
<p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>grad</code></b>: The gradient tensor to be applied.</li>
<li><b><code>local_step</code></b>: Time step at which the gradient was computed.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>The operation that (conditionally) applies a gradient to the accumulator.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If grad is of the wrong shape</li>
</ul>
<h3 id="num_accumulated">
<code>num_accumulated</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">num_accumulated(name<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<p>Number of gradients that have currently been aggregated in accumulator.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h4 id="returns-1">Returns:</h4>
<p>Number of accumulated gradients currently in accumulator.</p>
<h3 id="set_global_step">
<code>set_global_step</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">set_global_step(
    new_global_step,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Sets the global time step of the accumulator.</p>
<p>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>
<h4 id="returns-2">Returns:</h4>
<p>Operation that sets the accumulator's time step.</p>
<h3 id="take_grad">
<code>take_grad</code>
</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">take_grad(
    num_required,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Attempts to extract the average gradient from the accumulator.</p>
<p>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</p>
<p>Once successful, the following actions are also triggered:</p>
<ul>
<li>Counter of accumulated gradients is reset to 0.</li>
<li>Aggregated gradient is reset to 0 tensor.</li>
<li>Accumulator's internal time step is incremented by 1.</li>
</ul>
<h4 id="args-4">Args:</h4>
<ul>
<li><b><code>num_required</code></b>: Number of gradients that needs to have been aggregated</li>
<li><b><code>name</code></b>: Optional name for the operation</li>
</ul>
<h4 id="returns-3">Returns:</h4>
<p>A tensor holding the value of the average gradient.</p>
<h4 id="raises-1">Raises:</h4>
<ul>
<li><b><code>InvalidArgumentError</code></b>: If num_required &lt; 1</li>
</ul>
