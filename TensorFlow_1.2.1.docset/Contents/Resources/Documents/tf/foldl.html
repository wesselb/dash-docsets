<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.foldl" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.foldl" class="dashAnchor"></a><h1 id="tf.foldl">tf.foldl</h1>
<h3 id="tf.foldl-1"><code>tf.foldl</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">foldl(
    fn,
    elems,
    initializer<span class="op">=</span><span class="va">None</span>,
    parallel_iterations<span class="op">=</span><span class="dv">10</span>,
    back_prop<span class="op">=</span><span class="va">True</span>,
    swap_memory<span class="op">=</span><span class="va">False</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/functional_ops.py"><code>tensorflow/python/ops/functional_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/functional_ops.md#Higher_Order_Operators">Higher Order Functions &gt; Higher Order Operators</a></p>
<p>foldl on the list of tensors unpacked from <code>elems</code> on dimension 0.</p>
<p>This foldl operator repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn. If <code>initializer</code> is None, <code>elems</code> must contain at least one element, and its first element is used as the initializer.</p>
<p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is fn(initializer, values[0]).shape`.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>fn</code></b>: The callable to be performed.</li>
<li><b><code>elems</code></b>: A tensor to be unpacked on dimension 0.</li>
<li><b><code>initializer</code></b>: (optional) The initial value for the accumulator.</li>
<li><b><code>parallel_iterations</code></b>: (optional) The number of iterations allowed to run in parallel.</li>
<li><b><code>back_prop</code></b>: (optional) True enables support for back propagation.</li>
<li><b><code>swap_memory</code></b>: (optional) True enables GPU-CPU memory swapping.</li>
<li><b><code>name</code></b>: (optional) Name prefix for the returned tensors.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor resulting from applying <code>fn</code> consecutively to the list of tensors unpacked from <code>elems</code>, from first to last.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>fn</code> is not callable.</li>
</ul>
<p>Example: <code>python   elems = [1, 2, 3, 4, 5, 6]   sum = foldl(lambda a, x: a + x, elems)   # sum == 21</code></p>
