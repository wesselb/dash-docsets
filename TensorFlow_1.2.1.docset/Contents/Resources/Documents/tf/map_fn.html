<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.map_fn" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.map_fn" class="dashAnchor"></a><h1 id="tf.map_fn">tf.map_fn</h1>
<h3 id="tf.map_fn-1"><code>tf.map_fn</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">map_fn(
    fn,
    elems,
    dtype<span class="op">=</span><span class="va">None</span>,
    parallel_iterations<span class="op">=</span><span class="dv">10</span>,
    back_prop<span class="op">=</span><span class="va">True</span>,
    swap_memory<span class="op">=</span><span class="va">False</span>,
    infer_shape<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/functional_ops.py"><code>tensorflow/python/ops/functional_ops.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/functional_ops.md#Higher_Order_Operators">Higher Order Functions &gt; Higher Order Operators</a></p>
<p>map on the list of tensors unpacked from <code>elems</code> on dimension 0.</p>
<p>The simplest version of <code>map_fn</code> repeatedly applies the callable <code>fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code>elems</code>. <code>dtype</code> is the data type of the return value of <code>fn</code>. Users must provide <code>dtype</code> if it is different from the data type of <code>elems</code>.</p>
<p>Suppose that <code>elems</code> is unpacked into <code>values</code>, a list of tensors. The shape of the result tensor is <code>[values.shape[0]] + fn(values[0]).shape</code>.</p>
<p>This method also allows multi-arity <code>elems</code> and output of <code>fn</code>. If <code>elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of <code>fn</code> may match the structure of <code>elems</code>. That is, if <code>elems</code> is <code>(t1, [t2, t3, [t4, t5]])</code>, then an appropriate signature for <code>fn</code> is: <code>fn = lambda (t1, [t2, t3, [t4, t5]]):</code>.</p>
<p>Furthermore, <code>fn</code> may emit a different structure than its input. For example, <code>fn</code> may look like: <code>fn = lambda t1: return (t1 + 1, t1 - 1)</code>. In this case, the <code>dtype</code> parameter is not optional: <code>dtype</code> must be a type or (possibly nested) tuple of types matching the output of <code>fn</code>.</p>
<p>To apply a functional operation to the nonzero elements of a SparseTensor one of the following methods is recommended. First, if the function is expressible as TensorFlow ops, use</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">  result <span class="op">=</span> SparseTensor(<span class="bu">input</span>.indices, fn(<span class="bu">input</span>.values), <span class="bu">input</span>.dense_shape)</code></pre></div>
<p>If, however, the function is not expressible as a TensorFlow op, then use</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">result <span class="op">=</span> SparseTensor(
  <span class="bu">input</span>.indices, map_fn(fn, <span class="bu">input</span>.values), <span class="bu">input</span>.dense_shape)</code></pre></div>
<p>instead.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>fn</code></b>: The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as <code>elems</code>. Its output must have the same structure as <code>dtype</code> if one is provided, otherwise it must have the same structure as <code>elems</code>.</li>
<li><b><code>elems</code></b>: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be applied to <code>fn</code>.</li>
<li><b><code>dtype</code></b>: (optional) The output type(s) of <code>fn</code>. If <code>fn</code> returns a structure of Tensors differing from the structure of <code>elems</code>, then <code>dtype</code> is not optional and must have the same structure as the output of <code>fn</code>.</li>
<li><b><code>parallel_iterations</code></b>: (optional) The number of iterations allowed to run in parallel.</li>
<li><b><code>back_prop</code></b>: (optional) True enables support for back propagation.</li>
<li><b><code>swap_memory</code></b>: (optional) True enables GPU-CPU memory swapping.</li>
<li><b><code>infer_shape</code></b>: (optional) False disables tests for consistent output shapes.</li>
<li><b><code>name</code></b>: (optional) Name prefix for the returned tensors.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code>fn</code> to tensors unpacked from <code>elems</code> along the first dimension, from first to last.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: if <code>fn</code> is not callable or the structure of the output of <code>fn</code> and <code>dtype</code> do not match, or if elems is a SparseTensor.</li>
<li><b><code>ValueError</code></b>: if the lengths of the output of <code>fn</code> and <code>dtype</code> do not match.</li>
</ul>
<p>Examples: <code>python   elems = np.array([1, 2, 3, 4, 5, 6])   squares = map_fn(lambda x: x * x, elems)   # squares == [1, 4, 9, 16, 25, 36]</code></p>
<p><code>python   elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))   alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)   # alternate == [-1, 2, -3]</code></p>
<p><code>python   elems = np.array([1, 2, 3])   alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))   # alternates[0] == [1, 2, 3]   # alternates[1] == [-1, -2, -3]</code></p>
