<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.get_variable" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.get_variable" class="dashAnchor"></a><h1 id="tf.get_variable">tf.get_variable</h1>
<h3 id="tf.get_variable-1"><code>tf.get_variable</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">get_variable(
    name,
    shape<span class="op">=</span><span class="va">None</span>,
    dtype<span class="op">=</span><span class="va">None</span>,
    initializer<span class="op">=</span><span class="va">None</span>,
    regularizer<span class="op">=</span><span class="va">None</span>,
    trainable<span class="op">=</span><span class="va">True</span>,
    collections<span class="op">=</span><span class="va">None</span>,
    caching_device<span class="op">=</span><span class="va">None</span>,
    partitioner<span class="op">=</span><span class="va">None</span>,
    validate_shape<span class="op">=</span><span class="va">True</span>,
    use_resource<span class="op">=</span><span class="va">None</span>,
    custom_getter<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/variable_scope.py"><code>tensorflow/python/ops/variable_scope.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/state_ops.md#Sharing_Variables">Variables &gt; Sharing Variables</a></p>
<p>Gets an existing variable with these parameters or create a new one.</p>
<p>This function prefixes the name with the current variable scope and performs reuse checks. See the <a href="../../../programmers_guide/variable_scope.html">Variable Scope How To</a> for an extensive description of how reusing works. Here is a basic example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">with</span> tf.variable_scope(<span class="st">&quot;foo&quot;</span>):
    v <span class="op">=</span> tf.get_variable(<span class="st">&quot;v&quot;</span>, [<span class="dv">1</span>])  <span class="co"># v.name == &quot;foo/v:0&quot;</span>
    w <span class="op">=</span> tf.get_variable(<span class="st">&quot;w&quot;</span>, [<span class="dv">1</span>])  <span class="co"># w.name == &quot;foo/w:0&quot;</span>
<span class="cf">with</span> tf.variable_scope(<span class="st">&quot;foo&quot;</span>, reuse<span class="op">=</span><span class="va">True</span>):
    v1 <span class="op">=</span> tf.get_variable(<span class="st">&quot;v&quot;</span>)  <span class="co"># The same as v above.</span></code></pre></div>
<p>If initializer is <code>None</code> (the default), the default initializer passed in the variable scope will be used. If that one is <code>None</code> too, a <code>glorot_uniform_initializer</code> will be used. The initializer can also be a Tensor, in which case the variable is initialized to this value and shape.</p>
<p>Similarly, if the regularizer is <code>None</code> (the default), the default regularizer passed in the variable scope will be used (if that is <code>None</code> too, then by default no regularization is performed).</p>
<p>If a partitioner is provided, a <code>PartitionedVariable</code> is returned. Accessing this object as a <code>Tensor</code> returns the shards concatenated along the partition axis.</p>
<p>Some useful partitioners are available. See, e.g., <code>variable_axis_size_partitioner</code> and <code>min_max_variable_partitioner</code>.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>name</code></b>: The name of the new or existing variable.</li>
<li><b><code>shape</code></b>: Shape of the new or existing variable.</li>
<li><b><code>dtype</code></b>: Type of the new or existing variable (defaults to <code>DT_FLOAT</code>).</li>
<li><b><code>initializer</code></b>: Initializer for the variable if one is created.</li>
<li><b><code>regularizer</code></b>: A (Tensor -&gt; Tensor or None) function; the result of applying it on a newly created variable will be added to the collection <a href="../tf/GraphKeys.md#REGULARIZATION_LOSSES"><code>tf.GraphKeys.REGULARIZATION_LOSSES</code></a> and can be used for regularization.</li>
<li><b><code>trainable</code></b>: If <code>True</code> also add the variable to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see <code>tf.Variable</code>).</li>
<li><b><code>collections</code></b>: List of graph collections keys to add the Variable to. Defaults to <code>[GraphKeys.GLOBAL_VARIABLES]</code> (see <code>tf.Variable</code>).</li>
<li><b><code>caching_device</code></b>: Optional device string or function describing where the Variable should be cached for reading. Defaults to the Variable's device. If not <code>None</code>, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through <code>Switch</code> and other conditional statements.</li>
<li><b><code>partitioner</code></b>: Optional callable that accepts a fully defined <code>TensorShape</code> and <code>dtype</code> of the Variable to be created, and returns a list of partitions for each axis (currently only one axis can be partitioned).</li>
<li><b><code>validate_shape</code></b>: If False, allows the variable to be initialized with a value of unknown shape. If True, the default, the shape of initial_value must be known.</li>
<li><b><code>use_resource</code></b>: If False, creates a regular Variable. If true, creates an experimental ResourceVariable instead with well-defined semantics. Defaults to False (will later change to True).</li>
<li><p><b><code>custom_getter</code></b>: Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. The signature of <code>custom_getter</code> should match that of this method, but the most future-proof version will allow for changes: <code>def custom_getter(getter, *args, **kwargs)</code>. Direct access to all <code>get_variable</code> parameters is also allowed: <code>def custom_getter(getter, name, *args, **kwargs)</code>. A simple identity custom getter that simply creates variables with modified names is:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> custom_getter(getter, name, <span class="op">*</span>args, <span class="op">**</span>kwargs):
  <span class="cf">return</span> getter(name <span class="op">+</span> <span class="st">&#39;_suffix&#39;</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs)</code></pre></div></li>
</ul>
<h4 id="returns">Returns:</h4>
<p>The created or existing <code>Variable</code> (or <code>PartitionedVariable</code>, if a partitioner was used).</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: when creating a new variable and shape is not declared, when violating reuse during variable creation, or when <code>initializer</code> dtype and <code>dtype</code> don't match. Reuse is set inside <code>variable_scope</code>.</li>
</ul>
