<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.gradients" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.gradients" class="dashAnchor"></a><h1 id="tf.gradients">tf.gradients</h1>
<h3 id="tf.gradients-1"><code>tf.gradients</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">gradients(
    ys,
    xs,
    grad_ys<span class="op">=</span><span class="va">None</span>,
    name<span class="op">=</span><span class="st">&#39;gradients&#39;</span>,
    colocate_gradients_with_ops<span class="op">=</span><span class="va">False</span>,
    gate_gradients<span class="op">=</span><span class="va">False</span>,
    aggregation_method<span class="op">=</span><span class="va">None</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/ops/gradients_impl.py"><code>tensorflow/python/ops/gradients_impl.py</code></a>.</p>
<p>See the guide: <a href="../../../api_guides/python/train.md#Gradient_Computation">Training &gt; Gradient Computation</a></p>
<p>Constructs symbolic partial derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>.</p>
<p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p>
<p><code>gradients()</code> adds ops to the graph to output the partial derivatives of <code>ys</code> with respect to <code>xs</code>. It returns a list of <code>Tensor</code> of length <code>len(xs)</code> where each tensor is the <code>sum(dy/dx)</code> for y in <code>ys</code>.</p>
<p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each y in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of '1's of the shape of y for each y in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>ys</code></b>: A <code>Tensor</code> or list of tensors to be differentiated.</li>
<li><b><code>xs</code></b>: A <code>Tensor</code> or list of tensors to be used for differentiation.</li>
<li><b><code>grad_ys</code></b>: Optional. A <code>Tensor</code> or list of tensors the same size as <code>ys</code> and holding the gradients computed for each y in <code>ys</code>.</li>
<li><b><code>name</code></b>: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.</li>
<li><b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li>
<li><b><code>gate_gradients</code></b>: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.</li>
<li><b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class <code>AggregationMethod</code>.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A list of <code>sum(dy/dx)</code> for each x in <code>xs</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>LookupError</code></b>: if one of the operations between <code>x</code> and <code>y</code> does not have a registered gradient function.</li>
<li><b><code>ValueError</code></b>: if the arguments are invalid.</li>
</ul>
