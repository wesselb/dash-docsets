<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<p><meta itemprop="name" content="tf.layers.batch_normalization" /></p>
</div>
<a name="//apple_ref/cpp/Function/tf.layers.batch_normalization" class="dashAnchor"></a><h1 id="tf.layers.batch_normalization">tf.layers.batch_normalization</h1>
<h3 id="tf.layers.batch_normalization-1"><code>tf.layers.batch_normalization</code></h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">batch_normalization(
    inputs,
    axis<span class="op">=-</span><span class="dv">1</span>,
    momentum<span class="op">=</span><span class="fl">0.99</span>,
    epsilon<span class="op">=</span><span class="fl">0.001</span>,
    center<span class="op">=</span><span class="va">True</span>,
    scale<span class="op">=</span><span class="va">True</span>,
    beta_initializer<span class="op">=</span>tf.zeros_initializer(),
    gamma_initializer<span class="op">=</span>tf.ones_initializer(),
    moving_mean_initializer<span class="op">=</span>tf.zeros_initializer(),
    moving_variance_initializer<span class="op">=</span>tf.ones_initializer(),
    beta_regularizer<span class="op">=</span><span class="va">None</span>,
    gamma_regularizer<span class="op">=</span><span class="va">None</span>,
    training<span class="op">=</span><span class="va">False</span>,
    trainable<span class="op">=</span><span class="va">True</span>,
    name<span class="op">=</span><span class="va">None</span>,
    reuse<span class="op">=</span><span class="va">None</span>,
    renorm<span class="op">=</span><span class="va">False</span>,
    renorm_clipping<span class="op">=</span><span class="va">None</span>,
    renorm_momentum<span class="op">=</span><span class="fl">0.99</span>
)</code></pre></div>
<p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/python/layers/normalization.py"><code>tensorflow/python/layers/normalization.py</code></a>.</p>
<p>Functional interface for the batch normalization layer.</p>
<p>Reference: http://arxiv.org/abs/1502.03167</p>
<p>&quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot;</p>
<p>Sergey Ioffe, Christian Szegedy</p>
<p>Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in <code>tf.GraphKeys.UPDATE_OPS</code>, so they need to be added as a dependency to the <code>train_op</code>. For example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">  update_ops <span class="op">=</span> tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  <span class="cf">with</span> tf.control_dependencies(update_ops):
    train_op <span class="op">=</span> optimizer.minimize(loss)</code></pre></div>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: Tensor input.</li>
<li><b><code>axis</code></b>: Integer, the axis that should be normalized (typically the features axis). For instance, after a <code>Convolution2D</code> layer with <code>data_format=&quot;channels_first&quot;</code>, set <code>axis=1</code> in <code>BatchNormalization</code>.</li>
<li><b><code>momentum</code></b>: Momentum for the moving average.</li>
<li><b><code>epsilon</code></b>: Small float added to variance to avoid dividing by zero.</li>
<li><b><code>center</code></b>: If True, add offset of <code>beta</code> to normalized tensor. If False, <code>beta</code> is ignored.</li>
<li><b><code>scale</code></b>: If True, multiply by <code>gamma</code>. If False, <code>gamma</code> is not used. When the next layer is linear (also e.g. <code>nn.relu</code>), this can be disabled since the scaling can be done by the next layer.</li>
<li><b><code>beta_initializer</code></b>: Initializer for the beta weight.</li>
<li><b><code>gamma_initializer</code></b>: Initializer for the gamma weight.</li>
<li><b><code>moving_mean_initializer</code></b>: Initializer for the moving mean.</li>
<li><b><code>moving_variance_initializer</code></b>: Initializer for the moving variance.</li>
<li><b><code>beta_regularizer</code></b>: Optional regularizer for the beta weight.</li>
<li><b><code>gamma_regularizer</code></b>: Optional regularizer for the gamma weight.</li>
<li><b><code>training</code></b>: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). <strong>NOTE</strong>: make sure to set this parameter correctly, or else your training/inference will not work properly.</li>
<li><b><code>trainable</code></b>: Boolean, if <code>True</code> also add variables to the graph collection <code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><b><code>name</code></b>: String, the name of the layer.</li>
<li><b><code>reuse</code></b>: Boolean, whether to reuse the weights of a previous layer by the same name.</li>
<li><b><code>renorm</code></b>: Whether to use Batch Renormalization (https://arxiv.org/abs/1702.03275). This adds extra variables during training. The inference is the same for either value of this parameter.</li>
<li><b><code>renorm_clipping</code></b>: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar <code>Tensors</code> used to clip the renorm correction. The correction <code>(r, d)</code> is used as <code>corrected_value = normalized_value * r + d</code>, with <code>r</code> clipped to [rmin, rmax], and <code>d</code> to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.</li>
<li><b><code>renorm_momentum</code></b>: Momentum used to update the moving means and standard deviations with renorm. Unlike <code>momentum</code>, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that <code>momentum</code> is still applied to get the means and variances for inference.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>Output tensor.</p>
